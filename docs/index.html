<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Yliess Hati" />
  <meta name="keywords" content="keyword" />
  <title>AI-Assisted Creative Expression: a Case for Automatic Lineart Colorization</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title"><p>AI-Assisted Creative Expression: a Case for
Automatic Lineart Colorization</p></h1>
<p class="author">Yliess Hati</p>
</header>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Contents</h2>
<ul>
<li><a href="#list-of-abbreviations" id="toc-list-of-abbreviations">List
of Abbreviations</a></li>
<li><a href="#acronym-list" id="toc-acronym-list">Acronyms</a></li>
<li><a href="#abstract" id="toc-abstract">Abstract</a></li>
<li><a href="#aknowledgements"
id="toc-aknowledgements">Aknowledgements</a></li>
<li><a href="#context" id="toc-context">Context</a>
<ul>
<li><a href="#ch:introduction" id="toc-ch:introduction">Introduction</a>
<ul>
<li><a href="#motivations" id="toc-motivations">Motivations</a></li>
<li><a href="#problem-statement" id="toc-problem-statement">Problem
Statement</a></li>
<li><a href="#contributions"
id="toc-contributions">Contributions</a></li>
<li><a href="#concerns" id="toc-concerns">Concerns</a></li>
<li><a href="#outline" id="toc-outline">Outline</a></li>
</ul></li>
<li><a href="#ch:background" id="toc-ch:background">Background</a>
<ul>
<li><a href="#sec:history" id="toc-sec:history">A Brief History of
Artificial Intelligence</a></li>
<li><a href="#sec:core" id="toc-sec:core">Core Principles</a></li>
<li><a href="#sec:nn" id="toc-sec:nn">Neural Networks</a></li>
<li><a href="#sec:generative" id="toc-sec:generative">Generative
Architectures</a></li>
</ul></li>
<li><a href="#ch:methodology" id="toc-ch:methodology">Methodology</a>
<ul>
<li><a href="#implementation"
id="toc-implementation">Implementation</a></li>
<li><a href="#objective-evaluation"
id="toc-objective-evaluation">Objective Evaluation</a></li>
<li><a href="#subjective-evaluation"
id="toc-subjective-evaluation">Subjective Evaluation</a></li>
<li><a href="#reproducibility"
id="toc-reproducibility">Reproducibility</a></li>
</ul></li>
</ul></li>
<li><a href="#core" id="toc-core">Core</a>
<ul>
<li><a href="#ch:contrib-1" id="toc-ch:contrib-1">Contrib I (Find Catchy
Explicit Name)</a>
<ul>
<li><a href="#state-of-the-art" id="toc-state-of-the-art">State of the
Art</a></li>
<li><a href="#method" id="toc-method">Method</a></li>
<li><a href="#setup" id="toc-setup">Setup</a></li>
<li><a href="#results" id="toc-results">Results</a></li>
<li><a href="#summary" id="toc-summary">Summary</a></li>
</ul></li>
<li><a href="#ch:contrib-2" id="toc-ch:contrib-2">Contrib II (Find
Catchy Explicit Name)</a>
<ul>
<li><a href="#state-of-the-art-1" id="toc-state-of-the-art-1">State of
the Art</a></li>
<li><a href="#method-1" id="toc-method-1">Method</a></li>
<li><a href="#setup-1" id="toc-setup-1">Setup</a></li>
<li><a href="#results-1" id="toc-results-1">Results</a></li>
<li><a href="#summary-1" id="toc-summary-1">Summary</a></li>
</ul></li>
<li><a href="#ch:contrib-3" id="toc-ch:contrib-3">Contrib III (Find
Catchy Explicit Name)</a>
<ul>
<li><a href="#state-of-the-art-2" id="toc-state-of-the-art-2">State of
the Art</a></li>
<li><a href="#method-2" id="toc-method-2">Method</a></li>
<li><a href="#setup-2" id="toc-setup-2">Setup</a></li>
<li><a href="#results-2" id="toc-results-2">Results</a></li>
<li><a href="#summary-2" id="toc-summary-2">Summary</a></li>
</ul></li>
<li><a href="#ch:contrib-4" id="toc-ch:contrib-4">Contrib IV (Find
Catchy Explicit Name)</a>
<ul>
<li><a href="#state-of-the-art-3" id="toc-state-of-the-art-3">State of
the Art</a></li>
<li><a href="#method-3" id="toc-method-3">Method</a></li>
<li><a href="#setup-3" id="toc-setup-3">Setup</a></li>
<li><a href="#results-3" id="toc-results-3">Results</a></li>
<li><a href="#summary-3" id="toc-summary-3">Summary</a></li>
</ul></li>
</ul></li>
<li><a href="#reflection" id="toc-reflection">Reflection</a>
<ul>
<li><a href="#ch:ethical-and-societal-impact"
id="toc-ch:ethical-and-societal-impact">Ethical and Societal
Impact</a></li>
<li><a href="#ch:conclusion" id="toc-ch:conclusion">Conclusion</a></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul></li>
</ul>
</nav>
<h2 class="unnumbered" id="list-of-abbreviations">List of
Abbreviations</h2>
<h1 id="acronym-list">Acronyms</h1>
<ul>
<li><strong>ACT-R</strong>: Adaptive Control of Thought—Rational</li>
<li><strong>AD</strong>: Automatic Differentiation</li>
<li><strong>AE</strong>: Autoencoder</li>
<li><strong>AI</strong>: Artificial Intelligence</li>
<li><strong>ANN</strong>: Artificial Neural Network</li>
<li><strong>AST</strong>: Abstract Syntax Tree</li>
<li><strong>CNN</strong>: Convolutional Neural Network</li>
<li><strong>CV</strong>: Computer Vision</li>
<li><strong>DAG</strong>: Directed Acyclic Graph</li>
<li><strong>DDM</strong>: Denoising Diffusion Model</li>
<li><strong>DDPM</strong>: Denoising Diffusion Probabilistic Model</li>
<li><strong>DL</strong>: Deep Learning</li>
<li><strong>GAN</strong>: Generative Adversarial Network</li>
<li><strong>GD</strong>: Gradient Descent</li>
<li><strong>GPU</strong>: Graphical Processing Unit</li>
<li><strong>KL-Divergence</strong>: Kullback-Leibler Divergence</li>
<li><strong>LLM</strong>: Large Language Model</li>
<li><strong>LSTM</strong>: Long Short-Term Memory</li>
<li><strong>ML</strong>: Machine Learning</li>
<li><strong>MLP</strong>: Multi-Layer Perceptron</li>
<li><strong>MNIST</strong>: Modified National Institute of Standards and
Technology</li>
<li><strong>MSE</strong>: Mean Squared Error</li>
<li><strong>NLP</strong>: Natural Language Processing</li>
<li><strong>NN</strong>: Neural Network</li>
<li><strong>NPU</strong>: Neural Processing Unit</li>
<li><strong>RLHF</strong>: Reinforcement Learning from Human
Feedback</li>
<li><strong>RNN</strong>: Recurrent Neural Network</li>
<li><strong>ReLU</strong>: Rectified Linear Unit</li>
<li><strong>SGD</strong>: Stochastic Gradient Descent</li>
<li><strong>SVM</strong>: Support Vector Machine</li>
<li><strong>TPU</strong>: Tensor Processing Unit</li>
<li><strong>VAE</strong>: Variational Autoencoder</li>
<li><strong>VI</strong>: Variational Inference</li>
<li><strong>ViT</strong>: Vision Transformer</li>
<li><strong>WGAN</strong>: Wasserstein Generative Adversarial
Network</li>
</ul>
<h2 class="unnumbered" id="abstract">Abstract</h2>

<h2 class="unnumbered" id="aknowledgements">Aknowledgements</h2>

<h1 id="context">Context</h1>
<h2 id="ch:introduction">Introduction</h2>
<p>Humans possess the ability to perceive and understand the world
allowing us to accomplish a wide range of complex tasks through the
combination of visual recognition, scene understanding, and
communication. The ability to quickly and accurately extract information
from a single image is a testament to the complexity and sophistication
of the human brain and is often taken for granted. One of the Artificial
Intelligence (AI) field’s ultimate goals is to empower computers with
such human-like abilities, one of them being creativity, being able to
produce something original and worthwhile <span class="citation"
data-cites="mumford_2012">[<a href="#ref-mumford_2012"
role="doc-biblioref">53</a>]</span>.</p>
<p>Computational creativity is the field at the intersection of AI,
cognitive psychology, philosophy, and art, which aims at understanding,
simulating, replicating, or in some cases enhancing human creativity.
One definition of computational creativity <span class="citation"
data-cites="newell_1959">[<a href="#ref-newell_1959"
role="doc-biblioref">54</a>]</span> is the ability to produce something
that is novel and useful, demands that we reject common beliefs, results
from intense motivation and persistence, or comes from clarifying a
vague problem. Top-down approaches to this definition use a mix of
explicit formulations of recipes and randomness such as procedural
generation. On the opposite, bottom-up approaches use Artificial Neural
Networks (ANN) to learn patterns and heuristics from large datasets to
enable non-linear generation.</p>
<p>We, as a species, are currently witnessing the beginning of a new era
where the gap between machines and humans is starting to blur. Current
breakthroughs in the field of AI, more specifically in Deep Learning
(DL), are giving computers the ability to perceive and understand our
world, but also to interact with our environment using natural
interactions such as speech and natural language. ANNs, once mocked by
the AI community <span class="citation" data-cites="lecun_2019">[<a
href="#ref-lecun_2019" role="doc-biblioref">41</a>]</span>, are now
trainable using Gradient Descent (GD) <span class="citation"
data-cites="rumelhart_1986">[<a href="#ref-rumelhart_1986"
role="doc-biblioref">64</a>]</span> thanks to the massive availability
of data and the processing power of modern hardware accelerators such as
Graphical Processing Units (GPU), Tensor Processing Units (TPU), and
Neural Processing Units (NPU).</p>
<p>Neural Networks (NN), those trainable general function approximators,
gave rise to the field of generative NNs. Specialized DL architectures
such as Variational Autoencoders (VAE) <span class="citation"
data-cites="kingma_2013">[<a href="#ref-kingma_2013"
role="doc-biblioref">38</a>]</span>, Generative Adversarial Networks
(GAN) <span class="citation" data-cites="goodfellow_2014">[<a
href="#ref-goodfellow_2014" role="doc-biblioref">20</a>]</span>,
Denoising Diffusion Models (DDM) <span class="citation"
data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">28</a>]</span>, and Large Language Models (LLM)
<span class="citation" data-cites="vaswani_2017 brown_2020">[<a
href="#ref-brown_2020" role="doc-biblioref">7</a>, <a
href="#ref-vaswani_2017" role="doc-biblioref">72</a>]</span> are used to
generate artifacts such as text, audio, images, and videos of
unprecedented quality and complexity.</p>
<p>This dissertation aims at exploring how one could train and use
generative NN to create AI-powered tools capable of enhancing human
creative expression. The task of automatic lineart colorization act as
the example case used to illustrate this process throughout the entire
thesis.</p>
<figure id="fig:steps">
<img src="./figures/motivations_steps.svg"
alt="Common illustration process. From left to right: sketching, inking, coloring, and pros-processing. Credits: Taira Akitsu" />
<figcaption>Figure 1: Common illustration process. From left to right:
sketching, inking, coloring, and pros-processing. Credits: Taira
Akitsu</figcaption>
</figure>
<h3 id="motivations">Motivations</h3>
<p>Lineart colorization is an essential aspect of the work of artists,
illustrators, and animators. The task of manually coloring lineart can
be time-consuming, repetitive, and exhausting, particularly in the
animation industry, where every frame of an animated product must be
colored and shaded. This process is typically done using image editing
software such as Photoshop <span class="citation"
data-cites="photoshop">[<a href="#ref-photoshop"
role="doc-biblioref">59</a>]</span>, Clip Studio PAINT <span
class="citation" data-cites="clipstudiopaint">[<a
href="#ref-clipstudiopaint" role="doc-biblioref">11</a>]</span>, and
PaintMan <span class="citation" data-cites="paintman">[<a
href="#ref-paintman" role="doc-biblioref">56</a>]</span>. Automating the
colorization process can greatly improve the workflow of these creative
professionals and has the potential to lower the barrier for newcomers
and amateurs. Such a system was integrated into Clip Studio PAINT <span
class="citation" data-cites="clipstudiopaint">[<a
href="#ref-clipstudiopaint" role="doc-biblioref">11</a>]</span>,
demonstrating the growing significance of automatic colorization in the
field.</p>
<p>The most common digital illustration process can be broken down into
four distinct stages: sketching, inking, coloring, and post-processing
(see <a href="#fig:steps">Fig 1</a>). As demonstrated by the work of
Kandinsky <span class="citation" data-cites="kandinsky_1977">[<a
href="#ref-kandinsky_1977" role="doc-biblioref">34</a>]</span>, the
colorization process can greatly impact the overall meaning of a piece
of art through the introduction of various color schemes, shading, and
textures. These elements of the coloring process present significant
challenges for the Computer Vision (CV) task of automatic lineart
colorization, particularly in comparison to its grayscale counterpart
<span class="citation"
data-cites="furusawa_2O17 hensman_2017 zhang_richard_2017">[<a
href="#ref-furusawa_2O17" role="doc-biblioref">18</a>, <a
href="#ref-hensman_2017" role="doc-biblioref">26</a>, <a
href="#ref-zhang_richard_2017" role="doc-biblioref">77</a>]</span>.
Without the added semantic information provided by textures and shadows,
inferring materials and 3D shapes from black and white linearts is
difficult. They can only be deduced from silhouettes.</p>
<h3 id="problem-statement">Problem Statement</h3>
<p>One major challenge of automatic lineart colorization is the
availability of qualitative public datasets. Illustrations do not always
come with their corresponding lineart. The few datasets available for
the task are lacking consistency in the quality of the illustrations,
gathering images from different types, mediums and styles. For those
reasons, online scrapping and synthetic lineart extraction is the method
of choice for many of the contributions in the field <span
class="citation" data-cites="ci_2018 zhang_richard_2017">[<a
href="#ref-ci_2018" role="doc-biblioref">10</a>, <a
href="#ref-zhang_richard_2017" role="doc-biblioref">77</a>]</span>.</p>
<p>Previous works in automatic lineart colorization are based on the GAN
<span class="citation" data-cites="goodfellow_2014">[<a
href="#ref-goodfellow_2014" role="doc-biblioref">20</a>]</span>
architecture. They can generate unperfect but high-quality illustrations
in a quasi realtime setting. They achieve user control and guidance via
different means, color hints <span class="citation"
data-cites="frans_2017 liu_2017 sangkloy_2016 paintschainer_2017 ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">10</a>, <a
href="#ref-frans_2017" role="doc-biblioref">16</a>, <a
href="#ref-liu_2017" role="doc-biblioref">46</a>, <a
href="#ref-paintschainer_2017" role="doc-biblioref">58</a>, <a
href="#ref-sangkloy_2016" role="doc-biblioref">67</a>]</span>, style
transfer <span class="citation" data-cites="zhang_ji_2017">[<a
href="#ref-zhang_ji_2017" role="doc-biblioref">76</a>]</span>, tagging
<span class="citation" data-cites="kim_2019">[<a href="#ref-kim_2019"
role="doc-biblioref">36</a>]</span>, and more recently natural language
<span class="citation" data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">28</a>]</span>. One common pattern in these methods
is the use of a feature extractor such as Illustration2Vec <span
class="citation" data-cites="saito_2015">[<a href="#ref-saito_2015"
role="doc-biblioref">66</a>]</span> allowing to compensate for the lack
of semantic descriptors by injecting its feature vector into the
models.</p>
<h3 id="contributions">Contributions</h3>
<p>This work focuses on the use of color hints in the form of user
strokes as it fits the natural digital artist workflow and does not
involve learning and mastering a new skill. While previous works offers
improving quality compared to classical CV techniques, they are still
subject to noisy training data, artifacts, a lack of variety, and a lack
of fidelity in the user intent. In this dissertation we explore the
importance of a clean, qualitative and consistent dataset. We
investigate how to better capture the user intent via natural artistic
controls and how to reflect them into the generated model artifact while
preserving or improving its quality. We also look at how the creative
process can be transformed into a dynamic iterative workflow where the
user collaborates with the machine to refine and carry out variations of
his artwork.</p>
<p>Here is a brief enumeration of this thesis’s contributions:</p>
<ul>
<li>We present a recipe for curating datasets for the task of automatic
lineart colorization <span class="citation"
data-cites="hati_2019 hati_2023">[<a href="#ref-hati_2019"
role="doc-biblioref">23</a>, <a href="#ref-hati_2023"
role="doc-biblioref">24</a>]</span></li>
<li>We introduce three generative models:
<ul>
<li>PaintsTorch <span class="citation" data-cites="hati_2019">[<a
href="#ref-hati_2019" role="doc-biblioref">23</a>]</span>, a double GAN
generator that improved generation quality compared to previous work
while allowing realtime interaction with the user.</li>
<li>StencilTorch <span class="citation" data-cites="hati_2023">[<a
href="#ref-hati_2023" role="doc-biblioref">24</a>]</span>, an upgrade
upon PaintsTorch, shifting the colorization problem to in-painting
allowing for human collaboration to emerge as a natural workflow where
the input of a first pass becomes the potential input for a second.</li>
<li>StablePaint, an exploration of DDM for bringing more variety into
the generated outputs allowing for variation exploration and conserving
the iterative workflow introduced by StencilTorch for the cost of
inference speed.</li>
</ul></li>
<li>We offer an advised reflection on current generative AI ethical and
societal impact.</li>
</ul>
<h3 id="concerns">Concerns</h3>
<p>Recent advances in generative AI for text, image, audio, and video
synthesis are raising important ethical and societal concerns,
especially because of its availability and ease of use. Models such as
Stable Diffusion <span class="citation" data-cites="rombach_2021">[<a
href="#ref-rombach_2021" role="doc-biblioref">61</a>]</span> and more
recently Chat-GPT <span class="citation" data-cites="openai_2023">[<a
href="#ref-openai_2023" role="doc-biblioref">9</a>]</span> are
disturbing our common beliefs and relation with copyright, creativity,
the distribution of fake information and so on.</p>
<p>One of the main issues with generative AI is the potential for model
fabulation. Generative models can create entirely new, synthetic data
that is indistinguishable from real data. This can lead to the
dissemination of false information and the manipulation of public
opinion. Additionally, there are ambiguities surrounding the ownership
and copyright of the generated content, as it is unclear who holds the
rights to the generated images and videos. Training data is often
obtained via online scrapping and thus copyright ownership is not
propagated. This is especially true for commercial applications.</p>
<p>Another important concern is the potential for biases and
discrimination. These models are trained on large amounts of data, and
if the data is not diverse or representative enough, the model may
perpetuate or even amplify existing biases. The Microsoft Tay Twitter
bot <span class="citation" data-cites="wolf_2017">[<a
href="#ref-wolf_2017" role="doc-biblioref">74</a>]</span> scandal is an
outcome of such a phenomenon. This initially innocent chatbot has been
easily turned into a racist bot perpetuating hate speech. The task was
made easier because of the inherently biased dataset it was trained
on.</p>
<p>In this work, we are committed to addressing and raising awareness
for these concerns. The illustrations used for training our models and
for our experiments are only used for educational and research purposes.
We only provide recipes for reproducibility and do not distribute the
dataset nor the weights resulting from model training, only the code. We
hope this will not ensure that our work is used ethically and
responsibly but limit its potential misuse.</p>
<h3 id="outline">Outline</h3>
<p>The first part of this thesis (chapters <a
href="#ch:introduction">1</a>-<a href="#ch:methodology">3</a>) provides
context to the recent advances in generative AI and introduces the CV
task of user-guided automatic lineart colorization, its challenges, and
our contributions to the field. It then provides additional background,
from DL first principles to current architectures used in modern
generative NN, and introduces the methodology used throughout the entire
document. This part should be accessible to the majority, experts and
non-experts, and serve as an introduction to the field.</p>
<p>The second part (chapters <a href="#ch:contrib-1">4</a>-<a
href="#ch:contrib-4">7</a>) presents our contributions, some of which
have previously been presented in <span class="citation"
data-cites="hati_2019 hati_2023">[<a href="#ref-hati_2019"
role="doc-biblioref">23</a>, <a href="#ref-hati_2023"
role="doc-biblioref">24</a>]</span>. It introduces into detail our
recipe for sourcing and curating consistent and qualitative datasets for
automatic lineart colorization, PaintsTorch <span class="citation"
data-cites="hati_2019">[<a href="#ref-hati_2019"
role="doc-biblioref">23</a>]</span> our first double generator GAN
conditioned on user strokes, StencilTorch <span class="citation"
data-cites="hati_2023">[<a href="#ref-hati_2023"
role="doc-biblioref">24</a>]</span> our in-painting reformulation
introducing the use of masks to allow the emergence of iterative
workflow and collaboration with the machine, and finally StablePaint, an
exploration of the use of DDM models for variations qualitative
exploration.</p>
<p>The third and final part (chapters <a
href="#ch:ethdical-and-societal-impact">7</a>-<a
href="#ch:conclusion">8</a>) offers a detailed reflection on this
thesis’s contributions and more generally about the field of generative
AI ethical and societal impact, identifies the remaining challenges and
discusses future work.</p>
<p>The code base for the experiments and contributions is publicly
available on GitHub at <a
href="https://github.com/yliess86">https://github.com/yliess86</a>.</p>
<h2 id="ch:background">Background</h2>
<p>This chapter introduces the reader to the field of Deep Learning (DL)
from first principles to the current architectures used in modern
generative AI. The first section (section <a href="#sec:history">1</a>)
presents a brief history of AI to ground this technical dissertation
into its historical context. The following sections (sections <a
href="#sec:core">2</a>-<a href="#sec:attention">4</a>) are discussing
the first principles of modern DL from the early Perceptron to more
modern frameworks such as Large Language Models (LLM).</p>
<div class="sourceCode" id="lst:snippet"><pre
class="sourceCode python"><code class="sourceCode python"><span id="lst:snippet-1"><a href="#lst:snippet-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This is a code snippet</span></span>
<span id="lst:snippet-2"><a href="#lst:snippet-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Hello World!&quot;</span>)</span></code></pre></div>
<p>Additional code snippets (see Lst <strong>¿lst:snippet?</strong>) are
included to make this chapter more insightful and valuable for
newcomers.</p>
<figure id="fig:timeline">
<img src="./figures/boai_timeline.svg"
alt="A brief timeline of the History of Artificial Intelligence (AI)." />
<figcaption>Figure 2: A brief timeline of the History of Artificial
Intelligence (AI).</figcaption>
</figure>
<h3 id="sec:history">A Brief History of Artificial Intelligence</h3>
<p>The history of the field of AI is not a simple linear and
straightforward story. The field had its success and failures. The term
Artificial Intelligence (AI) has first been introduced in 1956 by John
Mc Carthy and Marvin Lee Minsky at a workshop sponsored by Dartmouth
College <span class="citation" data-cites="dartmouth_2006">[<a
href="#ref-dartmouth_2006" role="doc-biblioref">50</a>]</span>,
gathering about twenty researchers and intellectuals such as the
renowned Claude Shannon (see <a href="#fig:dartmouth">Fig 3</a>). The
field’s main questions were supposed to be solved in a short period.</p>
<p>However, the reality has been far less rosy. Over the years, AI has
gone through several “winters”, periods of inactivity and disillusion
where funding was cut and research interest dropped (see <a
href="#fig:timeline">Fig 2</a>). But with the advent of Big Data and the
rise of Deep Learning (DL), AI is once again in the spotlight. The
following sections provide a brief overview of the history of AI, from
its early days to the current state of the field. For a more in-depth
look at the history of modern AI, DL, we recommend “Quand la machine
apprend” from Yann LeCun <span class="citation"
data-cites="lecun_2019">[<a href="#ref-lecun_2019"
role="doc-biblioref">41</a>]</span>.</p>
<figure id="fig:dartmouth">
<img src="./figures/boai_dartmouth.png"
alt="Photography of seven of the Dartmouth workshop participants. From left to right: John McCarthy, Marvin Lee Minsky, Nathaniel Rochester, Claude Elwood Shannon, Ray Solomonoff, Trenchard More, and Oliver Gordon Selfridge. Credit: Margaret Minksy" />
<figcaption>Figure 3: Photography of seven of the Dartmouth workshop
participants. From left to right: John McCarthy, Marvin Lee Minsky,
Nathaniel Rochester, Claude Elwood Shannon, Ray Solomonoff, Trenchard
More, and Oliver Gordon Selfridge. Credit: Margaret Minksy</figcaption>
</figure>
<h4 id="the-early-years">The Early Years</h4>
<p>The term Artificial Intelligence (AI) was first used at the 1956
Dartmouth Workshop <span class="citation"
data-cites="dartmouth_2006">[<a href="#ref-dartmouth_2006"
role="doc-biblioref">50</a>]</span>, where John McCarthy proposed the
idea of creating a machine that could learn from its mistakes and
improve its performance over time. The twenty researchers and
intellectuals present worked on topics such as the automatic computer,
the use of natural language by machines, neuron nets (Neural Network
(NN)), randomness and creativity, and many more. This was a
revolutionary idea at the time, and the work done at Dartmouth attracted
a great deal of attention and funding.</p>
<p>Much of the early research focused on symbolic AI, which uses symbols
and logical operations to represent and manipulate data. Logic
programming, production rules, semantic nets and frames, knowledge-based
systems, symbolic mathematics, automatons, automated provers, ontologies
and other paradigms were at the core of symbolic AI <span
class="citation" data-cites="russell_2016">[<a href="#ref-russell_2016"
role="doc-biblioref">65</a>]</span>. This approach was based on the
early work of Alan Turing and the development of functional languages
such as the LISP by McCarthy and al. at MIT <span class="citation"
data-cites="mccarthy_1978">[<a href="#ref-mccarthy_1978"
role="doc-biblioref">49</a>]</span>.</p>
<p>One significant contribution of this period was the Perceptron by
Frank Rosenblatt <span class="citation" data-cites="rosenblatt_1958">[<a
href="#ref-rosenblatt_1958" role="doc-biblioref">63</a>]</span>, a
simplified biomimical model of a single neuron. This artificial neuron
fires when the weighted sum of its input is above a predefined
threshold. The weights, scalars attributed to the connection edges of
the neuron’s inputs, are tuned iteratively and manually given supervised
data, inputs with corresponding labels, until good enough classification
accuracy is met.</p>
<h4 id="the-first-ai-winter">The First AI Winter</h4>
<p>The Perceptron was an early example of a connectionist approach,
which uses a network of artificial neurons to process data. The
Perceptron was met with much enthusiasm but was eventually criticized by
Marvin L. Minsky and Seymour Papert <span class="citation"
data-cites="minsky_1969">[<a href="#ref-minsky_1969"
role="doc-biblioref">51</a>]</span>, who argued that it could not solve
a simple XOR problem. The criticisms, as well as other issues, led to a
period of disillusion in the field of AI, known as the “First AI
Winter”. It was a time when AI research lost its momentum and funding
was not abundant anymore. This period lasted from 1973 to 1980.</p>
<h4 id="expert-systems-and-symbolic-ai">Expert Systems and Symbolic
AI</h4>
<p>The eighties saw a resurgence of interest in AI. Expert systems <span
class="citation" data-cites="jackson_1998">[<a href="#ref-jackson_1998"
role="doc-biblioref">32</a>]</span> were the new hot AI topic. They are
made of hierarchical and specialized ensembles of symbolic reasoning
models and are used to solve complex problems. Symbolic AI continued to
prosper as the dominant approach until the mid-nineties.</p>
<p>During this period, AI was developed as logic-based systems,
search-based systems using depth-first-search, and genetic algorithms,
requiring complex engineering and domain-specific knowledge from experts
to work. It was also the time of the first cognitive architectures <span
class="citation" data-cites="lieto_2021">[<a href="#ref-lieto_2021"
role="doc-biblioref">44</a>]</span> inspired by advances in the field of
neuroscience such as SOAR <span class="citation"
data-cites="larid_2019">[<a href="#ref-larid_2019"
role="doc-biblioref">40</a>]</span> and Adaptive Control of
Thought—Rational (ACT-R) <span class="citation"
data-cites="john_1992">[<a href="#ref-john_1992"
role="doc-biblioref">2</a>]</span> attempting at simulating the human
cognitive process for solving and task automation.</p>
<p>Although the connectionist approaches were not well received by the
community at the time, some individuals are known for significant
contributions that later would form the basis for modern NN
architectures. It was the case for Kunihiko Fukushima and his
NeoCognitron <span class="citation" data-cites="fukushima_1980">[<a
href="#ref-fukushima_1980" role="doc-biblioref">17</a>]</span>, or David
E. Rumelhart et al. who introduced the most used learning procedure for
training Multi-Layer Perceptrons (MLP), the backpropagation <span
class="citation" data-cites="rumelhart_1986">[<a
href="#ref-rumelhart_1986" role="doc-biblioref">64</a>]</span>.</p>
<h4 id="the-second-ai-winter">The Second AI Winter</h4>
<p>Unfortunately, this period was also marked by a lack of progress
because of the resource limitations of the time. Those algorithms
required too much power, data, and investments to work. They were not
sufficient to make AI truly successful. The lack of progress in the
eighties led to the “Second AI Winter”. AI research was largely
abandoned during this period. Funding and enthusiasm dwindled. This
winter lasted from 1988 to early 2000.</p>
<h4 id="the-indomitable-researchers">The Indomitable Researchers</h4>
<p>The second AI winter limited research for NN. However, some
indomitable individuals continued their work. During this period,
Vladimir Vapnik et al. developed the Support Vector Machine (SVM) <span
class="citation" data-cites="cortes_1995">[<a href="#ref-cortes_1995"
role="doc-biblioref">12</a>]</span>, a robust non-probabilistic binary
linear classifier. The method has the advantage to generalize well even
with small datasets. Sepp Hochreiter et al. introduced the Long
Short-Term Memory (LSTM) for Recurrent Neural Networks (RNN) <span
class="citation" data-cites="hochreiter_1997">[<a
href="#ref-hochreiter_1997" role="doc-biblioref">29</a>]</span>, a
complex recurrent cell using gates to route the information flow and
simulate long and short-term memory buffers. In 1989, Yann LeCun
provided the first practical and industrial demonstration of
backpropagation at Bell Labs with a Convolutional Neural Network (CNN)
to read handwritten digits <span class="citation"
data-cites="lecun_1989 lecun_1998">[<a href="#ref-lecun_1989"
role="doc-biblioref">42</a>, <a href="#ref-lecun_1998"
role="doc-biblioref">43</a>]</span> later used by the American postal
services to sort letters.</p>
<figure id="fig:revolution">
<img src="./figures/boai_revolution.svg"
alt="A brief timeline of the Deep Learning (DL) Revolution." />
<figcaption>Figure 4: A brief timeline of the Deep Learning (DL)
Revolution.</figcaption>
</figure>
<h4 id="the-deep-learning-revolution">The Deep Learning Revolution</h4>
<p>The next significant evolutionary step Deep Learning (DL), those deep
hierarchical NN, descendants of the connectionist movement, occurred in
the early twenty-first century (see <a
href="#fig:revolution">Fig 4</a>). Computers were now faster and GPUs
were developed for high compute parallelization. Data was starting to be
abundant thanks to the internet and the rapid rise of search engines and
social networks. It is the era of Big Data. NN were competing with SVM.
In 2009 Fei-Fei Li and her group launched ImageNet <span
class="citation" data-cites="deng_2009">[<a href="#ref-deng_2009"
role="doc-biblioref">13</a>]</span>, a dataset assembling billions of
labeled images.</p>
<p>By 2011, the speed of GPUs had increased significantly, making it
possible to train CNNs without layer-by-layer pre-training. The rest of
the story includes a succession of deep NN architectures including,
AlexNet <span class="citation" data-cites="krizhevsky_2012">[<a
href="#ref-krizhevsky_2012" role="doc-biblioref">39</a>]</span>, one of
the first award-winning deep CNN, ResNet <span class="citation"
data-cites="he_2016">[<a href="#ref-he_2016"
role="doc-biblioref">25</a>]</span>, introducing residual connections,
the Generative Adversarial Networks (GAN) <span class="citation"
data-cites="goodfellow_2014">[<a href="#ref-goodfellow_2014"
role="doc-biblioref">20</a>]</span>, a high fidelity and high-resolution
generative framework, attention mechanisms with the rise of the
Transformer “Attention is all you Need” architecture <span
class="citation" data-cites="vaswani_2017">[<a href="#ref-vaswani_2017"
role="doc-biblioref">72</a>]</span> present in almost all modern DL
contributions, and more recently the Denoising Diffusion Model (DDM)
<span class="citation" data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">28</a>]</span>, the spiritual autoregressive
successor of the GAN.</p>
<figure id="fig:milestones">
<img src="./figures/boai_milestones.svg"
alt="A brief timeline of the Deep Learning (DL) Milestones." />
<figcaption>Figure 5: A brief timeline of the Deep Learning (DL)
Milestones.</figcaption>
</figure>
<h4 id="deep-learning-milestones">Deep Learning Milestones</h4>
<p>DL is responsible for many AI milestones in the past decade (see <a
href="#fig:milestones">Fig 5</a>). These milestones have been essential
in advancing the field and enabling its applications within various
sectors. One of the first notable milestones was AlphaGo from DeepMind
in 2016 <span class="citation" data-cites="silver_2016">[<a
href="#ref-silver_2016" role="doc-biblioref">69</a>]</span>, where an AI
system was able to beat the Korean world champion Lee Se Dol in the game
of Go. AlphaGo is an illustration of the compression and pattern
recognition capabilities of deep NN in combination with efficient search
algorithms.</p>
<p>In 2019, AlphStar <span class="citation"
data-cites="vinyals_2019">[<a href="#ref-vinyals_2019"
role="doc-biblioref">73</a>]</span> from DeepMind also was able to
compete and defeat grandmasters in StarCraft the real-time strategy game
of Blizzard. This demonstrated the capability of Deep Learning
algorithms to achieve beyond human-level performance in real-time and
long-term planification. In 2020, AlphaFold <span class="citation"
data-cites="senior_2020">[<a href="#ref-senior_2020"
role="doc-biblioref">68</a>]</span> improved the Protein Folding
competition by quite a margin, showing that DL could be used to help
solve complex problems that have implications for medical research and
drug discovery. In 2021 a follow-up model, AlphaFold 2 <span
class="citation" data-cites="jumper_2021">[<a href="#ref-jumper_2021"
role="doc-biblioref">33</a>]</span>, was presented as an impressive
successor of AlphaFold, showcasing further advances in this field.</p>
<p>In 2021, Stable Diffusion <span class="citation"
data-cites="rombach_2021">[<a href="#ref-rombach_2021"
role="doc-biblioref">61</a>]</span> from Stability AI was released. This
Latent DDM conditioned on text prompts allows to generate images of
unprecedented quality and met unprecedented public reach. Finally,
Chat-GPT <span class="citation" data-cites="openai_2023">[<a
href="#ref-openai_2023" role="doc-biblioref">9</a>]</span> was released
in 2023 as a chatbot based on GPT3 <span class="citation"
data-cites="brown_2020">[<a href="#ref-brown_2020"
role="doc-biblioref">7</a>]</span> and fine-tuned using Reinforcement
Learning from Human Feedback (RLHF) for natural question-answering
interaction publicly available as a web demo. However, these last two
milestones are also responsible for ethical and societal concerns about
copyright, creativity, and more. This highlights both the potential of
Deep Learning algorithms but also the need for further research around
their implications.</p>
<h3 id="sec:core">Core Principles</h3>
<p>This section introduces the technical background necessary to
understand this thesis dissertation. It introduces Neural Networks (NN)
from first principles. A more detailed and complete introduction to the
field can be found in “the Deep Learning book” by Ian Goodfellow et al
<span class="citation" data-cites="goodfellow_2016">[<a
href="#ref-goodfellow_2016" role="doc-biblioref">19</a>]</span> or in
“Dive into Deep Learning” by Aston Zhang et al. <span class="citation"
data-cites="aston_zhang_2021">[<a href="#ref-aston_zhang_2021"
role="doc-biblioref">75</a>]</span>.</p>
<h4 id="supervised-learning">Supervised Learning</h4>
<p>In Machine Learning (ML), problems are often formulated as
data-driven learning tasks, where a computer is used to find a mapping
<span class="math inline">\(f: X \rightarrow Y\)</span> from input space
<span class="math inline">\(X\)</span> to output space <span
class="math inline">\(Y\)</span>. For example, <span
class="math inline">\(X\)</span> could represent data about an e-mail
and <span class="math inline">\(Y\)</span> the probability of this
e-mail being spam. In practice, manually defining all the
characteristics of a function <span class="math inline">\(f\)</span>
that would satisfy this task is considered unpractical. It would require
one to manually describe all potential rules defining spam. In ML, the
supervised framework offers a practical solution consisting of acquiring
label data pairs, <span class="math inline">\((x, y) \in X \times
Y\)</span> for the current problem (see <a
href="#fig:dataflow">Fig 6</a>). In our case, this would require
gathering a dataset of e-mails and asking humans to label those as spam
or not.</p>
<p><strong>Objective Function</strong>: Let us consider such a training
dataset containing n independent pairs <span
class="math inline">\(\{(x_1, y_1), \dots, (x_n, y_n)\}\)</span> sampled
from the data distribution <span class="math inline">\(D\)</span>, <span
class="math inline">\((x_i, y_i) \sim D\)</span>. In ML, we seek for
learning a mapping <span class="math inline">\(f: X \rightarrow
Y\)</span> by searching the space of the candidates function class <span
class="math inline">\(\mathcal{F}\)</span>. Defining a scalar objective
function <span class="math inline">\(L(\hat{y}, y)\)</span> measuring
the distance from true label <span class="math inline">\(y\)</span> and
our prediction <span class="math inline">\(f(x_i) = \hat{y}_i\)</span>
given <span class="math inline">\(f \in \mathcal{F}\)</span>, the
ultimate objective is to find the function <span
class="math inline">\(f^* \in F\)</span> that best satisfy the following
minimization problem (see <a href="#eq:f_star_objective">Eq 1</a>):</p>
<p><span id="eq:f_star_objective"><span class="math display">\[
f^* = arg \; \underset{f \in \mathcal{F}}{min} \; E_{(x, y) \sim D}
L(\hat{y}, y)
\qquad{(1)}\]</span></span></p>
<p>The function <span class="math inline">\(f^*\)</span> must minimize
the expected loss <span class="math inline">\(L\)</span> over the entire
data distribution <span class="math inline">\(D\)</span>. Once such a
function is learned one can use it to perform inference and map any
element from the input space <span class="math inline">\(X\)</span> to
the output space <span class="math inline">\(Y\)</span>.</p>
<p>However, this minimization problem is intractable as it is impossible
to represent the entire distribution <span
class="math inline">\(D\)</span>. Fortunately, as every pair <span
class="math inline">\((x_i, y_i)\)</span> is independently sampled and
identically distributed, the objective can be approximated by sampling
and minimizing the loss over the training dataset (see <a
href="#eq:f_star_objective_approx">Eq 2</a>):</p>
<p><span id="eq:f_star_objective_approx"><span class="math display">\[
f^* \approx arg \; \underset{f \in \mathcal{F}}{min} \; \frac{1}{n}
\sum_{i=1}^{n} L(\hat{y}_i, y_i)
\qquad{(2)}\]</span></span></p>
<p><strong>Regularization</strong>: While simplifying the problem allows
us to perform loss minimization, this approximation comes at a cost.
This optimization problem can have multiple solutions, a set of
functions <span class="math inline">\(\{f_1, \dots, f_m\} \in F\)</span>
performing well on the given training set, but would behave differently
outside of the training data and outside of the data distribution. Those
functions would not necessarily be able to generalize. To mitigate those
concerns, we can introduce a regularization term <span
class="math inline">\(R\)</span> into the objective function (see <a
href="#eq:f_star_objective_regul">Eq 3</a>), a scalar function that is
independent of the data distribution and represent a preference on
certain function class.</p>
<p><span id="eq:f_star_objective_regul"><span class="math display">\[
f^* \approx arg \; \underset{f \in \mathcal{F}}{min} \; \frac{1}{n}
\sum_{i=1}^{n} L(\hat{y}_i, y_i) + R(f)
\qquad{(3)}\]</span></span></p>
<figure id="fig:dataflow">
<img src="./figures/core_nn_dataflow.svg"
alt="Supervised learning data flow. The dataset {(x_i, y_i)} \in D is used to train the model f \in \mathcal{F} to minimize an objective function with two terms, a data dependant loss L, and a regularization R measuring the system complexity." />
<figcaption>Figure 6: Supervised learning data flow. The dataset <span
class="math inline">\({(x_i, y_i)} \in D\)</span> is used to train the
model <span class="math inline">\(f \in \mathcal{F}\)</span> to minimize
an objective function with two terms, a data dependant loss <span
class="math inline">\(L\)</span>, and a regularization <span
class="math inline">\(R\)</span> measuring the system
complexity.</figcaption>
</figure>
<p>In the following, we investigate two examples where supervised
learning is first applied to a Neural Network (NN) regression problem,
and then a NN classification problem. The examples highlight the
objective functions composed by the loss and the regularization term for
regression and classification respectively.</p>
<p><strong>Regression Problem:</strong> Let us consider the distribution
<span class="math inline">\(D\)</span> represented by the <span
class="math inline">\(sin\)</span> function in the <span
class="math inline">\([-3 \pi; 3 \pi]\)</span> range (see <a
href="#fig:regression">Fig 7</a>). We sample <span
class="math inline">\(50\)</span> pairs <span
class="math inline">\((x_i, y_i)\)</span> with <span
class="math inline">\(X \in [-3 \pi; 3 \pi]\)</span> and <span
class="math inline">\(Y \in [-1; 1]\)</span>. Our objective is to learn
a regressor <span class="math inline">\(f_\theta\)</span>, a three
layers NN parametrized by its weights <span class="math inline">\(\{w_0,
W_1, w_2\} = \theta\)</span>. <span class="math inline">\(w_0\)</span>
contains <span class="math inline">\((1 \times 16) + 1\)</span> weights,
<span class="math inline">\(W_1\)</span>, <span
class="math inline">\((16 \times 16) + 1\)</span>, and <span
class="math inline">\(w_2\)</span>, <span class="math inline">\((16
\times 1) + 1\)</span>. In this case, the function space is limited to
the three layers NN family with <span class="math inline">\(291\)</span>
parameters <span class="math inline">\(\mathcal{F}\)</span>.</p>
<figure id="fig:regression">
<img src="./figures/core_nn_regression.svg"
alt="Neural Network (NN) regression example. The model f_\theta is fit on the training set (X, Y) \in D representing the sin function in the range [-3 \pi; 3 \pi]." />
<figcaption>Figure 7: Neural Network (NN) regression example. The model
<span class="math inline">\(f_\theta\)</span> is fit on the training set
<span class="math inline">\((X, Y) \in D\)</span> representing the <span
class="math inline">\(sin\)</span> function in the range <span
class="math inline">\([-3 \pi; 3 \pi]\)</span>.</figcaption>
</figure>
<p>To achieve this goal using supervised learning, we can optimize the
following objective function (see <a
href="#eq:reg_sin_objective">Eq 4</a>):</p>
<p><span id="eq:reg_sin_objective"><span class="math display">\[
f^* = arg \; \underset{\theta}{min} \; \frac{1}{n} \sum_{i=1}^{n}
(f_\theta(x_i) - y_i)^2 + \lambda ||\theta||_2^2
\qquad{(4)}\]</span></span></p>
<p>where the loss is the Mean Squared Error (MSE) <span
class="math inline">\(||.||_2^2\)</span> between the ground-truth <span
class="math inline">\(y_i\)</span> and the prediction <span
class="math inline">\(\hat{y_i} = f_\theta(x_i)\)</span>, and the
weighted regularization term <span class="math inline">\(\lambda
||\theta||_2^2\)</span> to penalize the model for having large weights
and converge to a simpler solution. A python code snippet for the
objective function and the model is provided below (see
Lst <strong>¿lst:regression?</strong>):</p>
<div class="sourceCode" id="lst:regression"><pre
class="sourceCode python"><code class="sourceCode python"><span id="lst:regression-1"><a href="#lst:regression-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> (Linear, Sequential, Tanh)</span>
<span id="lst:regression-2"><a href="#lst:regression-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:regression-3"><a href="#lst:regression-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss and Regularization</span></span>
<span id="lst:regression-4"><a href="#lst:regression-4" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> <span class="kw">lambda</span> y_, y <span class="op">=</span> (y_ <span class="op">-</span> y).<span class="bu">pow</span>(<span class="dv">2</span>)</span>
<span id="lst:regression-5"><a href="#lst:regression-5" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> <span class="kw">lambda</span> f: <span class="bu">sum</span>(w.<span class="bu">pow</span>(<span class="dv">2</span>).<span class="bu">sum</span>() <span class="cf">for</span> w <span class="kw">in</span> f.parameters())</span>
<span id="lst:regression-6"><a href="#lst:regression-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:regression-7"><a href="#lst:regression-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Neural Network model</span></span>
<span id="lst:regression-8"><a href="#lst:regression-8" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> Sequential(</span>
<span id="lst:regression-9"><a href="#lst:regression-9" aria-hidden="true" tabindex="-1"></a>    Linear(<span class="dv">1</span>, <span class="dv">16</span>), Tanh(),</span>
<span id="lst:regression-10"><a href="#lst:regression-10" aria-hidden="true" tabindex="-1"></a>    Linear(<span class="dv">16</span>, <span class="dv">16</span>), Tanh(),</span>
<span id="lst:regression-11"><a href="#lst:regression-11" aria-hidden="true" tabindex="-1"></a>    Linear(<span class="dv">16</span>, <span class="dv">1</span>),</span>
<span id="lst:regression-12"><a href="#lst:regression-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="lst:regression-13"><a href="#lst:regression-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:regression-14"><a href="#lst:regression-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Objective function</span></span>
<span id="lst:regression-15"><a href="#lst:regression-15" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> (<span class="dv">1</span> <span class="op">/</span> n) <span class="op">*</span> L(f(X), Y).<span class="bu">sum</span>() <span class="op">+</span> lam <span class="op">*</span>  R(f)</span></code></pre></div>
<p><strong>Classification Problem:</strong> Let us consider the
distribution <span class="math inline">\(D\)</span> representing the 2d
positions of two clusters <span class="math inline">\({0, 1} \in
K\)</span> of moons (see <a href="#fig:classification">Fig 8</a>). We
sample <span class="math inline">\(250\)</span> moon <span
class="math inline">\((x_i, y_i)\)</span> with <span
class="math inline">\(X \in [-1; 1]\)</span> and <span
class="math inline">\(Y \in [-1; 1]\)</span>. Our objective is to learn
a classifier <span class="math inline">\(f_\theta\)</span>, a three
layers NN parametrized by its weights <span class="math inline">\(\{w_0,
W_1, w_2\} = \theta\)</span>. <span class="math inline">\(w_0\)</span>
contains <span class="math inline">\((1 \times 32) + 1\)</span> weights,
<span class="math inline">\(W_1\)</span>, <span
class="math inline">\((32 \times 32) + 1\)</span>, and <span
class="math inline">\(w_2\)</span>, <span class="math inline">\((32
\times 1) + 1\)</span>. In this case, the function space is limited to
the three layers NN family with <span
class="math inline">\(1,091\)</span> parameters <span
class="math inline">\(\mathcal{F}\)</span>.</p>
<figure id="fig:classification">
<img src="./figures/core_nn_classification.svg"
alt="Neural Network (NN) classification example. The model f_\theta is trained to classify moons based on their positions. The decision boundary is shown." />
<figcaption>Figure 8: Neural Network (NN) classification example. The
model <span class="math inline">\(f_\theta\)</span> is trained to
classify moons based on their positions. The decision boundary is
shown.</figcaption>
</figure>
<p>To achieve this goal using supervised learning, we can optimize an
objective function similar to the regression problem (see <a
href="#eq:reg_sin_objective">Eq 4</a>) using the cross-entropy as the
loss function (see <a href="#eq:cross_entropy">Eq 5</a>), measuring the
classification discordance.</p>
<p><span id="eq:cross_entropy"><span class="math display">\[
\mathcal{L} (\hat{y}, y) = \sum_{k=1}^{K} y_k \; log \; \hat{y}_k
\qquad{(5)}\]</span></span></p>
<p>A python code snippet for the loss function and the model is provided
below (see Lst <strong>¿lst:classification?</strong>):</p>
<div class="sourceCode" id="lst:classification"><pre
class="sourceCode python"><code class="sourceCode python"><span id="lst:classification-1"><a href="#lst:classification-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> (Linear, Sequential, Tanh)</span>
<span id="lst:classification-2"><a href="#lst:classification-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.functional <span class="im">import</span> cross_entropy</span>
<span id="lst:classification-3"><a href="#lst:classification-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:classification-4"><a href="#lst:classification-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Loss</span></span>
<span id="lst:classification-5"><a href="#lst:classification-5" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> <span class="kw">lambda</span> y_, y <span class="op">=</span> cross_entropy(y_, y, <span class="bu">reduce</span><span class="op">=</span><span class="va">False</span>)</span>
<span id="lst:classification-6"><a href="#lst:classification-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:classification-7"><a href="#lst:classification-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Neural Network model</span></span>
<span id="lst:classification-8"><a href="#lst:classification-8" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> Sequential(</span>
<span id="lst:classification-9"><a href="#lst:classification-9" aria-hidden="true" tabindex="-1"></a>    Linear(<span class="dv">1</span>, <span class="dv">32</span>), Tanh(),</span>
<span id="lst:classification-10"><a href="#lst:classification-10" aria-hidden="true" tabindex="-1"></a>    Linear(<span class="dv">32</span>, <span class="dv">32</span>), Tanh(),</span>
<span id="lst:classification-11"><a href="#lst:classification-11" aria-hidden="true" tabindex="-1"></a>    Linear(<span class="dv">32</span>, <span class="dv">1</span>),</span>
<span id="lst:classification-12"><a href="#lst:classification-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<h4 id="sec:optimization">Optimization</h4>
<p>In ML, supervised problems can be reduced to an optimization problem
where the computer has to find a set of parameters, weights <span
class="math inline">\(\theta\)</span>, for a given function class <span
class="math inline">\(\mathcal{F}\)</span> by optimizing an objective
function <span class="math inline">\(\theta^* = arg \; min_\theta
\mathcal{C(\theta)}\)</span> made out of two components, a
data-dependant loss <span class="math inline">\(L\)</span> and a
regularization <span class="math inline">\(R\)</span>.</p>
<p><strong>Random Search:</strong> One way to find such a function <span
class="math inline">\(f_\theta\)</span> that satisfies this objective is
to estimate the objective function for a set of random parameter
initializations and take the one that minimizes <span
class="math inline">\(C\)</span> the most. This <span
class="math inline">\(\theta\)</span> setting can then be refined by
applying random perturbations to the parameters and repeating the
operation (see Lst <strong>¿lst:random_search?</strong>). This is
possible due to the fact that we can computer <span
class="math inline">\(C(\theta)\)</span> for any value of <span
class="math inline">\(\theta\)</span> taking the average loss for a
given dataset. However, such an approach to optimization is unpractical.
NN often comes with millions or billions of parameters <span
class="math inline">\(\theta\)</span> making random-search
intractable.</p>
<div class="sourceCode" id="lst:random_search"><pre
class="sourceCode python"><code class="sourceCode python"><span id="lst:random_search-1"><a href="#lst:random_search-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="lst:random_search-2"><a href="#lst:random_search-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="lst:random_search-3"><a href="#lst:random_search-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:random_search-4"><a href="#lst:random_search-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:random_search-5"><a href="#lst:random_search-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(steps):</span>
<span id="lst:random_search-6"><a href="#lst:random_search-6" aria-hidden="true" tabindex="-1"></a>    fs, os <span class="op">=</span> [f] <span class="op">+</span> [copy.deepcopy(f) <span class="cf">for</span> f <span class="kw">in</span> <span class="bu">range</span>(n_copy)], []</span>
<span id="lst:random_search-7"><a href="#lst:random_search-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> f_ <span class="kw">in</span> fs:</span>
<span id="lst:random_search-8"><a href="#lst:random_search-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply weight perturbation</span></span>
<span id="lst:random_search-9"><a href="#lst:random_search-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> w <span class="kw">in</span> f_.parameters():</span>
<span id="lst:random_search-10"><a href="#lst:random_search-10" aria-hidden="true" tabindex="-1"></a>            w.normal_(<span class="fl">0.0</span>, <span class="fl">1.0</span> <span class="op">/</span> step)</span>
<span id="lst:random_search-11"><a href="#lst:random_search-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Estimate the objective function</span></span>
<span id="lst:random_search-12"><a href="#lst:random_search-12" aria-hidden="true" tabindex="-1"></a>        os.append(C(f_(X), Y))</span>
<span id="lst:random_search-13"><a href="#lst:random_search-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="lst:random_search-14"><a href="#lst:random_search-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Retrieve the winner</span></span>
<span id="lst:random_search-15"><a href="#lst:random_search-15" aria-hidden="true" tabindex="-1"></a>    f <span class="op">=</span> fs[np.argmax(os)]</span></code></pre></div>
<p><strong>First Order Derivation:</strong> A more efficient approach is
to make the objective function <span class="math inline">\(C\)</span>
and the model <span class="math inline">\(f_\theta\)</span>
differentiable. This constraint allows us to compute the gradient of the
cost <span class="math inline">\(C\)</span> with respect to the model’s
parameters <span class="math inline">\(\theta\)</span>. The value <span
class="math inline">\(\nabla_\theta C\)</span> can be obtained using
backpropagation (discussed in the next sub-section <a
href="#sec:backpropagation">Sec 2.2.2.3</a>). This vector of first order
derivatives indicates the direction from which we need to move the
weights <span class="math inline">\(\theta\)</span> away. By taking
small iterative steps toward the negative direction of the gradients, we
can improve <span class="math inline">\(\theta\)</span>. This algorithm
is called GD. In practice, due to the very large size of the datasets
(<span class="math inline">\(14,197,122\)</span> images for ImageNet
<span class="citation" data-cites="deng_2009">[<a href="#ref-deng_2009"
role="doc-biblioref">13</a>]</span>), the objective gradient is
approximated using a small subset of the training data for each step
referred to as a minibatch. This approximation of the GD is called
Stochastic Gradient Descent (SGD) (see
Lst <strong>¿lst:sgd?</strong>).</p>
<div class="sourceCode" id="lst:sgd"><pre
class="sourceCode python"><code class="sourceCode python"><span id="lst:sgd-1"><a href="#lst:sgd-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1_000</span>):</span>
<span id="lst:sgd-2"><a href="#lst:sgd-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Retrieve the next minibatch</span></span>
<span id="lst:sgd-3"><a href="#lst:sgd-3" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> next_minibatch(X, Y)</span>
<span id="lst:sgd-4"><a href="#lst:sgd-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:sgd-5"><a href="#lst:sgd-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the objective function and the gradients</span></span>
<span id="lst:sgd-6"><a href="#lst:sgd-6" aria-hidden="true" tabindex="-1"></a>    C <span class="op">=</span> L(f(x), y) <span class="op">+</span> lam <span class="op">*</span> R(f)</span>
<span id="lst:sgd-7"><a href="#lst:sgd-7" aria-hidden="true" tabindex="-1"></a>    C.backward()</span>
<span id="lst:sgd-8"><a href="#lst:sgd-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:sgd-9"><a href="#lst:sgd-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update the weights and reset the gradients</span></span>
<span id="lst:sgd-10"><a href="#lst:sgd-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> w <span class="kw">in</span> f.parameters():</span>
<span id="lst:sgd-11"><a href="#lst:sgd-11" aria-hidden="true" tabindex="-1"></a>        w <span class="op">-=</span> eps <span class="op">*</span> w.grad</span>
<span id="lst:sgd-12"><a href="#lst:sgd-12" aria-hidden="true" tabindex="-1"></a>    f.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>One critical aspect of the SGD algorithm is the hyperparameter <span
class="math inline">\(\epsilon\)</span>, the learning rate. It controls
the size of the step we take toward the negative gradients. If it is too
height or too low, the optimization may not converge toward an
acceptable local minimum. A toy example is provided in <a
href="#fig:toysgd">Fig 9</a> where different learning rates are used to
find the minimum of the square function <span class="math inline">\(y =
x^2\)</span>.</p>
<figure id="fig:toysgd">
<img src="./figures/core_nn_sgd.svg"
alt="Toy example where different learning rates \epsilon are used to find the minimum of the square function y = x^2 using the Gradient Descent (GD) algorithm starting from x = -1. Some learning rate setup result in situations where the optimization does not converge to the solution. A learning rate \epsilon = 2 diverges toward infinity, \epsilon = 1 is stuck and bounces between two positions -1 and 1. However, a small learning rate \epsilon = 0.1 &lt; 1 converges towards the minimum y = 0. This example illustrates the impact of the hyperparameter \epsilon on GD." />
<figcaption>Figure 9: Toy example where different learning rates <span
class="math inline">\(\epsilon\)</span> are used to find the minimum of
the square function <span class="math inline">\(y = x^2\)</span> using
the Gradient Descent (GD) algorithm starting from <span
class="math inline">\(x = -1\)</span>. Some learning rate setup result
in situations where the optimization does not converge to the solution.
A learning rate <span class="math inline">\(\epsilon = 2\)</span>
diverges toward infinity, <span class="math inline">\(\epsilon =
1\)</span> is stuck and bounces between two positions <span
class="math inline">\(-1\)</span> and <span
class="math inline">\(1\)</span>. However, a small learning rate <span
class="math inline">\(\epsilon = 0.1 &lt; 1\)</span> converges towards
the minimum <span class="math inline">\(y = 0\)</span>. This example
illustrates the impact of the hyperparameter <span
class="math inline">\(\epsilon\)</span> on GD.</figcaption>
</figure>
<p><strong>First Order Derivation with Momentum:</strong> The DL
literature contains abundant work on first order optimizer variants
aiming for faster convergence such as SGD with Momentum <span
class="citation" data-cites="qian_1999">[<a href="#ref-qian_1999"
role="doc-biblioref">60</a>]</span>, Adagrad <span class="citation"
data-cites="duchi_2011">[<a href="#ref-duchi_2011"
role="doc-biblioref">15</a>]</span>, RMSProp <span class="citation"
data-cites="hinton_lecture6a">[<a href="#ref-hinton_lecture6a"
role="doc-biblioref">27</a>]</span>, Adam <span class="citation"
data-cites="kingma_2014">[<a href="#ref-kingma_2014"
role="doc-biblioref">37</a>]</span>, and its correction AdamW <span
class="citation" data-cites="loshchilov_2017">[<a
href="#ref-loshchilov_2017" role="doc-biblioref">47</a>]</span>. A toy
example is shown <a href="#fig:sgd_moments">Fig 10</a>.</p>
<p>The Momentum update <span class="citation" data-cites="qian_1999">[<a
href="#ref-qian_1999" role="doc-biblioref">60</a>]</span> introduces the
use of a momentum inspired by physics’ first principles to favor small
and consistent gradient directions. In this particular case, the
momentum is represented by a variable <span
class="math inline">\(v\)</span> updated to store an exponential
decaying sum of the previous gradients <span class="math inline">\(v :=
\alpha v + \nabla_\theta C(\theta)\)</span>. The weights are then
updated using negative <span class="math inline">\(v\)</span> as the
gradient direction instead of <span class="math inline">\(\nabla_\theta
C(\theta)\)</span>.</p>
<p>Other optimizers also make use of the second moment of the gradients.
Adagrad <span class="citation" data-cites="duchi_2011">[<a
href="#ref-duchi_2011" role="doc-biblioref">15</a>]</span> uses another
variable <span class="math inline">\(r\)</span> to store the second
moment <span class="math inline">\(r := r + \nabla_\theta C(\theta)
\odot \nabla_\theta C(\theta)\)</span> and modulate the update rule
toward the negative direction <span
class="math inline">\(\frac{1}{\delta + \sqrt{r}} \odot \nabla_\theta
C(\theta)\)</span> where <span class="math inline">\(\delta\)</span> is
a small value to avoid division by zero. Similarly, RMSProp <span
class="citation" data-cites="hinton_lecture6a">[<a
href="#ref-hinton_lecture6a" role="doc-biblioref">27</a>]</span>
maintains a running mean of the second moment <span
class="math inline">\(r := \rho r + (1 - \rho) \nabla_\theta C(\theta)
\odot \nabla_\theta C(\theta)\)</span>.</p>
<p>Finally Adam <span class="citation" data-cites="kingma_2014">[<a
href="#ref-kingma_2014" role="doc-biblioref">37</a>]</span>, and its
correction AdamW <span class="citation" data-cites="loshchilov_2017">[<a
href="#ref-loshchilov_2017" role="doc-biblioref">47</a>]</span>, are
applying both Momentum and RMSProp estimating the first and second
moment to make parameters with large gradients take small steps and
parameters with low gradients take larger ones. This has the advantage
to allow for bigger learning rates and faster convergence at the cost of
triple the amount of parameters to store during training. A simple
implementation of Adam is shown below (see
Lst <strong>¿lst:adam?</strong>):</p>
<!-- - Adagrad, RMSProp, AdamW
- Adam: Big Gradient = Small Steps, Small Gradient == Big Steps -->
<div class="sourceCode" id="lst:adam"><pre
class="sourceCode python"><code class="sourceCode python"><span id="lst:adam-1"><a href="#lst:adam-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Adam state (parameters, gradients first and second moments)</span></span>
<span id="lst:adam-2"><a href="#lst:adam-2" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> <span class="bu">list</span>(f.parameters())</span>
<span id="lst:adam-3"><a href="#lst:adam-3" aria-hidden="true" tabindex="-1"></a>d_means <span class="op">=</span> [w.clone().zeros_() <span class="cf">for</span> w <span class="kw">in</span> params]</span>
<span id="lst:adam-4"><a href="#lst:adam-4" aria-hidden="true" tabindex="-1"></a>d_vars  <span class="op">=</span> [w.clone().zeros_() <span class="cf">for</span> w <span class="kw">in</span> params]</span>
<span id="lst:adam-5"><a href="#lst:adam-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:adam-6"><a href="#lst:adam-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1_000</span>):</span>
<span id="lst:adam-7"><a href="#lst:adam-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Retrieve the next minibatch</span></span>
<span id="lst:adam-8"><a href="#lst:adam-8" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> next_minibatch(X, Y)</span>
<span id="lst:adam-9"><a href="#lst:adam-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:adam-10"><a href="#lst:adam-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the objective function and the gradients</span></span>
<span id="lst:adam-11"><a href="#lst:adam-11" aria-hidden="true" tabindex="-1"></a>    C <span class="op">=</span> L(f(x), y) <span class="op">+</span> lam <span class="op">*</span> R(f)</span>
<span id="lst:adam-12"><a href="#lst:adam-12" aria-hidden="true" tabindex="-1"></a>    C.backward()</span>
<span id="lst:adam-13"><a href="#lst:adam-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:adam-14"><a href="#lst:adam-14" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> <span class="bu">zip</span>(params, d_means, d_vars)</span>
<span id="lst:adam-15"><a href="#lst:adam-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> w_idx, (w, d_m, d_v) <span class="kw">in</span> <span class="bu">enumerate</span>(data):</span>
<span id="lst:adam-16"><a href="#lst:adam-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update the moments (mean and uncentered variance)</span></span>
<span id="lst:adam-17"><a href="#lst:adam-17" aria-hidden="true" tabindex="-1"></a>        d_m <span class="op">=</span> beta1 <span class="op">*</span> d_m <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta1) <span class="op">*</span> w.grad</span>
<span id="lst:adam-18"><a href="#lst:adam-18" aria-hidden="true" tabindex="-1"></a>        d_v <span class="op">=</span> beta2 <span class="op">*</span> d_v <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta2) <span class="op">*</span> (w.grad <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="lst:adam-19"><a href="#lst:adam-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:adam-20"><a href="#lst:adam-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute bias correction</span></span>
<span id="lst:adam-21"><a href="#lst:adam-21" aria-hidden="true" tabindex="-1"></a>        corr_m <span class="op">=</span> d_m <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> beta1 <span class="op">**</span> step)</span>
<span id="lst:adam-22"><a href="#lst:adam-22" aria-hidden="true" tabindex="-1"></a>        corr_v <span class="op">=</span> d_v <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> beta2 <span class="op">**</span> step)</span>
<span id="lst:adam-23"><a href="#lst:adam-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:adam-24"><a href="#lst:adam-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update weight and reset the gradient</span></span>
<span id="lst:adam-25"><a href="#lst:adam-25" aria-hidden="true" tabindex="-1"></a>        w <span class="op">-=</span> eps <span class="op">*</span> (corr_m <span class="op">/</span> (corr_v.sqrt() <span class="op">+</span> <span class="fl">1e-8</span>))</span>
<span id="lst:adam-26"><a href="#lst:adam-26" aria-hidden="true" tabindex="-1"></a>        w.grad <span class="op">=</span> <span class="va">None</span></span></code></pre></div>
<figure id="fig:sgd_moments">
<img src="./figures/core_nn_sgd_moments.svg"
alt="This toy example illustrates the impact of the optimizer choice during objective minimization with first order methods. SGD, Momentum, Adagrad, RMSProp and Adam are tasked to find the minimum of a 1-dimensional mixture of Gaussians given the same starting point x = 1 and the same learning rate \epsilon = 0.5. In this particular setup, Moments and Adagrad find the solution, RMSProp explodes, and SGD and Adam are stuck into a local minimum." />
<figcaption>Figure 10: This toy example illustrates the impact of the
optimizer choice during objective minimization with first order methods.
SGD, Momentum, Adagrad, RMSProp and Adam are tasked to find the minimum
of a 1-dimensional mixture of Gaussians given the same starting point
<span class="math inline">\(x = 1\)</span> and the same learning rate
<span class="math inline">\(\epsilon = 0.5\)</span>. In this particular
setup, Moments and Adagrad find the solution, RMSProp explodes, and SGD
and Adam are stuck into a local minimum.</figcaption>
</figure>
<p><strong>Cross-Validation and HyperParameter Search:</strong> As
illustrated by the toy examples (see Figs <a href="#fig:toysgd">9</a>,
<a href="#fig:sgd_moments">10</a>), the training of NN using SGD is
highly dependent on the initial setting of hyperparameters. One could
ask if there is a rule for choosing such parameters. Unfortunately, this
is not the case. The field is highly empirical and driven by exploration
using the scientific method.</p>
<p>One common approach is to set up metrics to evaluate the performance
of the model during the optimization process. It is a good practice to
divide the dataset into validation folds that are different from the
training data to evaluate the generalization capabilities of the model.
This practice is referred to as <span
class="math inline">\(k\)</span>-fold cross-validation and is most of
the time in DL, because of the large datasets, reduced to a single fold,
called the validation set. By defining such a process, NN can be
compared in a controlled manner and the hyperparameter space can be
searched. Hyperparameter search is so important that it is a subfield of
its own. The broad DL literature however contains many examples of
initial parameters and architectures that can be used to bootstrap this
search.</p>
<h4 id="sec:backpropagation">Backpropagation</h4>
<p>In the previous sub-section (see <a
href="#sec:optimization">Sec 2.2.2.2</a>), we saw how to learn
parametrized functions <span class="math inline">\(f_\theta\)</span>
given a training dataset. By evaluating the gradients of the objective
function with respect to the model’s parameters, it is possible to
obtain a good enough mapping <span class="math inline">\(f_\theta: X
\rightarrow Y\)</span>. In this sub-section, we discuss backpropagation,
the recursive algorithm used to efficiently compute those gradients
exploiting the chain rule <span class="math inline">\(\frac{\partial
z}{\partial x} = \frac{\partial z}{\partial y} \cdot \frac{\partial
y}{\partial x}\)</span> with <span class="math inline">\(z\)</span>
dependant on <span class="math inline">\(y\)</span> and <span
class="math inline">\(y\)</span> on <span
class="math inline">\(x\)</span>.</p>
<figure id="fig:dag">
<img src="./figures/core_nn_dag.svg"
alt="Illustration of reverse mode Automatic Differentiation (AD). This Directed Acyclic Graph (DAG) shows the forward pass in green and backward in red. The gradient of an activation is computed by multiplying the local gradient of a node by its output gradient computed in the previous step when following backward differentiation \frac{\partial C}{\partial x} = \frac{\partial z}{\partial x} \cdot \frac{\partial C}{\partial z} where \frac{\partial z}{\partial x} is the location derivative and \frac{\partial C}{\partial z} the output one." />
<figcaption>Figure 11: Illustration of reverse mode Automatic
Differentiation (AD). This Directed Acyclic Graph (DAG) shows the
forward pass in green and backward in red. The gradient of an activation
is computed by multiplying the local gradient of a node by its output
gradient computed in the previous step when following backward
differentiation <span class="math inline">\(\frac{\partial C}{\partial
x} = \frac{\partial z}{\partial x} \cdot \frac{\partial C}{\partial
z}\)</span> where <span class="math inline">\(\frac{\partial z}{\partial
x}\)</span> is the location derivative and <span
class="math inline">\(\frac{\partial C}{\partial z}\)</span> the output
one.</figcaption>
</figure>
<p><strong>Automatic Differentiation:</strong> In mathematics, AD
describes the set of techniques used to evaluate the derivative of a
function and exploits the fact that any complex computation can be
transformed into a sequence of elementary operations and functions with
known symbolic derivatives. By applying the chain rule recursively to
this sequence of operations, one can automatically compute the
derivatives with precision at the cost of storage.</p>
<p>We distinguish two modes of operation for AD, forward mode
differentiation, and reverse mode differentiation. In forward mode, the
derivatives are computed after applying each elementary operation and
function in order using the chain rule. It requires storing the
gradients along the way and carrying them until the last computation.
This mode is preferred when the size of the outputs exceeds the size of
the inputs. This is generally not the case for NN where the input, an
image for example, is larger than the output, a scalar for the objective
function. On the opposite, reverse mode differentiation traverses the
sequence of operations from end to start using the chain rule and
requires storing the output of the operations instead. This method is
preferred when the size of the inputs exceeds the outputs. This mode
thus has to happen in two passes, a forward pass where one computes the
output of every operation in the order, and a backward pass, where the
sequence of operations is traversed in backward order to compute the
derivatives.</p>
<p><strong>Computation Graph:</strong> A NN can be defined as a
succession of linear transformations followed by non-linear activations
(discussed in the next section <a href="#sec:nn">Sec 2.2.3</a>). Those
elementary operations are differentiable and when thinking of the data
flow can be viewed as a computation DAG to which backpropagation,
reverse mode differentiation, can be applied.</p>
<p>In modern DL frameworks <span class="citation"
data-cites="pytorch tensorflow">[<a href="#ref-tensorflow"
role="doc-biblioref">1</a>, <a href="#ref-pytorch"
role="doc-biblioref">57</a>]</span>, the AD is centered on the
implementation of a Graph object with Nodes. Both entities possess a
<code>forward()</code> and a <code>backward()</code> function. The
forward pass calls the <code>forward()</code> function of each node of
the graph by traversing it in order while saving the node output for
differentiation. The backward pass traverses the graph recursively in
backward order calling the <code>backward()</code> function responsible
for computing the local gradient of the node operation and multiplying
it by its output gradient following the chain rule. Nodes are in most
frameworks referred to as Layers, the elementary building block of the
NN operation chain.</p>
<p><strong>Toy Implementation:</strong> Here is a simple implementation
of such a computation graph for backpropagation and AD engines adapted
from Micrograd by Andrej Karpathy <span class="citation"
data-cites="karpathy_micrograd">[<a href="#ref-karpathy_micrograd"
role="doc-biblioref">35</a>]</span>. The Node class is responsible for
storing the value, the chained gradient, and additional information to
trace the graph for the backward pass.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Node:</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    value: <span class="bu">float</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    grad: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    _backward <span class="op">=</span> <span class="kw">lambda</span>: <span class="va">None</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    _children: <span class="bu">set</span>[Node] <span class="op">=</span> {}</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    _op <span class="op">=</span> <span class="st">&quot;&quot;</span></span></code></pre></div>
<p>The Node can then be populated with elementary operations
(<code>__add__</code>, <code>__mul__</code>) and functions
(<code>tanh</code>).</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Node:</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__add__</span>(<span class="va">self</span>, other: Node) <span class="op">-&gt;</span> Node:</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> Node(<span class="va">self</span>.value <span class="op">+</span> other.value, {<span class="va">self</span>, other}, <span class="st">&quot;+&quot;</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _backward() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad <span class="op">+=</span> out.grad</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>            other.grad <span class="op">+=</span> out.grad</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__mul__</span>(<span class="va">self</span>, other: Node) <span class="op">-&gt;</span> Node:</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> Node(<span class="va">self</span>.value <span class="op">*</span> other.value, {<span class="va">self</span>, other}, <span class="st">&quot;*&quot;</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _backward() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad <span class="op">+=</span> other.value <span class="op">*</span> out.grad</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>            other.grad <span class="op">+=</span> <span class="va">self</span>.value <span class="op">*</span> out.grad</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> tanh(<span class="va">self</span>) <span class="op">-&gt;</span> Node:</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        act <span class="op">=</span> np.tanh(<span class="va">self</span>.value)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> Node(act, {<span class="va">self</span>}, <span class="st">&quot;tanh&quot;</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> _backward() <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.grad <span class="op">+=</span> (<span class="fl">1.0</span> <span class="op">-</span> act <span class="op">**</span> <span class="dv">2</span>) <span class="op">*</span> out.grad</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>        out._backward <span class="op">=</span> _backward</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code></pre></div>
<p>Every elementary transformation needs to be differentiable and
implements its own backward function using the chain rule. The chained
gradient stored in the node is the multiplication of the local gradient
with its output gradient computed when the parent node is encountered
during the backward pass. The Node object needs to be extended with
support for other elementary operations (e.g. <code>__pow__</code>,
<code>__neg__</code>) and functions (e.g. <code>sigmoid</code>,
<code>relu</code>) to be useful for DL.</p>
<p>We add the ability for a Node to compute its backward pass by first
tracing all the current DAG operations recursively. The gradients can
then be computed by initializing the first node (the last in the graph)
gradient to <span class="math inline">\(1\)</span>. The backward call on
the graph iteratively traverses the graph from end to start and applies
the inner backward functions to compute the chain gradients along the
way while storing them in their respective Node object.</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Node:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> backward(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        trace, visited <span class="op">=</span> [], {}</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> trace_graph(node: Node) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> node <span class="kw">not</span> <span class="kw">in</span> visited:</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>                visited.add(node)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> child <span class="kw">in</span> node._children:</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>                    trace_graph(child)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>                trace.append(node)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        trace_graph(<span class="va">self</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.grad <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> node <span class="kw">in</span> trace[::<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            node._backward()</span></code></pre></div>
<p>The simple Automatic Differentiation (AD) engine is now ready to
perform forward and backward passes. The gradients stored in the node
can then be used for Stochastic Gradient Descent (SGD) to update the
weights of a Neural Network (NN) for example.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>w1, w2 <span class="op">=</span> Node(<span class="fl">0.1</span>), Node(<span class="fl">0.2</span>)  <span class="co"># Weights</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>a,  b  <span class="op">=</span> Node(<span class="fl">1.0</span>), Node(<span class="fl">0.0</span>)  <span class="co"># Inputs</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> (w1 <span class="op">*</span> a <span class="op">+</span> w2 <span class="op">*</span> b).tanh()   <span class="co"># Eager forward pass</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>z.backward()                   <span class="co"># Backward pass</span></span></code></pre></div>
<p>Fortunatly open-source implementations of such engines are already
available and extensively used by the DL community. They have the
adantage to work at the Tensor level, not at the Scalar level like
Micrograd, and offer support for accelerated hardware such as Graphical
Processing Units (GPU), Tensor Processing Units (TPU), and Neural
Processing Units (NPU). In this dissertation, most examples are using
the PyTorch <span class="citation" data-cites="pytorch">[<a
href="#ref-pytorch" role="doc-biblioref">57</a>]</span> framework, a
Python Tensor library written in C++ and equipped with a powerful eager
mode reverse AD engine.</p>
<p><strong>Eager or Graph Execution:</strong> Modern DL frameworks such
as PyTorch <span class="citation" data-cites="pytorch">[<a
href="#ref-pytorch" role="doc-biblioref">57</a>]</span> and Tensorflow
<span class="citation" data-cites="tensorflow">[<a
href="#ref-tensorflow" role="doc-biblioref">1</a>]</span> now propose
two execution modes. An eager mode, where the graph is built dynamically
and operations are applied immediately, and a graph mode where the
computational graph has to be defined beforehand. Both modes come with
advantages and inconveniences. Eager mode is useful for iterative
development and provides an intuitive interface similar to imperative
programming, it is easier to debug and offers natural control flows as
well as hardware acceleration support. On the other side, graph mode
allows for more efficient execution. The graph can be optimized by
applying operations similar to the ones used in programming language
Abstract Syntax Trees (AST). Graph edges can be merged into a single
fused operation, and execution can be optimized for parallelization. It
is often the preferred way for deployment where the execution time and
memory are at stake.</p>
<h3 id="sec:nn">Neural Networks</h3>
<p>In the previous section, we described the general setup for ML, where
one has to fit a model from a given function family <span
class="math inline">\(f \in \mathcal{F}\)</span> on a given dataset
<span class="math inline">\((X, Y) \in D\)</span> optimized using +sdg
and backpropagation. This section begins discussing a particular class
of parameterized function <span class="math inline">\(f_\theta\)</span>
called Neural Networks (NN).</p>
<h4 id="sec:perceptron">Perceptron</h4>
<p>The Perceptron, introduced by Frank Rosenblatt in 1958 <span
class="citation" data-cites="rosenblatt_1958">[<a
href="#ref-rosenblatt_1958" role="doc-biblioref">63</a>]</span>, is the
building block of Neural Networks (NN). It was introduced as a
simplified model of the human neuron, containing three parts: dendrites
handling incoming signals from other neurons, a soma with a nucleus
responsible for signal aggregation, and an axone responsible for the
transmission of the processed signal to other neurons. When the signal
aggregation in the soma reaches a predefined threshold, the neuron
activates. This phenomenon is called an action potential. Although this
is not an accurate representation of the modern neuroscience state of
knowledge, this simplified model was believed to be accurate at the
time.</p>
<figure id="fig:perceptron">
<img src="./figures/core_nn_perceptron.svg"
alt="Diagram of a Perceptron with three inputs \{x_1; x_2; x_3\}. The perceptron computes an activated weighted sum of its inputs y = \sigma(\sum_{i=1}^{3} w_i \cdot x_i) where \sigma, the activation function is a threshold function." />
<figcaption>Figure 12: Diagram of a Perceptron with three inputs <span
class="math inline">\(\{x_1; x_2; x_3\}\)</span>. The perceptron
computes an activated weighted sum of its inputs <span
class="math inline">\(y = \sigma(\sum_{i=1}^{3} w_i \cdot x_i)\)</span>
where <span class="math inline">\(\sigma\)</span>, the activation
function is a threshold function.</figcaption>
</figure>
<p>Similarly, the Perceptron computes a weighted sum of its inputs and
activates if a certain threshold is reached (see <a
href="#fig:perceptron">Fig 12</a>). The Perceptron is parametrized by
the weights representing the importance attributed to the incoming
inputs and are part of the parameters <span
class="math inline">\(\theta\)</span> that are trained on a given
dataset. It can be viewed as a learned linear regressor followed by a
non-linear activation, historically a threshold function, a function
<span class="math inline">\(\sigma\)</span> that activates <span
class="math inline">\(\sigma(x) = 1\)</span> when <span
class="math inline">\(x &gt; 0.5\)</span> and <span
class="math inline">\(\sigma(x) = 0\)</span> otherwise (see
Lst <strong>¿lst:perceptron?</strong>).</p>
<div class="sourceCode" id="lst:perceptron"><pre
class="sourceCode python"><code class="sourceCode python"><span id="lst:perceptron-1"><a href="#lst:perceptron-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> perceptron(<span class="va">self</span>, x: Tensor, W: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="lst:perceptron-2"><a href="#lst:perceptron-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (x <span class="op">*</span> <span class="va">self</span>.W.T) <span class="op">&gt;</span> <span class="fl">0.5</span></span></code></pre></div>
<p>The objective of a perceptron is to learn a hyperplane, a plane with
<span class="math inline">\(n - 1\)</span> dimensions where <span
class="math inline">\(n\)</span> is the number of inputs, that can
perform binary classification, separate two classes. However, as
mentioned by Marvin L. Minsky and al. in their controversial book
Perceptrons <span class="citation" data-cites="minsky_1969">[<a
href="#ref-minsky_1969" role="doc-biblioref">51</a>]</span>, a
hyperplane regressor cannot solve a simple XOR problem (see <a
href="#fig:xor">Fig 13</a>).</p>
<figure id="fig:xor">
<img src="./figures/core_nn_xor.svg"
alt="Illustration of the Perceptron’s decision hyperplane when trained to solve the AND problem on the left, the OR problem in the middle, and the XOR problem on the right. The first two problems are linearly sperable, thus adapted for a Perceptron. However, a single perceptron cannot solve the XOR problem as it is not linearly separable." />
<figcaption>Figure 13: Illustration of the Perceptron’s decision
hyperplane when trained to solve the AND problem on the left, the OR
problem in the middle, and the XOR problem on the right. The first two
problems are linearly sperable, thus adapted for a Perceptron. However,
a single perceptron cannot solve the XOR problem as it is not linearly
separable.</figcaption>
</figure>
<h4 id="sec:mlp">Multi-Layer Perceptron</h4>
<p>The real value of the Perceptron comes when assembled into a
hierarchical and layer-wise architecture, a Neural Network (NN). By
repeating matrix multiplications (linear transformations) and
non-linearities the network is able to handle non-linear problems and
act as a universal function approximator <span class="citation"
data-cites="hornik_1989">[<a href="#ref-hornik_1989"
role="doc-biblioref">30</a>]</span>. This arrangement of layered
perceptrons is called a Multi-Layer Perceptron (MLP) (see <a
href="#fig:mlp">Fig 14</a>).</p>
<figure id="fig:mlp" width="90%">
<img src="./figures/core_nn_mlp.svg" style="width:90.0%"
alt="Diagram of a 3-layer Multi-Layer Perceptron (MLP). When using the matrix formulation, this arrangement of neurons can be summarized into a single expression y = \sigma(\sigma(x \cdot W_1^T) \cdot W_2^T) \cdot W_3^T." />
<figcaption>Figure 14: Diagram of a 3-layer Multi-Layer Perceptron
(MLP). When using the matrix formulation, this arrangement of neurons
can be summarized into a single expression <span class="math inline">\(y
= \sigma(\sigma(x \cdot W_1^T) \cdot W_2^T) \cdot
W_3^T\)</span>.</figcaption>
</figure>
<p>A MLP with Identity as its activation function is useless as its
chain of linear transformations can be collapsed into a single one.
Since the advent of the Perceptron, the literature has moved away from
using threshold functions as activations. Common activation functions
are the sigmoid <span class="math inline">\(\sigma(x) = \frac{1}{1 +
e^{-x}}\)</span>, tanh <span class="math inline">\(tanh(x) = \frac{e^{z}
- e^{-z}}{e^{z} + e^{-z}}\)</span>, Rectified Linear Unit (ReLU) <span
class="math inline">\(ReLU(x) = max(x, 0)\)</span> functions and
variants presenting additional properties such as infinite continuity,
gradient smoothness, and more (see <a
href="#fig:activations">Fig 15</a>).</p>
<figure id="fig:activations">
<img src="./figures/core_nn_activations.svg"
alt="Activation functions. Sigmoid \sigma(x) = \frac{1}{1 + e^{-x}} acts as a filter y \in [0; 1], tanh tanh(x) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} acts as a normalization compressor y \in [-1; 1], ReLU ReLU(x) = max(x, 0) folds all negatives down to zero y \in [0; +\infty]." />
<figcaption>Figure 15: Activation functions. Sigmoid <span
class="math inline">\(\sigma(x) = \frac{1}{1 + e^{-x}}\)</span> acts as
a filter <span class="math inline">\(y \in [0; 1]\)</span>, tanh <span
class="math inline">\(tanh(x) = \frac{e^{z} - e^{-z}}{e^{z} +
e^{-z}}\)</span> acts as a normalization compressor <span
class="math inline">\(y \in [-1; 1]\)</span>, ReLU <span
class="math inline">\(ReLU(x) = max(x, 0)\)</span> folds all negatives
down to zero <span class="math inline">\(y \in [0;
+\infty]\)</span>.</figcaption>
</figure>
<p><strong>MNIST Classifier:</strong> A classic toy example showing the
capabilities of MLPs is the handwritten digit classification challenge
on the Modified National Institute of Standards and Technology (MNIST)
dataset <span class="citation" data-cites="mnist">[<a href="#ref-mnist"
role="doc-biblioref">71</a>]</span>. MNIST contains <span
class="math inline">\(60,000\)</span> training and <span
class="math inline">\(10,000\)</span> test examples. It has been written
by high school students and gather <span class="math inline">\(28 \times
28\)</span> centered black and white handwritten digits from <span
class="math inline">\(0\)</span> to <span
class="math inline">\(9\)</span> (see <a
href="#fig:mnist">Fig 16</a>).</p>
<figure id="fig:mnist">
<img src="./figures/core_nn_mnist.svg"
alt="First 27 handwritten digits from the Modified National Institute of Standards and Technology (MNIST) dataset. The digits are stored as 28 \times 28 centered black and white images." />
<figcaption>Figure 16: First <span class="math inline">\(27\)</span>
handwritten digits from the Modified National Institute of Standards and
Technology (MNIST) dataset. The digits are stored as <span
class="math inline">\(28 \times 28\)</span> centered black and white
images.</figcaption>
</figure>
<p>Training a MLP on such a challenge is simple and effective. With
little training, parameters (according to the DL standards), and no
hyperparameter tweaking, a vanilla 3-layer NN with ReLU activations can
achieve <span class="math inline">\(97.5%\)</span> accuracy on the test
set. The inputs however need to be transformed before ingestion by the
model as MLPs are constrained to <span
class="math inline">\(1\)</span>-dimensional input vectors. The
following demonstrates how to implement such a model and train it on
MNIST.</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> (Subset, DataLoader)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.datasets.mnist <span class="im">import</span> MNIST</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms.functional <span class="im">import</span> to_tensor</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load MNIST images as Tensors and Normalize [0; 1]</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="kw">lambda</span> x: to_tensor(x).<span class="bu">float</span>().flatten()</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> MNIST(<span class="st">&quot;dataset&quot;</span>, train<span class="op">=</span><span class="va">True</span>,  transform<span class="op">=</span>T.ToTensor())</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>testset <span class="op">=</span> MNIST(<span class="st">&quot;dataset&quot;</span>, train<span class="op">=</span><span class="va">False</span>, transform<span class="op">=</span>T.ToTensor())</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Split dataset in Train and Validation Splits</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>n, split <span class="op">=</span> <span class="bu">len</span>(dataset), <span class="bu">int</span>(np.floor(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(dataset)))</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>train_idxs <span class="op">=</span> np.random.choice(<span class="bu">range</span>(n), size<span class="op">=</span>split, replace<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>valid_idxs <span class="op">=</span> [idx <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(n) <span class="cf">if</span> idx <span class="kw">not</span> <span class="kw">in</span> train_idxs]</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>trainset <span class="op">=</span> Subset(dataset, indices<span class="op">=</span>train_idxs)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>validset <span class="op">=</span> Subset(dataset, indices<span class="op">=</span>valid_idxs)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Mini Batch Loaders (Shuffle Order for Training)</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>trainloader <span class="op">=</span> DataLoader(trainset, batch_size<span class="op">=</span><span class="dv">1_024</span>, shuffle<span class="op">=</span><span class="va">True</span> )</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>validloader <span class="op">=</span> DataLoader(validset, batch_size<span class="op">=</span><span class="dv">1_024</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>testloader  <span class="op">=</span> DataLoader(testset,  batch_size<span class="op">=</span><span class="dv">1_024</span>, shuffle<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<p>The first step consists in loading the MNIST dataset and applying
preprocessing to the data for preparing the ingestion by the model. The
images need to be transformed into a normalized tensor and flatten to
form a <span class="math inline">\(1\)</span>-dimensional vector. The
datasets are split into a training set, a validation set, and a test
set. A mini-batch loader is then used to wrap the dataset and load
multiple input and output pairs at the same time.</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> (Linear, Module, ReLU, Sequential)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> (AdamW, Optimizer)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Model and Optimizer</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential(</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    Linear(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>, <span class="dv">128</span>), ReLU(),</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    Linear(    <span class="dv">128</span>, <span class="dv">128</span>), ReLU(),</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    Linear(    <span class="dv">128</span>,  <span class="dv">10</span>),</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>optim <span class="op">=</span> AdamW(model.parameters(), lr<span class="op">=</span><span class="fl">1e-2</span>)</span></code></pre></div>
<p>Then, the model is defined as a sequence of three linear layers
(linear transformations with a bias for the intercept) and ReLU
activations except for the last one responsible for outputting the
logits, used for computing the loss, here the cross entropy for
multi-class classification. The enhanced SGD optimizer, Adam, is then
initialized with the model’s weight and a learning rate <span
class="math inline">\(\epsilon\)</span>. AdamW is a variant of Adam with
a corrected weight decay term for regularization.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> Tensor</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.functional <span class="im">import</span> cross_entropy</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform one Step and estimate Metrics</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> step(</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    model: Module,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    optim: Optimizer,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    imgs: Tensor,</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    labels: Tensor,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    split: <span class="bu">str</span>,</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Tuple[<span class="bu">float</span>, <span class="bu">float</span>]:</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> model(imgs)                  <span class="co"># Prediction</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> cross_entropy(logits, labels)  <span class="co"># Mean Loss</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    n_correct <span class="op">=</span> logits.argmax(dim<span class="op">=-</span><span class="dv">1</span>)     <span class="co"># Correct Predictions</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Train if split is &quot;train&quot;</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> split <span class="op">==</span> <span class="st">&quot;train&quot;</span>:</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        optim.step()</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>        optim.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss.item(), n_correct.item()</span></code></pre></div>
<p>The <code>step</code> function is responsible for performing one
training step when the given split is set to <code>"train"</code> and
computes the metrics used for monitoring. In our case, we monitor the
average loss and the accuracy of the model. For a more complete
evaluation, other metrics such as the F-<span
class="math inline">\(1\)</span> score, the perplexity, the recall, and
a confusion matrix can be evaluated. They are here omitted for the sake
of illustration and simplicity.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train for 10 epochs</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Training</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    loss, acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> imgs, labels <span class="kw">in</span> trainloader:</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        metrics <span class="op">=</span> step(model, optim, imgs, label, <span class="st">&quot;train&quot;</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">+=</span> metrics[<span class="dv">0</span>] <span class="op">/</span> <span class="bu">len</span>(trainloader)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        acc  <span class="op">+=</span> metrics[<span class="dv">1</span>] <span class="op">/</span> <span class="bu">len</span>(trainloader.dataset)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;[Train] Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, loss: </span><span class="sc">{</span>loss<span class="sc">:.2e}</span><span class="ss">, acc: </span><span class="sc">{</span>acc <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%&quot;</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Validation</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.inference_mode():</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        loss, acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> imgs, labels <span class="kw">in</span> validloader:</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>            metrics <span class="op">=</span> step(model, optim, imgs, label, <span class="st">&quot;valid&quot;</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">+=</span> metrics[<span class="dv">0</span>] <span class="op">/</span> <span class="bu">len</span>(validloader)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>            acc  <span class="op">+=</span> metrics[<span class="dv">1</span>] <span class="op">/</span> <span class="bu">len</span>(validloader.dataset)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;[Valid] Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, loss: </span><span class="sc">{</span>loss<span class="sc">:.2e}</span><span class="ss">, acc: </span><span class="sc">{</span>acc <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%&quot;</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Test</span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.inference_mode():</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    loss, acc <span class="op">=</span> <span class="dv">0</span>, <span class="dv">0</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> imgs, labels <span class="kw">in</span> testloader:</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        metrics <span class="op">=</span> step(model, optim, imgs, label, <span class="st">&quot;test&quot;</span>)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">+=</span> metrics[<span class="dv">0</span>] <span class="op">/</span> <span class="bu">len</span>(testloader)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>        acc  <span class="op">+=</span> metrics[<span class="dv">1</span>] <span class="op">/</span> <span class="bu">len</span>(testloader.dataset)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;[Test] loss: </span><span class="sc">{</span>loss<span class="sc">:.2e}</span><span class="ss">, acc: </span><span class="sc">{</span>acc <span class="op">*</span> <span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%&quot;</span>)</span></code></pre></div>
<p>Finally, the model is trained for <span
class="math inline">\(10\)</span> epochs, the number of times the entire
dataset is looped through. This number was arbitrarily chosen to
correspond with the loss saturation when the model does not improve
much. A training loop is divided into a few steps, a training phase
where one continuously performs a training step followed by a validation
step to monitor generalization, and when stopped, a test phase to
monitor model generalization without bias. This last step prevents
trying to overfit the validation set specifically and should be
performed at the very end. An example of training history is shown in <a
href="#fig:mnist_history">Fig 17</a>. In this example, the model reaches
<span class="math inline">\(97.5%\)</span> accuracy. By spending time
tweaking the hyperparameters (the model’s weights, the learning rate,
the number of epochs, …), the model can be improved further.</p>
<figure id="fig:mnist_history">
<img src="./figures/core_nn_mnist_history.svg"
alt="Training history of a 3-layer Multi-Layer Perceptron (MLP) with 128 neurons in every layer on the MNIST dataset. The average loss (cross-entropy) on the left, and the accuracy on the right are displayed for the training, validation, and test splits." />
<figcaption>Figure 17: Training history of a 3-layer Multi-Layer
Perceptron (MLP) with <span class="math inline">\(128\)</span> neurons
in every layer on the MNIST dataset. The average loss (cross-entropy) on
the left, and the accuracy on the right are displayed for the training,
validation, and test splits.</figcaption>
</figure>
<h4 id="sec:cnn">Convolutional Neural Network</h4>
<p>While MLPs can be viewed as universal function approximators, they
scale poorly with respect to high dimensional inputs such as images,
videos, sound representations such as a spectrogram, volumetric data,
and long sequences. For example, if we consider a small RGB image of
size <span class="math inline">\(256 \times 256 \times 3\)</span>, the
input of a MLP would be a 1-dimensional vector of size <span
class="math inline">\(196,608\)</span>. The input layer of a MLP with
<span class="math inline">\(64\)</span> neurons would already mean that
the network contains more than <span
class="math inline">\(12,582,912\)</span> parameters. For this reason,
researchers have created specialized NNs with biases in their
architecture inspired by cognitive and biophysical mechanisms.
Convolutional Neural Networks (CNN) (ConvNets) are such a NN specialized
in handling spatially correlated data such as images.</p>
<figure id="fig:convolution">
<img src="./figures/core_nn_convolution.svg"
alt="Illustration of a single 3 \times 3 \times 3 filter convolution in the middle applied to a 8 \times 8 \times 3 input tensor on the left. The result is a 6 \times 6 \times 1 activation map on the right. The filter receptive field is drawn in dashed lines. This convolution is applied in valid model, no passing was applied to the input resulting in a lower resolution output tensor." />
<figcaption>Figure 18: Illustration of a single <span
class="math inline">\(3 \times 3 \times 3\)</span> filter convolution in
the middle applied to a <span class="math inline">\(8 \times 8 \times
3\)</span> input tensor on the left. The result is a <span
class="math inline">\(6 \times 6 \times 1\)</span> activation map on the
right. The filter receptive field is drawn in dashed lines. This
convolution is applied in valid model, no passing was applied to the
input resulting in a lower resolution output tensor.</figcaption>
</figure>
<p><strong>Convolution:</strong> The core component of a ConvNet is the
convolution operation. A +CNN operates by convolving (rolling) a set of
parametrized filters on the input. If we reconsider our <span
class="math inline">\(W_1 \times H_1 \times D_1 = 256 \times 256 \times
3\)</span>, convolving a single filter of size <span
class="math inline">\(F_W \times F_H \times D_1 = 3 \times 3 \times
3\)</span> would require sliding the filter across the entire input
image tensor and computing the dot product of the overlapping tensor
chunk and the filter. This operation results in what is called an
activation map, or feature map. The filter can be convolved in different
configurations. The stride <span class="math inline">\(S\)</span>
defines the hop size when rolling the filter over the input, and the
padding <span class="math inline">\(P\)</span> defines the additional
border added to the input tensor in order to parkour the input border
(<span class="math inline">\(252\)</span> unique positions for the
filter in the <span class="math inline">\(256\)</span> image, <span
class="math inline">\(256\)</span> positions with a padding of <span
class="math inline">\(1\)</span> on each side of the input). A CNN
convolves multiple parametrized filters <span
class="math inline">\(K\)</span> in a single convolution operation.
Given a convolution setting, the operation requires <span
class="math inline">\(F_H \times F_W \times D_1 \times D_2\)</span>
parameters and outputs a feature map tensor of size <span
class="math inline">\(W_2 = (W_1 - F_W + 2P_W) / S + 1\)</span>, <span
class="math inline">\(H_2 = (H_1 - F_H + 2P_H) / S + 1\)</span>, and
<span class="math inline">\(D_2 = K\)</span> (see <a
href="#fig:convolution">Fig 18</a>). The different filters are
responsible for looking for the activation of different patterns in the
input. The Convolution layer introduces the notion of weight sharing
enabled by the sliding filter (neurons) and reduces computation by a
large margin in comparison to a standard MLP layer.</p>
<p><strong>Pooling:</strong> It is common to follow convolution layers
by pooling layers to reduce the dimensionality when growing the ConvNet
deeper. The pooling layer reduces its input by applying a reduction
operation. The reduction operation can be taking the <code>max</code>,
<code>min</code>, or <code>average</code>, of a rolling window. This
operation does not involve any additional parameter and is applied
channel-wise. If we consider a max-pooling operation with a <span
class="math inline">\(2 \times 2\)</span> kernel and a stride of <span
class="math inline">\(2\)</span>, the output becomes half the size of
the input. It also has the benefit of making the CNN more robust to
scale and translation. It is sometimes more strategic to make use of
stride instead of adding pooling layers. It has the same benefit of
reducing the feature map size while avoiding an additional
operation.</p>
<figure id="fig:convnet">
<img src="./figures/core_nn_convnet.svg"
alt="Illustration of a small Convolutional Neural Networks (CNN) containing a convolution (conv) layer, a max-pooling (maxpool), and another convolution followed by another max-pooling. The last feature map is then flattened into a 1-dimensional vector and used as the input for the Multi-Layer Perceptron (MLP) classifier." />
<figcaption>Figure 19: Illustration of a small Convolutional Neural
Networks (CNN) containing a convolution (conv) layer, a max-pooling
(maxpool), and another convolution followed by another max-pooling. The
last feature map is then flattened into a <span
class="math inline">\(1\)</span>-dimensional vector and used as the
input for the Multi-Layer Perceptron (MLP) classifier.</figcaption>
</figure>
<p><strong>ConvNet:</strong> Finally, a CNN is assembled by stacking
multiple convolution layers and pooling layers. When the feature maps
are small enough, the final feature map is flattened and passed to an
additional MLP in charge of the classification or regression. This
combination of a parametric convolutional feature extractor and a MLP is
what we call a ConvNet.</p>
<figure id="fig:vgg_activations">
<img src="./figures/core_nn_vgg_activations.svg"
alt="Visualization of VGG16’s four first activation maps (feature maps). The input image is left and the activations are shown in order of the layers top to bottom and left to right. Credit https://images.all4ed.org/" />
<figcaption>Figure 20: Visualization of VGG16’s four first activation
maps (feature maps). The input image is left and the activations are
shown in order of the layers top to bottom and left to right. Credit <a
href="https://images.all4ed.org/">https://images.all4ed.org/</a></figcaption>
</figure>
<p><strong>Feature Maps:</strong> The feature maps learned by a CNN are
hierarchical. In the first layers, the learned filters are focusing on
simple features such as lines, diagonals, and arcs, and act as edge
detectors. The deeper the layers are, the more complex the features are
because they are resulting from a succession of combinations from
previous activations (see <a
href="#fig:vgg_activations">Fig 20</a>).</p>
<p><strong>Finetuning:</strong> Training CNNs on bigger and more diverse
datasets allows learning more general filters increasing the likelihood
that the network will perform on out-of-domain data. In practice, CNNs
are not often trained from scratch. Such a process requires the use of
expensive dedicated hardware and hours of training. However, thanks to
the open-source mindset of the DL field, big actors often share the
weights of such models referred to as pretrained models, or foundation
models <span class="citation" data-cites="foundation_2021">[<a
href="#ref-foundation_2021" role="doc-biblioref">6</a>]</span>.</p>
<p>Foundation models can be further refined through smaller training on
smaller and specialized datasets containing few good-quality examples.
This process is called finetuning and is less expensive and
time-consuming than full training. One method for finetuning consists in
removing the classification head of a pre-trained model such as VGG16
<span class="citation" data-cites="simonyan_2014">[<a
href="#ref-simonyan_2014" role="doc-biblioref">70</a>]</span> and
replacing it with a new one adapted to the number of classes required
for the task. The pretrained weights are then frozen (not updated during
training), and the new weights are trained following a standard
supervised-learning procedure (see
Lst <strong>¿lst:finetune?</strong>).</p>
<div class="sourceCode" id="lst:finetune"><pre
class="sourceCode python"><code class="sourceCode python"><span id="lst:finetune-1"><a href="#lst:finetune-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models <span class="im">import</span> (vgg16, VGG16_Weights)</span>
<span id="lst:finetune-2"><a href="#lst:finetune-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:finetune-3"><a href="#lst:finetune-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Import pretrained VGG16 model</span></span>
<span id="lst:finetune-4"><a href="#lst:finetune-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> vgg16(weights<span class="op">=</span>VGG16_Weights.DEFAULT)</span>
<span id="lst:finetune-5"><a href="#lst:finetune-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:finetune-6"><a href="#lst:finetune-6" aria-hidden="true" tabindex="-1"></a><span class="co">#Freeze pretrained features weights</span></span>
<span id="lst:finetune-7"><a href="#lst:finetune-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> model.features.parameters():</span>
<span id="lst:finetune-8"><a href="#lst:finetune-8" aria-hidden="true" tabindex="-1"></a>    param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="lst:finetune-9"><a href="#lst:finetune-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="lst:finetune-10"><a href="#lst:finetune-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Replace the classifier head</span></span>
<span id="lst:finetune-11"><a href="#lst:finetune-11" aria-hidden="true" tabindex="-1"></a>model.classifier <span class="op">=</span> Sequential(</span>
<span id="lst:finetune-12"><a href="#lst:finetune-12" aria-hidden="true" tabindex="-1"></a>    Linear(<span class="dv">512</span> <span class="op">*</span> <span class="dv">7</span> <span class="op">*</span> <span class="dv">7</span>, <span class="dv">512</span>), ReLU(),</span>
<span id="lst:finetune-13"><a href="#lst:finetune-13" aria-hidden="true" tabindex="-1"></a>    Linear(<span class="dv">512</span>,         <span class="dv">512</span>), ReLU(),</span>
<span id="lst:finetune-14"><a href="#lst:finetune-14" aria-hidden="true" tabindex="-1"></a>    Linear(<span class="dv">512</span>, num_classes),</span>
<span id="lst:finetune-15"><a href="#lst:finetune-15" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p><strong>MNIST Classifier:</strong> Let us reconsider the MNIST toy
classification example and replace the MLP with a CNN. The model is
divided in two sections, the feature extractor made out of two
convolutional and max-pooling layers with <span class="math inline">\(5
\times 5\)</span> filters, the middle layer responsible for flattening
the feature maps down to a <span
class="math inline">\(1\)</span>-dimensional vector fed to the
classifier head, a <span class="math inline">\(3\)</span>-layer MLP
similar to the first one. The training procedure is left unchanged, the
number of parameters is approximately similar, a little less for the
CNN, and the number of epochs is the same. The input is however not
flattened as the CNN consumes a full image tensor.</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> OrderedDict</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> (Conv2d, Flatten, Linear, MaxPool2d, ReLU, Sequential)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.transforms.functional <span class="im">import</span> to_tensor</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Load MNIST images as Tensors and Normalize [0; 1]</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="kw">lambda</span> x: to_tensor(x).<span class="bu">float</span>()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>...</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Model</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential(OrderedDict(</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    features<span class="op">=</span>Sequential(</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        Conv2d(<span class="dv">1</span>,  <span class="dv">6</span>, <span class="dv">5</span>), ReLU(), MaxPool2d(<span class="dv">2</span>),</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        Conv2d(<span class="dv">6</span>, <span class="dv">16</span>, <span class="dv">5</span>), ReLU(), MaxPool2d(<span class="dv">2</span>),</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    flatten<span class="op">=</span>Flatten(),</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    classifier<span class="op">=</span>Sequential(</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        Linear(<span class="dv">256</span>, <span class="dv">128</span>), ReLU(),</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>        Linear(<span class="dv">128</span>,  <span class="dv">64</span>), ReLU(),</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        Linear( <span class="dv">64</span>,  <span class="dv">10</span>),</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>...</span></code></pre></div>
<p>The CNN is able to achieve a <span class="math inline">\(99%\)</span>
accuracy on the test set early during training (epoch <span
class="math inline">\(5\)</span>). The CNN is a more robust,
specialized, and thus more efficient architecture for handling images.
The training history can be observed in <a
href="#fig:mnist_convnet_history">Fig 21</a>.</p>
<figure id="fig:mnist_convnet_history">
<img src="./figures/core_nn_mnist_convnet_history.svg"
alt="Training history of a Convolutional Neural Network (CNN) made out of a sequence of two convolutions followed by max-pooling and a 3-layer Multi-Layer Perceptron (MLP) classifier on the MNIST dataset. The average loss (cross-entropy) on the left, and the accuracy on the right are displayed for the training, validation, and test splits." />
<figcaption>Figure 21: Training history of a Convolutional Neural
Network (CNN) made out of a sequence of two convolutions followed by
max-pooling and a 3-layer Multi-Layer Perceptron (MLP) classifier on the
MNIST dataset. The average loss (cross-entropy) on the left, and the
accuracy on the right are displayed for the training, validation, and
test splits.</figcaption>
</figure>
<h4 id="sec:transformers">Transformers</h4>
<p>When considering sequences such as text, audio, video, or a single
image divided into a sequence of <span class="math inline">\(n \times
n\)</span> blocks, MLPs and CNNs fail at capturing long-term
relationships being subject to vanishing and exploding gradients. When
taking a guess from a long sequence of information, we humans do not
attribute as much weight to every bit of the sequence. We selectively
gather and retain information that we feel serves our decision-making
called a task-dependent context. This selective mechanism is called
attention and has been the subject of implementation attempts, first in
the field of Natural Language Processing (NLP) <span class="citation"
data-cites="bahdanau_2014 luong_2015 vaswani_2017">[<a
href="#ref-bahdanau_2014" role="doc-biblioref">5</a>, <a
href="#ref-luong_2015" role="doc-biblioref">48</a>, <a
href="#ref-vaswani_2017" role="doc-biblioref">72</a>]</span>, and later
transposed to field of Computer Vision (CV) <span class="citation"
data-cites="dosovitskiy_2020 caron_2021">[<a href="#ref-caron_2021"
role="doc-biblioref">8</a>, <a href="#ref-dosovitskiy_2020"
role="doc-biblioref">14</a>]</span>.</p>
<p>This section focuses on visual applications of the attention
mechanism only as this thesis is about image generation, and more
specifically a particular type of attention that made the success of the
modern Transformer <span class="citation" data-cites="vaswani_2017">[<a
href="#ref-vaswani_2017" role="doc-biblioref">72</a>]</span>
architecture called Self-Attention.</p>
<p><strong>Self-Attention:</strong> Computing self-attention requires
the creation of three vectors for each element of the sequence in latent
space. For every element <span class="math inline">\(x_i\)</span>, we
create a query vector <span class="math inline">\(q_i\)</span>, a key
vector <span class="math inline">\(k_i\)</span>, and a value vector
<span class="math inline">\(v_i\)</span>. Those vectors are obtained by
multiplying the latent input vectors <span
class="math inline">\(x_i\)</span> by trainable embedding weight
matrices <span class="math inline">\(W_q\)</span>, <span
class="math inline">\(W_k\)</span>, and <span
class="math inline">\(W_v\)</span>. The generated output embeddings are
usually chosen to be smaller than the original input size to save
computation.</p>
<p>The query and key vectors are then combined using a dot product to
compute a score <span class="math inline">\(s_{i,j} = q_i \cdot
k_j\)</span> matching every query vector with every key. This score
determines how much focus is put on the element at position <span
class="math inline">\(j\)</span> in order to encode the element at
position <span class="math inline">\(i\)</span>. The scores are then
scaled by dividing them by the square root of the key vectors dimensions
<span class="math inline">\(\sqrt{d_k}\)</span> and normalizing them
using a softmax so they add up to one and form probability vectors. This
value describes how much each element attends to the others.</p>
<p>Finally, we apply and make the element attend to each other, by
multiplying the softmax vectors with the value vectors and aggregating
them with a sum. This final vector representing the new context for
which attention is used can be propagated to the next layers of the
NN.</p>
<p>When summarized and expressed in matrix form, by stacking the element
vectors <span class="math inline">\(x_i\)</span> into the matrix <span
class="math inline">\(X\)</span>, the self-attention mechanism can be
summarized into the following formula:</p>
<p><span id="eq:nn_sa_formula"><span class="math display">\[
\begin{aligned}
Q = (x \cdot W_q), \; K = (X \cdot W_k), \; V = (X \cdot W_v) \\
Z = softmax(\frac{Q \cdot K^T}{\sqrt{d_k}}) \cdot V
\end{aligned}
\qquad{(6)}\]</span></span></p>
<p>One other advantage of using such a technique is explainability. We
can explore the “reasoning” of the NN by looking at the attention scores
and observing how the sequence elements attend to each other.</p>
<p><strong>Multihead Self-Attention:</strong> In their paper <span
class="citation" data-cites="vaswani_2017">[<a href="#ref-vaswani_2017"
role="doc-biblioref">72</a>]</span>, the authors further refine this
notion of self-attention. Let us consider one self-attention module and
call it an attention head. They propose to stack multiple attention
heads and demonstrate improved performances. This notion enables models
equipped with multi-head attention to possess an ensemble of query, key,
and value subspaces with different representations. To optimize for
parallelism every head computation is done at the same time by combining
the weight matrices.</p>
<figure id="fig:core_nn_posenc">
<img src="./figures/core_nn_posenc.svg"
alt="Sinusoidal positional encoding for Transformers. The positional encoding is precomputed as a n \times d_x matrix, queried and added to the input embeddings before traversing the transformer blocks, with n the sequence length, and d_x the dimension of an embedding vector." />
<figcaption>Figure 22: Sinusoidal positional encoding for Transformers.
The positional encoding is precomputed as a <span
class="math inline">\(n \times d_x\)</span> matrix, queried and added to
the input embeddings before traversing the transformer blocks, with
<span class="math inline">\(n\)</span> the sequence length, and <span
class="math inline">\(d_x\)</span> the dimension of an embedding
vector.</figcaption>
</figure>
<p><strong>Positional Encoding:</strong> There is however an issue.
Contrary to CNNs exploiting the spatially local correlation of
information, self-attention does not encode the order of the vectors. To
alleviate this problem, Vaswani et al. <span class="citation"
data-cites="vaswani_2017">[<a href="#ref-vaswani_2017"
role="doc-biblioref">72</a>]</span> propose to add (concatenate) a
vector to each embedding input. This vector is designed to follow a
specific pattern learned by the model that will help encode the position
of information along its flow in the network. This is called a
positional encoding vector. Most positional encodings introduced by the
community follow some kind of general Fourier pattern that can scale
with the length of the sequence (see <a
href="#fig:core_nn_posenc">Fig 22</a>).</p>
<figure id="fig:core_nn_transformer_block" width="80%">
<img src="./figures/core_nn_transformer_block.svg" style="width:80.0%"
alt="Illustrated Transformer block. The input is a sequence of embeddings x_i. The embeddings are augmented with positional encoding and sent to the transformer blocks. A block contains a succession of a first block of self-attention, residual connections, layer normalization, and a second block of feed-forward layer, residual connections, and layer normalization. The blocks are repeated N times." />
<figcaption>Figure 23: Illustrated Transformer block. The input is a
sequence of embeddings <span class="math inline">\(x_i\)</span>. The
embeddings are augmented with positional encoding and sent to the
transformer blocks. A block contains a succession of a first block of
self-attention, residual connections, layer normalization, and a second
block of feed-forward layer, residual connections, and layer
normalization. The blocks are repeated <span
class="math inline">\(N\)</span> times.</figcaption>
</figure>
<p><strong>Transformers:</strong> A Transformer architecture is defined
by a succession of transformer blocks (see <a
href="#fig:core_nn_transformer_block">Fig 23</a>). Those blocks follow a
similar pattern and are using tricks from previous work such as layer
normalization <span class="citation" data-cites="ba_2016">[<a
href="#ref-ba_2016" role="doc-biblioref">4</a>]</span> and residual
connections <span class="citation" data-cites="he_2016">[<a
href="#ref-he_2016" role="doc-biblioref">25</a>]</span> to allow to
train deeper networks with a large number of blocks. A block is made out
of a multi-head self-attention layer, followed by a residual injection
and a normalization layer followed by a feed-forward layer and another
residual injection and normalization. The input embeddings are first
augmented with positional encoding and go through every block one by
one. This architecture is flexible and has first been presented as a
sequence-to-sequence network with encoder blocks and decoder blocks.</p>
<figure id="fig:core_nn_patches">
<img src="./figures/core_nn_patches.svg"
alt="256 \times 256 image processed into 32 \times 32 patches with a no overlap. Each patch can be processed by a CNN or a MLP after flattening and their embedding used in a Transformer network which is thus called a Vision Transformer (ViT)" />
<figcaption>Figure 24: <span class="math inline">\(256 \times
256\)</span> image processed into <span class="math inline">\(32 \times
32\)</span> patches with a no overlap. Each patch can be processed by a
CNN or a MLP after flattening and their embedding used in a Transformer
network which is thus called a Vision Transformer (ViT)</figcaption>
</figure>
<p><strong>Vision Transformers:</strong> The success of the Transformer
architecture <span class="citation" data-cites="vaswani_2017">[<a
href="#ref-vaswani_2017" role="doc-biblioref">72</a>]</span> in the NLP
community has pushed the CV community to explore how one can use this
kind of NNs for vision tasks. It turns out that by expressing images as
sequences of patches, <span class="math inline">\(n \times n\)</span>
image blocks (see <span class="citation"
data-cites="core_nn_patches">[<a href="#ref-core_nn_patches"
role="doc-biblioref"><strong>core_nn_patches?</strong></a>]</span>), and
embedding them, transformers can exceed the performance of a CNN for
classification <span class="citation" data-cites="dosovitskiy_2020">[<a
href="#ref-dosovitskiy_2020" role="doc-biblioref">14</a>]</span>, but
also present emerging faculties resulting from the use of self-attention
such as segmentation and saliency maps <span class="citation"
data-cites="caron_2021">[<a href="#ref-caron_2021"
role="doc-biblioref">8</a>]</span>. This architecture is referred to as
a ViT.</p>
<p><strong>MNIST Classifier:</strong> …</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> (Linear, Module)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear layer with no bias</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>Lnb <span class="op">=</span> <span class="kw">lambda</span> i, o: Linear(i, o, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Multi-Head Attention Module</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiHeadAttention(Module):</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        emb_dim: <span class="bu">int</span>,</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        n_heads: <span class="bu">int</span>,</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        head_dim: <span class="bu">int</span>,</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.N <span class="op">=</span> n_heads</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.H <span class="op">=</span> head_dim</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.qkv <span class="op">=</span> Lnb(emb_dim, <span class="dv">3</span> <span class="op">*</span> n_heads <span class="op">*</span> head_dim)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out <span class="op">=</span> Lnb(n_heads <span class="op">*</span> head_dim, emb_dim)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scale <span class="op">=</span> head_dim <span class="op">**</span> <span class="op">-</span><span class="fl">0.5</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        B, S, _ <span class="op">=</span> x.shape</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> <span class="va">self</span>.qkv(x).chunk(<span class="dv">3</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> <span class="bu">map</span>(<span class="kw">lambda</span> x: x.reshape(B, S, <span class="va">self</span>.N, <span class="va">self</span>.H), qkv)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        qkv <span class="op">=</span> <span class="bu">map</span>(<span class="kw">lambda</span> x: x.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>), qkv)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        q, k, v <span class="op">=</span> qkv</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> <span class="va">self</span>.scale <span class="op">*</span> (q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>        a <span class="op">=</span> torch.softmax(s, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> (a <span class="op">@</span> v).reshape(B, S, <span class="va">self</span>.N <span class="op">*</span> <span class="va">self</span>.H)</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.out(z)</span></code></pre></div>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> (ReLU, Sequential)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FeedForward(Sequential):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_dim, h_dim: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>            Lnb(emb_dim, h_dim), ReLU(),</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>            Lnb(h_dim, emb_dim),</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        )</span></code></pre></div>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> LayerNorm</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerBlock(Module):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>,</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>            emb_dim: <span class="bu">int</span>,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>            n_heads: <span class="bu">int</span>,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>            head_dim: <span class="bu">int</span>,</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>            h_dim: <span class="bu">int</span>,</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        ) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm1 <span class="op">=</span> LayerNorm(emb_dim, eps<span class="op">=</span><span class="fl">1e-5</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mha <span class="op">=</span> MultHeadAttention(emb_dim, n_heads, head_dim)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm2 <span class="op">=</span> LayerNorm(emb_dim, eps<span class="op">=</span><span class="fl">1e-5</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ff <span class="op">=</span> FeadForward(emb_dim, h_dim)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.mha(<span class="va">self</span>.norm1(x)) <span class="op">+</span> x</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.ff(<span class="va">self</span>.norm2(x)) <span class="op">+</span> x</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code></pre></div>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Precompute Positional Encoding</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pos_enc(emb_dim: <span class="bu">int</span>, max_len: <span class="bu">int</span>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    dtype <span class="op">=</span> torch.float32</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    pe <span class="op">=</span> torch.zeros(max_len, emb_dim)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    pos <span class="op">=</span> torch.arange(<span class="dv">0</span>, max_len, dtype<span class="op">=</span>dtype)[:, <span class="va">None</span>]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    denom <span class="op">=</span> torch.arange(<span class="dv">0</span>, emb_dim, <span class="dv">2</span>, dtype<span class="op">=</span>dtype)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    denom <span class="op">=</span> denom <span class="op">*</span> (<span class="op">-</span>np.log(<span class="fl">10_000.0</span>) <span class="op">/</span> emb_dim)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    denom <span class="op">=</span> torch.exp(denom)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(pos <span class="op">*</span> denom)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(pos <span class="op">*</span> denom)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pe[<span class="va">None</span>, :, :]</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Positional Encoder</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoder(Module):</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, emb_dim: <span class="bu">int</span>, max_len: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">&quot;pe&quot;</span>, pos_enc(emb_dim, max_len))</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">+</span> <span class="va">self</span>.pe[:, :x.size(<span class="dv">1</span>)]</span></code></pre></div>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparameters</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>PS <span class="op">=</span> <span class="dv">32</span>  <span class="co"># Patch size</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>E <span class="op">=</span> <span class="dv">128</span>  <span class="co"># Embedding dim</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">8</span>    <span class="co"># Number of heads</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># ViT: Visition Transformer</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VisionTransformer(Module):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.emb <span class="op">=</span> Sequential(</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>            Linear(P <span class="op">*</span> P, E), ReLU(),</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>            Linear(E, E),</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pe <span class="op">=</span> PositionalEncoder(E)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.block <span class="op">=</span> TransformerBlock(E, N, E <span class="op">//</span> N, E)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.head <span class="op">=</span> Linear(E, <span class="dv">10</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        B, <span class="op">*</span>_ <span class="op">=</span> x.shape</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.unfold(<span class="dv">2</span>, P, P).unfold(<span class="dv">3</span>, P, P)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(<span class="op">-</span><span class="dv">1</span>, P <span class="op">*</span> P)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.emb(x)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(B, <span class="op">-</span><span class="dv">1</span>, E)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pe(x)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.block(x)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.mean(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.head(x)</span></code></pre></div>
<figure id="fig:core_nn_vit_history">
<img src="./figures/core_nn_vit_history.svg"
alt="Vision Transformer (ViT) training history…" />
<figcaption>Figure 25: Vision Transformer (ViT) training
history…</figcaption>
</figure>
<figure id="fig:core_nn_vit_mha">
<img src="./figures/core_nn_vit_mha.svg"
alt="Vision Transformer (ViT) multi-head attention maps…" />
<figcaption>Figure 26: Vision Transformer (ViT) multi-head attention
maps…</figcaption>
</figure>
<!-- TODO: You are here -->
<h3 id="sec:generative">Generative Architectures</h3>
<p>In this section, we extend our Deep Learning (DL) architecture
toolbox with generative AI architectures such as the Autoencoder (AE)
(see <a href="#sec:ae">Sec 2.2.4.1</a>), the Variational Autoencoder
(VAE) (see <a href="#sec:vae">Sec 2.2.4.2</a>), the Generative
Adversarial Network (GAN) (see <a href="#sec:gan">Sec 2.2.4.3</a>), the
Denoising Diffusion Model (DDM) (see <a
href="#sec:ddm">Sec 2.2.4.4</a>), and Large Language Models (LLM) (see
<a href="#sec:llm">Sec 2.2.4.5</a>) with a strong focus on image
generation. Similarly to the previous sections, the Modified National
Institute of Standards and Technology (MNIST) dataset is used for
illustrative purposes.</p>
<p>This section does not only discuss the technical details of those
architectures but also compares them on three criteria, generation
inference speed, generation variance, and generation quality and
complexity.</p>
<h4 id="sec:ae">Autoencoders</h4>
<p>Autoencoders (AE) are part of a family of feedforward NNs for which
the input tensor is the same as the output tensor. They encode
(compress), the input into a low-dimensional code in a latent space, and
then decode (reconstruct) the original input from this compressed
representation (see <a href="#fig:gai_autoencoder">Fig 27</a>). An AE is
built using two network parts, an encoder <span
class="math inline">\(E\)</span>, NN that reduces the input dimension, a
decoder <span class="math inline">\(D\)</span> that recovers the input
<span class="math inline">\(x\)</span> from the reduced tensor <span
class="math inline">\(z\)</span>, and a reconstruction objective. This
architecture can be viewed as a dimensionality reduction technique but
can be used as a generative model. By feeding the decoder <span
class="math inline">\(D\)</span> with arbitrary latent codes <span
class="math inline">\(z\)</span>, one can generate unseen data points
<span class="math inline">\(\hat{x}\)</span> similar to the training
distribution by interpolation. Additional training objectives can be
used to disentangle the latent representation so that the data points
are organized mindfully in the latent space, semantically for
example.</p>
<figure id="fig:gai_autoencoder">
<img src="./figures/core_gai_autoencoder.svg"
alt="+ae architecture. The encoder compresses the input into a latent code that is reconstructed using the decoder. architecture." />
<figcaption>Figure 27: <span class="full">+ae architecture. The encoder
compresses the input into a latent code that is reconstructed using the
decoder.</span> architecture.</figcaption>
</figure>
<p><strong>Properties:</strong> Compared to a traditional compression
method, AEs are tied to their training data. They are trained to learn
data-specific features useful for in-domain compression not for
out-of-domain. An AE trained on MNIST cannot be used for compressing
photos of faces. Such architecture cannot be considered a lossless
compression algorithm. The reconstruction is most of the time degraded.
One strong advantage of using an AE is that they do not require complex
data preparation. They are part of the unsupervised training family,
where labeled data is not needed for training, and in this case,
self-supervised learning where the target output is built synthetically
from the input.</p>
<p><strong>MNIST Digit Image Generation:</strong> Let us consider MNIST
and train a small Autoencoder (AE) to compress handwritten digits to
<span class="math inline">\(32\)</span> latent codes. Our AE is made out
of a small MLP encoder and decoder both with two inner layers with a
hidden dimension of <span class="math inline">\(128\)</span>.</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> OrderedDict</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> (Linear, ReLU, Sequential, Sigmoid)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Model definition</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential(OrderedDict(</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    encoder<span class="op">=</span>Sequential(</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        Linear(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>, <span class="dv">256</span>), ReLU(),</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        Linear(    <span class="dv">256</span>,   <span class="dv">2</span>),</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    decoder<span class="op">=</span>Sequential(</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        Linear(  <span class="dv">2</span>,     <span class="dv">256</span>), ReLU(),</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        Linear(<span class="dv">256</span>, <span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>), Sigmoid(),</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>))</span></code></pre></div>
<p>The model is trained on <span class="math inline">\(10\)</span>
epochs with no hyperparameter tuning using the
<code>binar_cross_entropy</code> objective function as the dataset
contains black and white images normalized in the <span
class="math inline">\([0; 1]\)</span> range.</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.functional <span class="im">import</span> binary_cross_entropy</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute loss</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> binary_cross_entropy(model(x), x)</span></code></pre></div>
<figure id="fig:gai_autoencoder_history">
<img src="./figures/core_gai_autoencoder_history.svg"
alt="Training history of a small 2-layer Autoencoder (AE). The binary cross entropy loss is shown on the left, a training sample in the middle, and its corresponding reconstruction on the right." />
<figcaption>Figure 28: Training history of a small <span
class="math inline">\(2\)</span>-layer Autoencoder (AE). The binary
cross entropy loss is shown on the left, a training sample in the
middle, and its corresponding reconstruction on the right.</figcaption>
</figure>
<p>The result of the training can be observed in <a
href="#fig:gai_autoencoder_history">Fig 28</a>. Despite little
degradation, our model can reconstruct the handwritten digits from their
latent code. The degradation is minimized by the fact that we are
dealing with a toy dataset. The phenomenon can be observed by reducing
the number of parameters of the network or the size of the latent space.
To reconstruct the images, we first need to get a latent code, either by
encoding an existing image, or by randomly initializing a latent vector
in a reasonable range, and providing it to the decoder as shown
below.</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample given latent-code</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>x_ <span class="op">=</span> model.decoder(z)</span></code></pre></div>
<figure id="fig:gai_autoencoder_latent" width="60%">
<img src="./figures/core_gai_autoencoder_latent.svg" style="width:60.0%"
alt="Trained Autoencoder (AE) 2-dimensional latent space visualization. The data points represent the encoded latent code of images from the MNIST dataset and are colored based on their corresponding label (digit). The latent space is not organized in a way that allows us to visually separate these classes." />
<figcaption>Figure 29: Trained Autoencoder (AE) <span
class="math inline">\(2\)</span>-dimensional latent space visualization.
The data points represent the encoded latent code of images from the
MNIST dataset and are colored based on their corresponding label
(digit). The latent space is not organized in a way that allows us to
visually separate these classes.</figcaption>
</figure>
<p>The <span class="math inline">\(2\)</span>-dimensional latent space
can be observed in <a href="#fig:gai_autoencoder_latent">Fig 29</a>. Our
latent space is not organized in a way that we can visually distinguish
between the digit classes. This clearly demonstrates a lack of
structural organization preventing the AE from being used as a generator
by sampling its latent space.</p>
<figure id="fig:gai_autoencoder_latent_sampling">
<img src="./figures/core_gai_autoencoder_latent_sampling.svg"
alt="Trained Autoencoder (AE) 2-dimensional latent space sampling visualization. The decoder is used for generation by sampling the latent space in a grid pattern." />
<figcaption>Figure 30: Trained Autoencoder (AE) <span
class="math inline">\(2\)</span>-dimensional latent space sampling
visualization. The decoder is used for generation by sampling the latent
space in a grid pattern.</figcaption>
</figure>
<h4 id="sec:vae">Variational Autoencoders</h4>
<p>Due to a lack of latent space regularization as shown in the previous
sub-section, AE cannot be used without any hacking to generate, or
produce unseen samples. A vanilla AE does not encode any structure on
the latent space. It is trained only for reconstruction and is thus
subject to high overfitting resulting in a meaningless structural
organization of the latent codes. The Variational Autoencoder (VAE)
architecture <span class="citation" data-cites="kingma_2013">[<a
href="#ref-kingma_2013" role="doc-biblioref">38</a>]</span> is one
answer to this issue. It can be viewed as a special AE hacked by adding
a regularization objective enabling generation by exploring the learned
and structured latent space (see <a href="#fig:gai_vae">Fig 31</a>).</p>
<figure id="fig:gai_vae">
<img src="./figures/core_gai_vae.svg"
alt="+vae architecture. The encoder compresses the input and regresses the latent distribution parameters \mu and \rho from which a latent code is sampled using the reparametrization trick with a surrogate parameter \epsilon sampled from the standard Gaussian distribution and then decoded to recover the input using the decoder. architecture." />
<figcaption>Figure 31: <span class="full">+vae architecture. The encoder
compresses the input and regresses the latent distribution parameters
<span class="math inline">\(\mu\)</span> and <span
class="math inline">\(\rho\)</span> from which a latent code is sampled
using the reparametrization trick with a surrogate parameter <span
class="math inline">\(\epsilon\)</span> sampled from the standard
Gaussian distribution and then decoded to recover the input using the
decoder.</span> architecture.</figcaption>
</figure>
<p><strong>Regularization:</strong> VAEs are topologically similar to
AE. They possess an encoder to compress the input into a latent code,
and a decoder to reconstruct the signal from it. However, instead of
encoding the input as a single point, it encodes it as a distribution in
the latent space. In practice, the distribution used is chosen to be
close to a normal distribution. The encoder is changed to output the
parameters of this distribution, the mean <span
class="math inline">\(\mu\)</span>, and the variance <span
class="math inline">\(\sigma^2\)</span>. <span
class="math inline">\(\sigma^2\)</span> is often replaced by a proxy
<span class="math inline">\(\rho = log(\sigma^2)\)</span> to enforce
positivity and stability. The new inference scheme is changed for <span
class="math inline">\(\hat{x} = D(z)\)</span>, where the latent code
<span class="math inline">\(z \sim \mathcal{N}(E(x)_\mu,
exp(E(x)_\rho))\)</span>.</p>
<p><strong>Probabilistic Formulation:</strong> Let us consider the VAE
as a probabilistic model. <span class="math inline">\(x\)</span>, our
data, is generated from the latent variable <span
class="math inline">\(z\)</span> that cannot be observed. In this
framework, the generation steps are the following: <span
class="math inline">\(z\)</span> is sampled from the prior distribution
<span class="math inline">\(p(z)\)</span>, and <span
class="math inline">\(x\)</span> is sampled from the conditional
likelihood <span class="math inline">\(x \sim p(x | z)\)</span>. In this
setting, the probabilistic decoder is <span class="math inline">\(p(x |
z)\)</span>, and the probabilistic encoder is <span
class="math inline">\(p(z | x)\)</span>. The Bayes theorem allows
expressing a natural relation between the prior <span
class="math inline">\(p(z)\)</span>, the likelihood <span
class="math inline">\(p(x | z)\)</span>, and the posterior <span
class="math inline">\(p(z|x)\)</span>:</p>
<p><span id="eq:vae_bayes"><span class="math display">\[
p(z | x) = \frac{p(x | z)p(z)}{p(x)} = \frac{p(x | z)p(z)}{\int p(x | u)
p(u) du}
\qquad{(7)}\]</span></span></p>
<p>A standard Gaussian distribution is often assumed for the prior <span
class="math inline">\(p(z)\)</span>, and a parametric Gaussian for the
likelihood <span class="math inline">\(p(x | z)\)</span> with its mean
being defined by a deterministic function <span class="math inline">\(f
\in F\)</span> and a positive constant <span class="math inline">\(c
\cdot I\)</span> for the covariance. In this setting:</p>
<p><span id="eq:vae_gaussian"><span class="math display">\[
\begin{aligned}
p(z)     &amp;\sim \mathcal{N}(0, I) \\
p(x | z) &amp;\sim \mathcal{N}(f(z), cI), \; f \in F, \; c &gt; 0
\end{aligned}
\qquad{(8)}\]</span></span></p>
<p>These equations (see Eqns <a href="#eq:vae_bayes">7</a>, <a
href="#eq:vae_gaussian">8</a>) define a classical Bayesian Inference
problem. This problem is however intractable because of the
denominator’s integral <span class="math inline">\(\int p(x | u) p(u)
du\)</span> and thus requires the use of approximation techniques.</p>
<p><strong>Variational Inference</strong>: In statistics, Variational
Inference (VI) is one of the techniques used to approximate complex
distributions. It consists in setting a parametrized distribution
family, in our case Gaussians with its mean and covariance, and
searching for the best approximation of the target distribution in this
family. To search for the best candidate, we use the Kullback-Leibler
Divergence (KL-Divergence) between the approximation and the target and
minimize it with Gradient Descent (GD).</p>
<p>Let us approximate the posterior <span
class="math inline">\(p(z|x)\)</span> using VI with a Gaussian
distribution <span class="math inline">\(q_x(z)\)</span> with a mean
<span class="math inline">\(g(x) \in G\)</span> and covariance <span
class="math inline">\(h(x) \in H\)</span> where <span
class="math inline">\(q_x(z) \sim \mathcal{N}(g(x), h(x))\)</span>. we
can now look for the optimal <span class="math inline">\(g^*\)</span>
and <span class="math inline">\(h^*\)</span> minimizing the
KL-Divergence between the target and the approximation:</p>
<p><span id="eq:vae_objective"><span class="math display">\[
\begin{aligned}
(g^*, h^*) &amp;= \underset{(g, h) \in G \times H}{arg \; min} KL(q_x(z)
|| p(z | x)) \\
           &amp;= \underset{(g, h) \in G \times H}{arg \; min} (E_{z
\sim q_x} \; log \; q_x(z) - E_{z \sim q_x} \; log \; \frac{p(x | z)
p(z)}{p(x)}) \\
           &amp;= \underset{(g, h) \in G \times H}{arg \; min} (E_z \;
log \; q_x(z) - E_z \; log \; p(z) - E_z \; log \; p(x | z) + E_z \; log
\; p(x)) \\
           &amp;= \underset{(g, h) \in G \times H}{arg \; min} (E_z [log
\; p(x | z) - KL(q_x(z) || p(z)]) \\
           &amp;= \underset{(g, h) \in G \times H}{arg \; min} (E_z \;
log \; p(x | z) - KL(q_x(z) || p(z)) \\
           &amp;= \underset{(g, h) \in G \times H}{arg \; min} (E_z
[-\frac{||x - f(z)||^2}{2c}] - KL(q_x(z) || p(z)) \\
\end{aligned}
\qquad{(9)}\]</span></span></p>
<p>This rewrite of the objective equations demonstrates a natural
tradeoff between the data confidence <span class="math inline">\(E_z
[-\frac{||x - f(z)||^2}{2c}]\)</span> and the prior confidence <span
class="math inline">\(KL(q_x(z) || p(z))\)</span>. The first term
describes a reconstruction loss where the decoder parametrized by the
function <span class="math inline">\(f \in F\)</span> has to recover the
input <span class="math inline">\(x\)</span> from the latent code <span
class="math inline">\(z\)</span>, and the second term a regularization
objective between <span class="math inline">\(q_x(z)\)</span> and the
prior <span class="math inline">\(p(z)\)</span> which is gaussian. We
can view the constant <span class="math inline">\(c\)</span> as a
strength parameter that can adjust how we favor the regularization.</p>
<p><strong>Reparametrization Trick:</strong> The VAE architecture is
trained to find the parameters of the functions <span
class="math inline">\(f\)</span>, <span
class="math inline">\(g\)</span>, and <span
class="math inline">\(h\)</span> by minimizing the VI objective (see <a
href="#eq:vae_objective">Eq 9</a>). The encoder is charged to output two
vectors, one for representing <span class="math inline">\(g(x)\)</span>
the mean, in the case of a Gaussian distribution <span
class="math inline">\(\mu\)</span>, and the other representing the
variance of the distribution <span class="math inline">\(h(x)\)</span>,
<span class="math inline">\(\rho = log(\sigma^2)\)</span>. The latent
code <span class="math inline">\(z\)</span> is then sampled from the
Gaussian distribution <span class="math inline">\(z \sim
\mathcal{N}(\mu, \sigma)\)</span> and finally decoded to reconstruct the
original input <span class="math inline">\(x\)</span>.</p>
<p>There is however a catch. The sampling process is stochastic and thus
not differentiable. And we know that a NN needs to be differentiable to
be optimized using SGD. To solve this problem, Kingma et al. <span
class="citation" data-cites="kingma_2013">[<a href="#ref-kingma_2013"
role="doc-biblioref">38</a>]</span> propose to use what they call a
reparametrization trick. It consists in sampling a surrogate standard
Gaussian distribution <span class="math inline">\(\epsilon \sim
\mathcal{N}(0, I)\)</span> and scaling it by the output of the learned
encoder <span class="math inline">\(\mu\)</span> and <span
class="math inline">\(\sigma^2\)</span>. This the process becomes:</p>
<p><span id="eq:vae_reparametrization"><span class="math display">\[
\begin{aligned}
E(x)    &amp;= (\mu, \rho) \\
\hat{x} &amp;= D(\mu + \epsilon \; exp(\rho)), \; \epsilon \sim N(O, I)
\end{aligned}
\qquad{(10)}\]</span></span></p>
<p>Performing the latent sampling using the reparametrization trick (see
<a href="#eq:vae_reparametrization">Eq 10</a>) conserves the gradient
flow. The VAE can thus be trained to learn a structured latent space
that can be used to interpolate latent codes and decode them into
samples similar to the training distribution.</p>
<p><strong>MNIST Digit Image Generation:</strong> Let us reconsider our
toy example with training a VAE on the MNIST handwritten digit dataset.
There is almost nothing change in comparison to the AE.</p>
<p>We implement a <code>GaussianDistribution</code> class helper for
handling the parametrized Gaussian distribution taking the output
parameters <span class="math inline">\((\mu, \rho)\)</span> from the
encoder in its constructor from which the mean <span
class="math inline">\(\mu\)</span> and variance <span
class="math inline">\(\sigma^2\)</span> are derived. The class is
augmented with utility functions such as <code>sample</code> to sample
the distribution using the reparametrization trick, and a
<code>kld</code> function to compute the average KL-Divergence
regularization loss when compared with a standard Gaussian distribution
target prior.</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> Tensor</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Parametrized Gaussian Distribution</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GaussianDistribution:</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params: Tensor) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mu, <span class="va">self</span>.rho <span class="op">=</span> params.chunk(<span class="dv">2</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.std <span class="op">=</span> torch.exp(<span class="fl">0.5</span> <span class="op">*</span> <span class="va">self</span>.rho)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.var <span class="op">=</span> torch.exp(<span class="va">self</span>.rho)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reparametrization Trick</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.mu <span class="op">+</span> <span class="va">self</span>.std <span class="op">*</span> torch.randn_like(<span class="va">self</span>.mu)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Average KL-Divergence with Gaussian Prior</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> kld(<span class="va">self</span>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>        kld <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (<span class="va">self</span>.mu.<span class="bu">pow</span>(<span class="dv">2</span>) <span class="op">+</span> <span class="va">self</span>.var <span class="op">-</span> <span class="fl">1.0</span> <span class="op">-</span> <span class="va">self</span>.rho)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.mean(torch.<span class="bu">sum</span>(kld, dim<span class="op">=</span><span class="dv">1</span>), dim<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div>
<p>The network is the same as for the AE except in the encoder output
which in our case has to double its output size to encode both the mean
<span class="math inline">\(\mu\)</span> and the <span
class="math inline">\(\rho = log(\sigma^2)\)</span> of the latent
distribution.</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> OrderedDict</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> (Linear, ReLU, Sequential, Sigmoid)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Model definition (encoder out: mu and rho)</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential(OrderedDict(</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    encoder<span class="op">=</span>Sequential(</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        Linear(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>,  <span class="dv">256</span>), ReLU(),</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        Linear(    <span class="dv">256</span>, <span class="dv">2</span> <span class="op">*</span> <span class="dv">2</span>),</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    decoder<span class="op">=</span>Sequential(</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        Linear(  <span class="dv">2</span>,     <span class="dv">256</span>), ReLU(),</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        Linear(<span class="dv">256</span>, <span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>), Sigmoid(),</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>))</span></code></pre></div>
<p>The encoder can then be used to compress the input into the latent
distribution parameters from which the latent code can be sampled and
decoded using the decoder. The loss is a combination of the
reconstruction term using the <code>binary_cross_entropy</code>, and the
regularization term using the <code>kld</code> computed from the
posterior distribution helper. The KL-Divergence term is weighted by the
<code>kld_weight</code> and is set to <span
class="math inline">\(0\)</span> at the beginning of training and slowly
increased toward a defined weight set to <span
class="math inline">\(1e-4\)</span> in this case.</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.functional <span class="im">import</span> binary_cross_entropy</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute output</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>posterior <span class="op">=</span> GaussianDistribution(model.encoder(x))</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>x_ <span class="op">=</span> model.decoder(posterior.sample())</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute loss</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>loss_reco <span class="op">=</span> binary_cross_entropy(x_, x)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>loss_kld <span class="op">=</span> p.kld()</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> loss_reco <span class="op">+</span> kld_weight <span class="op">*</span> loss_kld</span></code></pre></div>
<p>The model is trained to minimize the objective functions. Satisfying
both the reconstruction loss and the KL-Divergence regularization is
harder than the task of the vanilla AE. Intuitively this has to result
in a structured latent space at the cost of a small reconstruction
quality degradation. The training history and reconstruction
capabilities are shown in <a href="#fig:gai_vae_history">Fig 32</a>.</p>
<figure id="fig:gai_vae_history">
<img src="./figures/core_gai_vae_history.svg"
alt="Training history of a small Variational Autoencoder (VAE). The combination of the binary cross entropy loss and the KL-Divergence regularization is shown on the left, a training sample in the middle, and its corresponding reconstruction on the right." />
<figcaption>Figure 32: Training history of a small Variational
Autoencoder (VAE). The combination of the binary cross entropy loss and
the KL-Divergence regularization is shown on the left, a training sample
in the middle, and its corresponding reconstruction on the
right.</figcaption>
</figure>
<p>As expected, the latent space (see <a
href="#fig:gai_vae_latent">Fig 33</a>) presents structural organization.
The specific digit classes are visible and separable.</p>
<figure id="fig:gai_vae_latent" width="60%">
<img src="./figures/core_gai_vae_latent.svg" style="width:60.0%"
alt="Trained Variational Autoencoder (VAE) 2-dimensional latent space visualization. The data points represent the encoded latent code of images from the MNIST dataset and are colored based on their corresponding label (digit). The latent space is organized in a way that allows us to visually separate these classes." />
<figcaption>Figure 33: Trained Variational Autoencoder (VAE) <span
class="math inline">\(2\)</span>-dimensional latent space visualization.
The data points represent the encoded latent code of images from the
MNIST dataset and are colored based on their corresponding label
(digit). The latent space is organized in a way that allows us to
visually separate these classes.</figcaption>
</figure>
<p>The structure of the trained VAE latent space allows the generation
of new data points by sampling latent codes and decoding them using the
trained decoder (see <a
href="#fig:gai_vae_latent_sampling">Fig 34</a>).</p>
<figure id="fig:gai_vae_latent_sampling">
<img src="./figures/core_gai_vae_latent_sampling.svg"
alt="Trained Variational Autoencoder (VAE) 2-dimensional latent space sampling visualization. The decoder is used for generation by sampling the latent space in a grid pattern." />
<figcaption>Figure 34: Trained Variational Autoencoder (VAE) <span
class="math inline">\(2\)</span>-dimensional latent space sampling
visualization. The decoder is used for generation by sampling the latent
space in a grid pattern.</figcaption>
</figure>
<h4 id="sec:gan">Generative Adversarial Networks</h4>
<p>While VAE allow learning structured latent spaces ready for sampling
and generation, they suffer from quality degradation. The Generative
Adversarial Network (GAN) architecture, introduced by Ian Goodfellow et
al. <span class="citation" data-cites="goodfellow_2014">[<a
href="#ref-goodfellow_2014" role="doc-biblioref">20</a>]</span>, is one
answer to this problem. GANs can generate high-quality images in
real-time settings.</p>
<p><strong>Vanilla GAN:</strong> In its vanilla formulation, a GAN
consists of a generator <span class="math inline">\(G\)</span> trained
to produce images <span class="math inline">\(G(z)\)</span> similar to
the training distribution given a latent code <span
class="math inline">\(z\)</span> which is then fed to a discriminator
<span class="math inline">\(D\)</span> trained to differentiate fake
images (generated images) from true images <span
class="math inline">\(x\)</span>. The two networks are jointly trained
in an end-to-end fashion to optimize a Min-Max objective (see <a
href="#eq:gan_minmax">Eq 11</a>) that can be split into two objectives,
one for the generator, and another for the discriminator (see <a
href="#eq:gan_minmax_split">Eq 12</a>).</p>
<p><span id="eq:gan_minmax"><span class="math display">\[
\underset{D}{min} \; \underset{G}{max} \; E_x \; log \; D(x) + E_z \;
log(1 - D(G(z))) \\
\qquad{(11)}\]</span></span></p>
<p><span id="eq:gan_minmax_split"><span class="math display">\[
\begin{aligned}
\frac{1}{n} \sum_{i=1}^{n} \; &amp;log \; D(x_i) + log(1 - D(G(z_i))) \\
\frac{1}{n} \sum_{i=1}^{n} \; &amp;log \; D(G(z_i))
\end{aligned}
\qquad{(12)}\]</span></span></p>
<p><strong>Vanishing Gradients and Mode Collapse</strong>: In practice,
the vanilla formulation is however unstable. The generator training
often saturates if it cannot keep up with the discriminator training
which is in most cases easier to satisfy. It suffers from vanishing
gradients where the loss signal becomes too small and gradients do not
propagate to layers resulting in the early stop of the generator
training. It is also subject to mode collapse where the generator finds
a simple solution fooling the discriminator and failing at generating
diverse enough outputs. Solutions such as the hinge loss <span
class="citation" data-cites="lim_2017">[<a href="#ref-lim_2017"
role="doc-biblioref">45</a>]</span>, the Wasserstein distance <span
class="citation" data-cites="arjovsky_2017">[<a
href="#ref-arjovsky_2017" role="doc-biblioref">3</a>]</span>, gradient
penalty <span class="citation" data-cites="ishaan_2017">[<a
href="#ref-ishaan_2017" role="doc-biblioref">22</a>]</span>, the use of
batch <span class="citation" data-cites="ioffe_2015">[<a
href="#ref-ioffe_2015" role="doc-biblioref">31</a>]</span> and spectral
<span class="citation" data-cites="miyato_2018">[<a
href="#ref-miyato_2018" role="doc-biblioref">52</a>]</span>
normalization can be found in the literature.</p>
<p><strong>Wasserstein GAN:</strong> The most famous improvement of the
vanilla GAN is the Wasserstein Generative Adversarial Network (WGAN)
<span class="citation" data-cites="arjovsky_2017">[<a
href="#ref-arjovsky_2017" role="doc-biblioref">3</a>]</span>. It both
resolves the mode collapse and limits the vanishing gradients issues.
The last activation of the discriminator is swapped from a sigmoid to a
linear activation. This small change turns the discriminator into a
critic network in charge of scoring the quality (fidelity to the
original distribution) of input images. The new simplified objectives
are shown in <a href="#eq:gan_wasserstein">Eq 13</a> and are often
combined with gradient clipping to satisfy a Lipschitz constraint.</p>
<p><span id="eq:gan_wasserstein"><span class="math display">\[
\begin{aligned}
\frac{1}{n} \sum_{i=1}^{n} \; &amp; D(x_i) - D(G(z_i)) \\
\frac{1}{n} \sum_{i=1}^{n} \; &amp; D(G(z_i))
\end{aligned}
\qquad{(13)}\]</span></span></p>
<p><strong>Gradient Penalty:</strong> In their contribution, Gulrajani
et al. <span class="citation" data-cites="gulrajani_2017">[<a
href="#ref-gulrajani_2017" role="doc-biblioref">21</a>]</span> propose
to replace gradient clipping with gradient penalty to enforce a
constraint on the critic such that its gradients with respect to the
inputs are unit vectors. The critic loss is thus augmented with an
additional term (see <a href="#eq:gan_gp">Eq 14</a>) where <span
class="math inline">\(\hat{x}\)</span> is sampled from a linear
interpolation between real and fake samples to satisfy the critic’s
Lipschitz constraint.</p>
<p><span id="eq:gan_gp"><span class="math display">\[
\lambda \; E_{\hat{x}} \; (||\nabla_{\hat{x}}D(\hat{x})||_2 - 1)^2
\qquad{(14)}\]</span></span></p>
<p><strong>MNIST Digit Image Generation:</strong> Let us reconsider the
MNIST dataset and train a GAN to enable qualitative generation by
providing a latent code to the generator. We first need to define the
generator <span class="math inline">\(G\)</span> model and the critic
network <span class="math inline">\(C\)</span>. In this case, we keep
the same architecture for both. They are <span
class="math inline">\(3\)</span>-layer NNs with a sigmoid activation for
the generator and a linear activation for the critic. For the sake of
the example and comparability with the previous generative architecture,
we choose a latent dimension of <span
class="math inline">\(2\)</span>.</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> (Linear, ReLU, Sequential, Sigmoid)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Generator model</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> Sequential(</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    Linear(  <span class="dv">2</span>,      <span class="dv">256</span>), ReLU(),</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    Linear(<span class="dv">256</span>,      <span class="dv">256</span>), ReLU(),</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    Linear(<span class="dv">256</span>,  <span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>), Sigmoid(),</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Critic model</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>critic <span class="op">=</span> Sequential(</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    Linear(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>, <span class="dv">256</span>), ReLU(),</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    Linear(    <span class="dv">256</span>, <span class="dv">256</span>), ReLU(),</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    Linear(    <span class="dv">256</span>, <span class="dv">1</span>),</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>).cuda()</span></code></pre></div>
<p>Even though the formulation of the Minmax objective is common, both
networks require their own optimizer in practice. We thus define the
optimizer for the generator and the critic. Their hyperparameters can be
different and tweaked to further optimize the training. For the sake of
simplicity, we keep the default parameters in this toy example.</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> AdamW</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimizers (one per model)</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>g_optim <span class="op">=</span> AdamW(generator.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>c_optim <span class="op">=</span> AdamW(critic.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span></code></pre></div>
<p>Latent variables are sampled from the standard normal distribution
<span class="math inline">\(z \sim \mathcal{N}(O, I)\)</span>. The
<code>gen_latent_code</code> is a helper that generates such latent
codes given a mini-batch size as DL frameworks work on the batch
level.</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate latent code</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>gen_latent_code <span class="op">=</span> <span class="kw">lambda</span> B: torch.randn((B, <span class="dv">2</span>))</span></code></pre></div>
<p>One generator training step consists in sampling a latent code <span
class="math inline">\(z\)</span>, using it to generate a handwritten
digit fake image <span class="math inline">\(G(z)\)</span> and computing
the average generator hinge/Wasserstein loss <span
class="math inline">\(ReLU(1 - C(G(z)))\)</span>. This loss is then used
to backpropagate gradients with respect to the generator’s parameters.
The generator’s weights are finally updated using the Adam optimizer
policy.</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> Tensor</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> Module</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generator_step(generator: Module, optim: AdamW, real: Tensor) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    fake <span class="op">=</span> generator(gen_latent_code(real.shape[<span class="dv">0</span>]))</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> torch.relu(<span class="fl">1.0</span> <span class="op">-</span> critic(fake)).mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    optim.step()</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    optim.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>For stability purposes and to satisfy the <span
class="math inline">\(1\)</span>-Lipschitz constraint of the critic
network we implement a <code>grad_penalty</code> helper for computing
the gradient penalty term. It consists of sampling interpolations
between fake and real samples <span class="math inline">\(t = \alpha
x_{real} + (1 - \alpha) x_{fake}\)</span> with <span
class="math inline">\(\alpha \sim \mathcal{U}(0, 1)\)</span>, scoring
them using the critic network <span class="math inline">\(C\)</span>. We
then compute the gradients with respect to the critic’s parameters and
optimize them to be close to <span class="math inline">\(1\)</span>,
<span class="math inline">\(\; E_{\hat{x}} \;
(||\nabla_{\hat{x}}D(\hat{x})||_2 - 1)^2\)</span>.</p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> Tensor</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.autograd <span class="im">import</span> grad</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> Module</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Gradient Penatly for Critic</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> grad_penalty(critic: Module, real: Tensor, fake: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> torch.rand((real.shape[<span class="dv">0</span>], <span class="dv">1</span>), requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> alpha <span class="op">*</span> real <span class="op">-</span> (<span class="dv">1</span> <span class="op">-</span> alpha) <span class="op">*</span> fake</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    mixed <span class="op">=</span> critic(t)</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> grad(mixed, t, torch.ones_like(mixed), <span class="va">True</span>, <span class="va">True</span>, <span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> grads.view(grads.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    grads <span class="op">=</span> grads.norm(<span class="dv">2</span>, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.mean((grads <span class="op">-</span> <span class="dv">1</span>).<span class="bu">pow</span>(<span class="dv">2</span>))</span></code></pre></div>
<p>One critic training step consists in sampling a latent code <span
class="math inline">\(z\)</span>, using it to generate a fake image
<span class="math inline">\(G(z)\)</span> and computing the three
average critic’s loss terms <span class="math inline">\(ReLU(1 +
C(G(z)))\)</span>, $ReLU(1 - C(x)), and <span
class="math inline">\(\lambda \nabla_{gp}\)</span> with <span
class="math inline">\(\lambda = 10\)</span> to minimize. The losses are
then backward and the resulting gradients are used to update the
critic’s weights using the Adam policy.</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> Tensor</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> Module</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> critic_step(critic: Module, optim: AdamW, real: Tensor) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>        fake <span class="op">=</span> generator(gen_latent_code(real.shape[<span class="dv">0</span>]))</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    loss_fake <span class="op">=</span> torch.relu(<span class="fl">1.0</span> <span class="op">+</span> critic(fake)).mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>    loss_real <span class="op">=</span> torch.relu(<span class="fl">1.0</span> <span class="op">-</span> critic(real)).mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>    loss_gp <span class="op">=</span> grad_penalty(critic, real.detach(), fake.detach())</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> loss_fake <span class="op">+</span> loss_real <span class="op">+</span> <span class="dv">10</span> <span class="op">*</span> loss_gp</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    optim.step()</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    optim.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>The overall training loop consists in alternating between generator
training steps and critic training steps until convergence. It is common
to train the critic for more cycles, especially at the beginning of
training where the training signal for the generator is not constructive
enough. In this toy example, we optimize the critic five times more than
the generator.</p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training Loop</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> real <span class="kw">in</span> loader:</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train the Generator</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>        generator_step(generator, g_optim, real)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train the Critic</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>            critic_step(critic, c_optim, real)</span></code></pre></div>
<p>The training history monitoring the generator’s and critic’s loss
terms are shown in <a href="#fig:gai_gan_history">Fig 35</a>. GAN are in
practice hard to train and monitoring each term independently allows us
to diagnose mode collapse and vanishing gradients when the gradient
distributions are checked.</p>
<figure id="fig:gai_gan_history">
<img src="./figures/core_gai_gan_history.svg"
alt="Training history of a small Generative Adversarial Network (GAN). The objective terms are plotted separately and follow a hinge loss and Wasserstein objective. The loss for the generator is shown in orange and the losses for the critic in blue." />
<figcaption>Figure 35: Training history of a small Generative
Adversarial Network (GAN). The objective terms are plotted separately
and follow a hinge loss and Wasserstein objective. The loss for the
generator is shown in orange and the losses for the critic in
blue.</figcaption>
</figure>
<p>Overall, the GAN architecture is highly efficient at generating new
samples giving latent codes. The quality of the generation is superior
to those of VAE at the cost of a small loss in variation (see <a
href="#fig:gai_gan_latent_sampling">Fig 36</a>).</p>
<figure id="fig:gai_gan_latent_sampling">
<img src="./figures/core_gai_gan_latent_sampling.svg"
alt="Trained Generative Adversarial Network (GAN) 2-dimensional latent space sampling visualization. The generator is used for generation by sampling the latent space in a grid pattern." />
<figcaption>Figure 36: Trained Generative Adversarial Network (GAN)
<span class="math inline">\(2\)</span>-dimensional latent space sampling
visualization. The generator is used for generation by sampling the
latent space in a grid pattern.</figcaption>
</figure>
<h4 id="sec:ddm">Denoising Diffusion Models</h4>
<p>Generative Adversarial Network (GAN) architectures have been the
framework of choice when approaching image generation tasks for
real-time or near real-time applications with perceptual quality needs.
However, a more recent proposal named Denoising Diffusion Model (DDM)
<span class="citation" data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">28</a>]</span> is currently challenging this
position. They allow for better quality generation and free user-guided
controls such as in-paint, out-painting, super-resolution, and more at
the cost of inference time, at least concerning its vanilla
formulation.</p>
<p>The DDM architecture is inspired by non-equilibrium thermodynamics
and consists of a Markov chain on diffusion steps slowly adding Gaussian
noise to the data. The task is then to learn the reverse operation to
reconstruct the original data from noise. In this particular framework,
the latent code is as big as the input tensor and reversed using a fixed
procedure.</p>
<p><strong>Forward Diffusion:</strong> Let us consider a data point
sampled from real data distribution <span class="math inline">\(x_0 \sim
q(x)\)</span>. The forward diffusion process consists of adding small
and successive Gaussian noise to the initial data sample during <span
class="math inline">\(T\)</span> steps. The chained noisy
transformations produce <span class="math inline">\(x_1, \dots,
x_T\)</span> samples (see <a href="#eq:ddm_froward">Eq 15</a>). The step
size is given by a variance <span
class="math inline">\(\beta\)</span>-scheduler <span
class="math inline">\({\beta \in [0; 1]}_{t=1}^{T}\)</span>.</p>
<p><span id="eq:ddm_froward"><span class="math display">\[
\begin{aligned}
q(x_t | x_{t - 1}) = \mathcal{N}(x_t; \sqrt{1 -  \beta_t} x_{t - 1},
\beta_t I) \\
q(x_{1:T} | x_0) = \prod_{t = 1}^{T} q(x_t | x_{t - 1})
\end{aligned}
\qquad{(15)}\]</span></span></p>
<p>By adding noise to the input data in small enough steps, when <span
class="math inline">\(T \rightarrow +\infty\)</span>, <span
class="math inline">\(x_T\)</span> converge towards an isotropic
Gaussian distribution. One feature of the forward diffusion process is
that <span class="math inline">\(x\)</span> can be computed in a closed
form making use of the reparametrization trick introduced previously
(see <a href="#sec:vae">Sec 2.2.4.2</a>). Defining <span
class="math inline">\(\alpha_t = 1 - \beta_t\)</span> and <span
class="math inline">\(\bar{\alpha_t} = \prod_{i=1}^{t}
\alpha_i\)</span>,</p>
<p><span id="eq:ddm_froward_closed"><span class="math display">\[
\begin{aligned}
x_t &amp;= \sqrt{\alpha_t} x_{t - 1} + \sqrt{1 - \alpha_t}
\epsilon_{t-1} \\
    &amp;= \sqrt{\alpha_t \alpha_{t - 1}} x_{t - 2} + \sqrt{1 - \alpha_t
\alpha_{t - 1}} \epsilon_{t-2} \\
    &amp;= \dots \\
    &amp;= \sqrt{\bar{\alpha_t}} x_{0} + \sqrt{1 - \bar{\alpha_t}}
\epsilon_0
\end{aligned}
\qquad{(16)}\]</span></span></p>
<p>where <span class="math inline">\(\epsilon_t \sim \mathcal{N}(0,
I)\)</span> and <span class="math inline">\(\epsilon_{t-2}\)</span>
results from merging two Gaussian distributions, <span
class="math inline">\(\mathcal{N}(0, \sigma_1^2 I) + \mathcal{N}(0,
\sigma_2^2 I) = \mathcal{N}(0, (\sigma_1^2 + \sigma_2^2)
I)\)</span>.</p>
<p>To summarize, <span class="math inline">\(x_t\)</span> can be sampled
as follow:</p>
<p><span id="eq:ddm_froward_xt"><span class="math display">\[
x_t \sim q(x_t | x_0) = \mathcal{N}(x_T; \sqrt{\bar{\alpha_t}} x_0, (1 -
\bar{\alpha}_t) I)
\qquad{(17)}\]</span></span></p>
<p><strong>Beta Schedule:</strong> <span
class="math inline">\(\beta_t\)</span>, the variance parameter can be
fixed to a constant or chosen using a schedule over <span
class="math inline">\(T\)</span> timesteps (see <a
href="#fig:gai_ddpm_forward">Fig 37</a>). In the original paper <span
class="citation" data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">28</a>]</span> and follow-up contribution <span
class="citation" data-cites="nichol_2021">[<a href="#ref-nichol_2021"
role="doc-biblioref">55</a>]</span>, the authors propose a linear (<span
class="math inline">\(\beta_1=1e^{-4}\)</span>, <span
class="math inline">\(\beta_T=0.02\)</span>), a quadratic, and a cosine
schedule. Their experiments show that the cosine schedule results are
better.</p>
<figure id="fig:gai_ddpm_forward">
<img src="./figures/core_gai_ddpm_forward.svg"
alt="Visualization of a Denoising Diffusion Probabilistic Model (DDPM) forward diffusion with 1,000 timesteps applied to a handwritten image of a digit extracted from the MNIST training dataset." />
<figcaption>Figure 37: Visualization of a Denoising Diffusion
Probabilistic Model (DDPM) forward diffusion with <span
class="math inline">\(1,000\)</span> timesteps applied to a handwritten
image of a digit extracted from the MNIST training dataset.</figcaption>
</figure>
<p><strong>Backward Diffusion:</strong> The backward diffusion process,
also called reverse diffusion, consists in learning the reverse mapping
<span class="math inline">\(q(x_{t-1} | x_t)\)</span>. By taking into
account that with enough steps <span class="math inline">\(T \rightarrow
+\infty\)</span>, the latent variable <span
class="math inline">\(x_T\)</span> follows an isotropic Gaussian
distribution, <span class="math inline">\(x_T\)</span> can be sampled
from <span class="math inline">\(\mathcal{0, I}\)</span> and <span
class="math inline">\(x_0\)</span> reconstructed by successively
applying this process resulting in <span
class="math inline">\(q(x_0)\)</span>, a novel data sample from the
training data distribution.</p>
<p>The reverse transformation <span class="math inline">\(q(x_{t-1} |
x_t)\)</span> is however intractable. It would require sampling the
entire data distribution. Similarly to the VAE, <span
class="math inline">\(q(x_{t-1} | x_t)\)</span> is approximated using a
parametrized model <span class="math inline">\(p_\theta\)</span>, in our
case a NN. For small enough steps, <span
class="math inline">\(p_\theta\)</span> can be chosen to be a Gaussian
distribution whose mean <span class="math inline">\(\mu_\theta\)</span>
and variance <span class="math inline">\(\Sigma_\theta\)</span> need to
be parameterized.</p>
<p><span id="eq:ddm_backward_param"><span class="math display">\[
\begin{aligned}
p_\theta(x_{t - 1} | x_t) = \mathcal{N}(x_{t - 1}; \mu_\theta(x_t, t),
\Sigma_\theta(x_t, t)) \\
p_\theta(x_{0:T}) = p_\theta(x_T) \prod_{t = 1}^{T} p_\theta(x_{t - 1} |
x_t)
\end{aligned}
\qquad{(18)}\]</span></span></p>
<p>We can then optimize the negative log-likelihood of the training
data. After a series of arrangements and simplifications, see the
original paper <span class="citation" data-cites="ho_2020">[<a
href="#ref-ho_2020" role="doc-biblioref">28</a>]</span> for full
derivation, the objective can be written as follow:</p>
<p><span id="eq:ddm_backward_elbo"><span class="math display">\[
\begin{aligned}
log \; p(x) &amp;\geq L_0 - L_T - \sum_{t=2}^{T} L_{t-1} \\
L_0 &amp;= E_{q(x_1 | x_0)} log \; p_\theta(x_0 | x_1) \\
L_T &amp;= D_{KL}(q(x_T | x_0) || p(x_T)) \\
L_t &amp;= E_{q(x_t | x_0)} \; D_{KL}(q(x_{t- 1 } | x_t, x_0) ||
p_\theta(x_{t - 1} | x_t))
\end{aligned}
\qquad{(19)}\]</span></span></p>
<p>where <span class="math inline">\(L_0\)</span> can be seen as a
reconstruction term, <span class="math inline">\(L_T\)</span> as a
similarity between <span class="math inline">\(x_T\)</span>’s
distribution and the standard Gaussian prior, and <span
class="math inline">\(L_t\)</span> the difference between the target
noise step and its estimation. It can be demonstrated that <span
class="math inline">\(q(x_{t - 1} | x_t)\)</span> can be made tractable
by conditioning it on <span class="math inline">\(x_0\)</span>, <span
class="math inline">\(q(x_{t - 1} | x_t, x_0)\)</span>. In this
setting:</p>
<p><span id="eq:ddm_backward_conditionned"><span class="math display">\[
\begin{aligned}
q(x_{t - 1} | x_t, x_0) &amp;= \mathcal{N}(x_{t - 1}; \tilde{\mu}(x_t,
x_0), \tilde{\beta_t} I) \\
\tilde{\beta_t} &amp;= [(1 - \bar{\alpha}_{t - 1}) \beta_t] / (1 -
\bar{\alpha}_t) \\
\tilde{\mu}(x_t, x_0) &amp;= [(\sqrt{\bar{\alpha}_{t - 1}} \beta_t) x_0
+ (\sqrt{\bar{\alpha}_t} (1 - \bar{\alpha}_{t - 1})) x_t] / (1 -
\bar{\alpha}_t)
\end{aligned}
\qquad{(20)}\]</span></span></p>
<p>Using the reparametrization trick in <a
href="#eq:ddm_froward_closed">Eq 16</a>, we can express <span
class="math inline">\(x_0\)</span> using <span
class="math inline">\(x_t\)</span> and <span
class="math inline">\(\epsilon \sim \mathcal{N}(0, I)\)</span>:</p>
<p><span id="eq:ddm_backward_x0"><span class="math display">\[
x_0 = (x_t - \sqrt{1 - \bar{\alpha}}_t \epsilon) / \sqrt{\bar{\alpha}_t}
\qquad{(21)}\]</span></span></p>
<p>Injecting <a href="#eq:ddm_backward_x0">Eq 21</a> in <a
href="#eq:ddm_backward_conditionned">Eq 20</a> allows us to express the
target mean <span class="math inline">\(\tilde{\mu}_t\)</span> to only
depend on <span class="math inline">\(x_t\)</span> and trained a NN to
approximate the noise <span class="math inline">\(\epsilon_\theta(x_t,
t)\)</span>:</p>
<p><span id="eq:ddm_backward_mutilde"><span class="math display">\[
\begin{aligned}
\tilde{\mu}_t(x_t)        &amp;= (x_t - \frac{\beta_t}{\sqrt{1 -
\bar{\alpha}}_t} \epsilon) / \sqrt{\alpha_t} \\
\tilde{\mu_\theta}_t(x_t) &amp;= (x_t - \frac{\beta_t}{\sqrt{1 -
\bar{\alpha}}_t} \epsilon_\theta(x_t, t)) / \sqrt{\alpha_t}
\end{aligned}
\qquad{(22)}\]</span></span></p>
<p>The <span class="math inline">\(L_t\)</span> denoising loss can thus
be expressed as follow:</p>
<p><span id="eq:ddm_backward_lt_rewrite"><span class="math display">\[
\begin{aligned}
L_t &amp;= E_{x_0, t, \epsilon} [\frac{1}{2 ||\Sigma_\theta(x_t,
t)||_2^2} ||\tilde{\mu}_t - \mu_\theta(x_t, t)||_2^2] \\
    &amp;= E_{x_0, t, \epsilon} [\frac{\beta_t^2}{2 \alpha_t (1 -
\bar{\alpha}_t) ||\Sigma_\theta(x_t, t)||_2^2} ||\epsilon_t -
\epsilon_\theta(x_t, t)||_2^2]
\end{aligned}
\qquad{(23)}\]</span></span></p>
<p>This tells us that the objective can be simplified to learning a
noise model to predict the noise <span
class="math inline">\(\epsilon\)</span> at timestep <span
class="math inline">\(t\)</span>. Ho et al. <span class="citation"
data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">28</a>]</span> propose to further simplify the
objective function <span class="math inline">\(L_t^{simple}\)</span> by
removing the weighting term and making the variance fixed.</p>
<p><span id="eq:ddm_backward_lt_simple"><span class="math display">\[
\begin{aligned}
L_t^{simple} &amp;= E_{x_0, t, \epsilon} [||\epsilon_t -
\epsilon_\theta(x_t, t)||_2^2] \\
x_t          &amp;= \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 -
\bar{\alpha}_t} \epsilon
\end{aligned}
\qquad{(24)}\]</span></span></p>
<p>Finally, when the model <span
class="math inline">\(\epsilon_\theta\)</span> is trained, we just need
to iteratively denoise the latent code <span class="math inline">\(z
\sim \mathcal{N}(0, I)\)</span> using:</p>
<p><span id="eq:ddm_backward_denoise"><span class="math display">\[
x_{t - 1} = \frac{1}{\sqrt{\alpha_t}} (x_t - \frac{1 - \alpha_t}{\sqrt{1
- \bar{\alpha}_t}} \epsilon\theta(x_t, t))
\qquad{(25)}\]</span></span></p>
<p><strong>MNIST Digit Image Generation:</strong> For the last time, let
us reconsider the MNIST handwritten digit dataset and train a Denoising
Diffusion Probabilistic Model (DDPM) to generate new data samples from
Gaussian distributed latent codes <span
class="math inline">\(z\)</span>. For this example, the data samples
must be normalized in the <span class="math inline">\([-1; 1]\)</span>
range.</p>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data Transformation and Normalization</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="co"># [-1, 1] range</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">2</span> <span class="op">*</span> to_tensor(x).<span class="bu">float</span>().flatten() <span class="op">-</span> <span class="dv">1</span></span></code></pre></div>
<p>Next, we define the <span
class="math inline">\(\beta\)</span>-scheduler as a function taking the
number of timesteps and precomputing the corresponding beta values
following the desired schedule. Here we only implement the
<code>linear_beta_schedule</code> defining a linear pattern between
<span class="math inline">\(1e^{-4}\)</span> and <span
class="math inline">\(0.02\)</span>.</p>
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> Tensor</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="co"># BetaSchedule type</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>BetaSchedule <span class="op">=</span> Callable[[<span class="bu">int</span>], Tensor]</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Linear Beta Schedule from Ho et al.</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear_beta_schedule(</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    T: <span class="bu">int</span>,</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    beta_0: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-4</span>,</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    beta_T: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.02</span>,</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> torch.arange(<span class="dv">0</span>, T <span class="op">+</span> <span class="dv">1</span>, dtype<span class="op">=</span>torch.float32) <span class="op">/</span> T</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (beta_T <span class="op">-</span> beta_0) <span class="op">*</span> t <span class="op">+</span> beta_0</span></code></pre></div>
<p>The <code>DDPM</code> class is implemented as a PyTorch
<code>Module</code> to store precomputed values and perform the forward
and reverse diffusion processes. Because many values such as <span
class="math inline">\(\beta_t\)</span>, <span
class="math inline">\(\alpha_t\)</span>, and <span
class="math inline">\(\bar{\alpha}_t\)</span> do not depend on <span
class="math inline">\(x_t\)</span> nor <span
class="math inline">\(t\)</span>, they can be cached to save time during
training and inference. The pre-computed values are stored as tensor
buffers to inherit the transfer to specialized devices and Automatic
Differentiation (AD) features.</p>
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> Module</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Denoising Diffusion Probabilistic Model class</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DDPM(Module):</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, T: <span class="bu">int</span>, schedule: BetaSchedule) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.T <span class="op">=</span> T</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute Beta Schedule</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        betas <span class="op">=</span> schedule(T)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        alphas <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> betas</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>        alphas_cp <span class="op">=</span> torch.cumprod(alphas, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Precompute and Store as Buffers</span></span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;betas&quot;</span>,</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>            betas,</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;alphas&quot;</span>,</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>            alphas,</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;alphas_cp&quot;</span>,</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>            alphas_cp,</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;sqrt_betas&quot;</span>,</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>            torch.sqrt(betas),</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;sqrt_recip_alphas&quot;</span>,</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>            torch.sqrt(<span class="fl">1.0</span> <span class="op">/</span> alphas),</span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;sqrt_alphas_cp&quot;</span>,</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>            torch.sqrt(alphas_cp),</span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;sqrt_one_minus_alphas_cp&quot;</span>,</span>
<span id="cb30-41"><a href="#cb30-41" aria-hidden="true" tabindex="-1"></a>            torch.sqrt(<span class="fl">1.0</span> <span class="op">-</span> alphas_cp),</span>
<span id="cb30-42"><a href="#cb30-42" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb30-43"><a href="#cb30-43" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(</span>
<span id="cb30-44"><a href="#cb30-44" aria-hidden="true" tabindex="-1"></a>            <span class="st">&quot;beta_over_sqrt_one_minus_alphas_cp&quot;</span>,</span>
<span id="cb30-45"><a href="#cb30-45" aria-hidden="true" tabindex="-1"></a>            betas <span class="op">/</span> torch.sqrt(<span class="fl">1.0</span> <span class="op">-</span> alphas_cp),</span>
<span id="cb30-46"><a href="#cb30-46" aria-hidden="true" tabindex="-1"></a>        )</span></code></pre></div>
<p>We augment the <code>DDPM</code> class with a special
<code>_extract</code> helper responsible for extracting the given
precomputed value from its timestep <span
class="math inline">\(t\)</span> and reshaping it for broadcasting and
allowing operations with image tensors.</p>
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DDPM(Module):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Helper function to extract Precomputed Buffers</span></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Along the t timestep and reshape</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _extract(<span class="va">self</span>, x: Tensor, t: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x.gather(<span class="op">-</span><span class="dv">1</span>, t).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span></code></pre></div>
<p>The <code>DDPM</code> is augmented with its corresponding
<code>forward_diffusion</code> function responsible for computing a
forward diffusion process given the initial image <span
class="math inline">\(x_0\)</span> and the given timestep <span
class="math inline">\(t\)</span>: <span class="math inline">\(x_t =
\sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t}
\epsilon\)</span>.</p>
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DDPM(Module):</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward Diffusion Step</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward_diffusion(</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>        x_0: Tensor,</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        t: Tensor,</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        noise: Tensor,</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>        extract <span class="op">=</span> partial(<span class="va">self</span>._extract, t<span class="op">=</span>t)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>            extract(<span class="va">self</span>.sqrt_alphas_cp) <span class="op">*</span> x_0 <span class="op">+</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>            extract(<span class="va">self</span>.sqrt_one_minus_alphas_cp) <span class="op">*</span> noise</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>        )</span></code></pre></div>
<p>Finally, the <code>DPPM</code> class is equipped with a
<code>reverse_diffusion</code> function responsible for computing a
single reverse diffusion step given the model noise estimate <span
class="math inline">\(\epsilon_\theta(x_t, t)\)</span>
(<code>eps</code>), the current <span
class="math inline">\(x_t\)</span>, its corresponding timestep <span
class="math inline">\(t\)</span>, and gaussian noise <span
class="math inline">\(\epsilon\)</span> (<code>noise</code>): <span
class="math inline">\(x_{t - 1} = (1/\sqrt{\alpha_t}) (x_t -
(\beta/\sqrt{1 - \bar{\alpha}_t}) \epsilon_\theta(x_t, t)) +
\sqrt{\beta_t} \epsilon\)</span></p>
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DDPM(Module):</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reverse Diffusion Step</span></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reverse_diffusion(</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>        eps: Tensor,</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a>        x_t: Tensor,</span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>        t: Tensor,</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>        noise: Tensor,</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a>        extract <span class="op">=</span> partial(<span class="va">self</span>._extract, t<span class="op">=</span>t)</span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        eps <span class="op">=</span> eps <span class="op">*</span> extract(<span class="va">self</span>.beta_over_sqrt_one_minus_alphas_cp)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>            extract(<span class="va">self</span>.sqrt_recip_alphas) <span class="op">*</span> (x_t <span class="op">-</span> eps) <span class="op">+</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>            extract(<span class="va">self</span>.sqrt_betas) <span class="op">*</span> noise</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>        )</span></code></pre></div>
<p>A simple MLP does not perform well enough using this approach. Thus,
we implement a simple CNN. The network conditioning on the timestep
<span class="math inline">\(t\)</span> is omitted for the sake of
simplicity. It is however required for a faithful implementation using a
more common U-net architecture <span class="citation"
data-cites="ronneberger_2015">[<a href="#ref-ronneberger_2015"
role="doc-biblioref">62</a>]</span>.</p>
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> OrderedDict</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> (BatchNorm2d, Conv2d, GELU, Module, Sequential)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Conv Batchnorm GELU Layer</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>CBG <span class="op">=</span> <span class="kw">lambda</span> ic, oc: Sequential(</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    Conv2d(ic, oc, <span class="dv">7</span>, <span class="dv">1</span>, <span class="dv">3</span>), BatchNorm2d(oc), GELU(),</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Noise Model</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NoiseModel(Module):</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> Sequential(</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>            CBG(<span class="dv">1</span>, h_dim <span class="op">//</span> <span class="dv">4</span>), GELU(),</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>            CBG(h_dim <span class="op">//</span> <span class="dv">4</span>, h_dim <span class="op">//</span> <span class="dv">2</span>), GELU(),</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>            CBG(h_dim <span class="op">//</span> <span class="dv">2</span>, h_dim), GELU(),</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>            CBG(h_dim, h_dim <span class="op">//</span> <span class="dv">2</span>), GELU(),</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>            CBG(h_dim <span class="op">//</span> <span class="dv">2</span>, h_dim <span class="op">//</span> <span class="dv">4</span>), GELU(),</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>            Conv2d(h_dim <span class="op">//</span> <span class="dv">4</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x_t: Tensor, t: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(x_t)</span></code></pre></div>
<p>The training procedure is straightforward and consists of sampling
<span class="math inline">\(t\)</span> and <span
class="math inline">\(\epsilon\)</span> values, and computing the
corresponding <span class="math inline">\(x_t\)</span> using forward
diffusion starting from the original images <span
class="math inline">\(x_0\)</span>. This step does not require any
gradient to be computed and stored. Next, the noise model is used to
predict the noise applied <span class="math inline">\(\epsilon\)</span>
and used to compute the objective function which is finally used for
backpropagation and SGD using the Adam policy. The training history is
shown in <a href="#fig:gai_ddpm_history">Fig 38</a>.</p>
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the Noise Model</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_step(</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    ddpm: DDPM,</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    model: NoiseModel,</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    optim: AdamW,</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    x_0: Tensor,</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>    B <span class="op">=</span> x_0.shape[<span class="dv">0</span>]</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> torch.randint(<span class="dv">0</span>, ddpm.T <span class="op">+</span> <span class="dv">1</span>, size<span class="op">=</span>(B, ))</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        noise <span class="op">=</span> torch.randn_like(x_0)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>        x_t <span class="op">=</span> ddpm.forward_diffusion(x_0, t, noise)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> model(x_t, t[:, <span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>] <span class="op">/</span> ddpm.T)</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> mse_loss(eps, noise)</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    optim.step()</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>    optim.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<figure id="fig:gai_ddpm_history">
<img src="./figures/core_gai_ddpm_history.svg"
alt="Denoising Diffusion Probabilistic Model (DDPM) training history. The noise model is trained to generate the noise value used for a DDM reverse process and generate handwritten digits resembling those of the MNIST dataset." />
<figcaption>Figure 38: Denoising Diffusion Probabilistic Model (DDPM)
training history. The noise model is trained to generate the noise value
used for a DDM reverse process and generate handwritten digits
resembling those of the MNIST dataset.</figcaption>
</figure>
<p>The sampling procedure consists of applying reverse diffusion steps
in sequence going from a <span class="math inline">\(t = T\)</span>
timestep to <span class="math inline">\(t = 0\)</span>. The trained
noise model is used to infer the noise at each step of the process. The
final image is clipped in the <span class="math inline">\([-1;
1]\)</span> range and unnormalized <span class="math inline">\([0;
1]\)</span>. Generated samples are shown in Figs <a
href="#fig:gai_ddpm_latent_sampling">39</a>, <a
href="#fig:gai_ddpm_latent_samples">40</a>.</p>
<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample using the Noise Model</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample(</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    ddpm: DDPM,</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    model: NoiseModel,</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    n_samples: <span class="bu">int</span>,</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    x_t <span class="op">=</span> torch.randn((n_samples, <span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span>))</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(ddpm.T, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> torch.tensor([t] <span class="op">*</span> n_samples)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>        eps <span class="op">=</span> model(x_t, t[:, <span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>, <span class="va">None</span>] <span class="op">/</span> ddpm.T)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>        noise <span class="op">=</span> torch.randn_like(x_t)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>        x_t <span class="op">=</span> ddpm.reverse_diffusion(eps, x_t, t, noise)</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> x_t.clip(<span class="op">-</span><span class="fl">1.0</span>, <span class="fl">1.0</span>)</span></code></pre></div>
<figure id="fig:gai_ddpm_latent_sampling">
<img src="./figures/core_gai_ddpm_latent_sampling.svg"
alt="Visualization of a sequence of Denoising Diffusion Probabilistic Model (DDPM) reverse diffusion steps applied to a Gaussian distributed latent code and a trained noise model. The result is a new data sample resembling the MNIST training data distribution" />
<figcaption>Figure 39: Visualization of a sequence of Denoising
Diffusion Probabilistic Model (DDPM) reverse diffusion steps applied to
a Gaussian distributed latent code and a trained noise model. The result
is a new data sample resembling the MNIST training data
distribution</figcaption>
</figure>
<figure id="fig:gai_ddpm_latent_samples">
<img src="./figures/core_gai_ddpm_latent_samples.svg"
alt="Selection of generated samples using a trained Denoising Diffusion Probabilistic Model (DDPM) noise model. The initial latent codes are reversed for 1,000 steps." />
<figcaption>Figure 40: Selection of generated samples using a trained
Denoising Diffusion Probabilistic Model (DDPM) noise model. The initial
latent codes are reversed for <span class="math inline">\(1,000\)</span>
steps.</figcaption>
</figure>
<h4 id="sec:llm">Large Language Models</h4>

<h2 id="ch:methodology">Methodology</h2>
<h3 id="implementation">Implementation</h3>
<h3 id="objective-evaluation">Objective Evaluation</h3>
<h3 id="subjective-evaluation">Subjective Evaluation</h3>
<h3 id="reproducibility">Reproducibility</h3>

<h1 id="core">Core</h1>
<h2 id="ch:contrib-1">Contrib I (Find Catchy Explicit Name)</h2>
<h3 id="state-of-the-art">State of the Art</h3>
<h3 id="method">Method</h3>
<h3 id="setup">Setup</h3>
<h3 id="results">Results</h3>
<h3 id="summary">Summary</h3>

<h2 id="ch:contrib-2">Contrib II (Find Catchy Explicit Name)</h2>
<h3 id="state-of-the-art-1">State of the Art</h3>
<h3 id="method-1">Method</h3>
<h3 id="setup-1">Setup</h3>
<h3 id="results-1">Results</h3>
<h3 id="summary-1">Summary</h3>

<h2 id="ch:contrib-3">Contrib III (Find Catchy Explicit Name)</h2>
<h3 id="state-of-the-art-2">State of the Art</h3>
<h3 id="method-2">Method</h3>
<h3 id="setup-2">Setup</h3>
<h3 id="results-2">Results</h3>
<h3 id="summary-2">Summary</h3>

<h2 id="ch:contrib-4">Contrib IV (Find Catchy Explicit Name)</h2>
<h3 id="state-of-the-art-3">State of the Art</h3>
<h3 id="method-3">Method</h3>
<h3 id="setup-3">Setup</h3>
<h3 id="results-3">Results</h3>
<h3 id="summary-3">Summary</h3>

<h1 id="reflection">Reflection</h1>
<h2 id="ch:ethical-and-societal-impact">Ethical and Societal Impact</h2>

<h2 id="ch:conclusion">Conclusion</h2>

<h2 class="unnumbered" id="references">References</h2>
<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-tensorflow" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div
class="csl-right-inline">Abadi, M., Barham, P., Chen, J., Chen, Z.,
Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et
al. 2016. Tensorflow: A system for large-scale machine learning.
<em>Osdi</em> (2016), 265–283.</div>
</div>
<div id="ref-john_1992" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div
class="csl-right-inline">Anderson, J.R. 1992. Automaticity and the ACT
theory. <em>The American Journal of Psychology</em>. 105, 2 (1992),
165–180. DOI:https://doi.org/<a
href="https://doi.org/10.2307/1423026">10.2307/1423026</a>.</div>
</div>
<div id="ref-arjovsky_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div
class="csl-right-inline">Arjovsky, M., Chintala, S. and Bottou, L. 2017.
<a
href="https://proceedings.mlr.press/v70/arjovsky17a.html"><span>W</span>asserstein
generative adversarial networks</a>. <em>Proceedings of the 34th
international conference on machine learning</em> (2017), 214–223.</div>
</div>
<div id="ref-ba_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">Ba,
J.L., Kiros, J.R. and Hinton, G.E. 2016. Layer normalization. <em>arXiv
preprint arXiv:1607.06450</em>. (2016).</div>
</div>
<div id="ref-bahdanau_2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div
class="csl-right-inline">Bahdanau, D., Cho, K. and Bengio, Y. 2014.
Neural machine translation by jointly learning to align and translate.
<em>arXiv preprint arXiv:1409.0473</em>. (2014).</div>
</div>
<div id="ref-foundation_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div
class="csl-right-inline">Bommasani, R., Hudson, D.A., Adeli, E., Altman,
R., Arora, S., Arx, S. von, Bernstein, M.S., Bohg, J., Bosselut, A.,
Brunskill, E., et al. 2021. On the opportunities and risks of foundation
models. <em>arXiv preprint arXiv:2108.07258</em>. (2021).</div>
</div>
<div id="ref-brown_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div
class="csl-right-inline">Brown, T., Mann, B., Ryder, N., Subbiah, M.,
Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. 2020. Language models are few-shot learners.
<em>Advances in neural information processing systems</em>. 33, (2020),
1877–1901.</div>
</div>
<div id="ref-caron_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div
class="csl-right-inline">Caron, M., Touvron, H., Misra, I., Jégou, H.,
Mairal, J., Bojanowski, P. and Joulin, A. 2021. Emerging properties in
self-supervised vision transformers. <em>Proceedings of the IEEE/CVF
international conference on computer vision</em> (2021),
9650–9660.</div>
</div>
<div id="ref-openai_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div
class="csl-right-inline">CHATGPT: Optimizing language models for
dialogue: 2023. <a
href="https://openai.com/blog/chatgpt/"><em>https://openai.com/blog/chatgpt/</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-ci_2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div
class="csl-right-inline">Ci, Y., Ma, X., Wang, Z., Li, H. and Luo, Z.
2018. <a href="https://doi.org/10.1145/3240508.3240661">User-guided deep
anime line art colorization with conditional adversarial networks</a>.
<em>Proceedings of the 26th ACM international conference on
multimedia</em> (New York, NY, USA, 2018), 1536–1544.</div>
</div>
<div id="ref-clipstudiopaint" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div
class="csl-right-inline">Clip studio PAINT: <a
href="https://www.clipstudio.net/"><em>https://www.clipstudio.net/</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-cortes_1995" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div
class="csl-right-inline">Cortes, C. and Vapnik, V. 1995. Support-vector
networks. <em>Machine Learning</em>. 20, 3 (Sep. 1995), 273–297.
DOI:https://doi.org/<a
href="https://doi.org/10.1007/BF00994018">10.1007/BF00994018</a>.</div>
</div>
<div id="ref-deng_2009" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div
class="csl-right-inline">Deng, J., Dong, W., Socher, R., Li, L.-J., Li,
K. and Fei-Fei, L. 2009. <a
href="https://doi.org/10.1109/CVPR.2009.5206848">ImageNet: A large-scale
hierarchical image database</a>. <em>2009 IEEE conference on computer
vision and pattern recognition</em> (2009), 248–255.</div>
</div>
<div id="ref-dosovitskiy_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div
class="csl-right-inline">Dosovitskiy, A., Beyer, L., Kolesnikov, A.,
Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. 2020. An image is worth 16x16 words:
Transformers for image recognition at scale. <em>arXiv preprint
arXiv:2010.11929</em>. (2020).</div>
</div>
<div id="ref-duchi_2011" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div
class="csl-right-inline">Duchi, J., Hazan, E. and Singer, Y. 2011. <a
href="http://jmlr.org/papers/v12/duchi11a.html">Adaptive subgradient
methods for online learning and stochastic optimization</a>. <em>Journal
of Machine Learning Research</em>. 12, 61 (2011), 2121–2159.</div>
</div>
<div id="ref-frans_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div
class="csl-right-inline">Frans, K. 2017. <a
href="http://arxiv.org/abs/1704.08834">Outline colorization through
tandem adversarial networks</a>. <em>CoRR</em>. abs/1704.08834,
(2017).</div>
</div>
<div id="ref-fukushima_1980" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div
class="csl-right-inline">Fukushima, K. 1980. Neocognitron: A
self-organizing neural network model for a mechanism of pattern
recognition unaffected by shift in position. <em>Biological
Cybernetics</em>. 36, 4 (Apr. 1980), 193–202. DOI:https://doi.org/<a
href="https://doi.org/10.1007/BF00344251">10.1007/BF00344251</a>.</div>
</div>
<div id="ref-furusawa_2O17" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div
class="csl-right-inline">Furusawa, C., Hiroshiba, K., Ogaki, K. and
Odagiri, Y. 2017. <a
href="https://doi.org/10.1145/3145749.3149430">Comicolorization:
Semi-automatic manga colorization</a>. <em>SIGGRAPH asia 2017 technical
briefs</em> (New York, NY, USA, 2017).</div>
</div>
<div id="ref-goodfellow_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[19] </div><div
class="csl-right-inline">Goodfellow, I.J., Bengio, Y. and Courville, A.
2016. <em>Deep learning</em>. MIT Press.</div>
</div>
<div id="ref-goodfellow_2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[20] </div><div
class="csl-right-inline">Goodfellow, I., Pouget-Abadie, J., Mirza, M.,
Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y. 2014.
<a
href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">Generative
adversarial nets</a>. <em>Advances in neural information processing
systems</em> (2014).</div>
</div>
<div id="ref-gulrajani_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[21] </div><div
class="csl-right-inline">Gulrajani, I., Ahmed, F., Arjovsky, M.,
Dumoulin, V. and Courville, A.C. 2017. <a
href="https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf">Improved
training of wasserstein GANs</a>. <em>Advances in neural information
processing systems</em> (2017).</div>
</div>
<div id="ref-ishaan_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[22] </div><div
class="csl-right-inline">Gulrajani, I., Ahmed, F., Arjovsky, M.,
Dumoulin, V. and Courville, A.C. 2017. <a
href="https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf">Improved
training of wasserstein GANs</a>. <em>Advances in neural information
processing systems</em> (2017).</div>
</div>
<div id="ref-hati_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[23] </div><div
class="csl-right-inline">Hati, Y., Jouet, G., Rousseaux, F. and Duhart,
C. 2019. <a href="https://doi.org/10.1145/3359998.3369401">PaintsTorch:
A user-guided anime line art colorization tool with double generator
conditional adversarial network</a>. <em>European conference on visual
media production</em> (New York, NY, USA, 2019).</div>
</div>
<div id="ref-hati_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[24] </div><div
class="csl-right-inline">Hati, Y., Thevenin, V., Nolot, F., Rousseaux,
F. and Duhart, C. 2023. StencilTorch: An iterative and user-guided
framework for anime lineart colorization. <em>Image and vision
computing</em> (Cham, 2023), 1–17.</div>
</div>
<div id="ref-he_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[25] </div><div
class="csl-right-inline">He, K., Zhang, X., Ren, S. and Sun, J. 2016.
Deep residual learning for image recognition. <em>Proceedings of the
IEEE conference on computer vision and pattern recognition</em> (2016),
770–778.</div>
</div>
<div id="ref-hensman_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[26] </div><div
class="csl-right-inline">Hensman, P. and Aizawa, K. 2017. <a
href="https://doi.org/10.1109/ICDAR.2017.295">cGAN-based manga
colorization using a single training image</a>. <em>2017 14th IAPR
international conference on document analysis and recognition
(ICDAR)</em> (Los Alamitos, CA, USA, Nov. 2017), 72–77.</div>
</div>
<div id="ref-hinton_lecture6a" class="csl-entry" role="listitem">
<div class="csl-left-margin">[27] </div><div
class="csl-right-inline">Hinton, G., Srivastava, N. and Swersky, K.
Neural networks for machine learning: Overview of mini-batch gradient
descent.</div>
</div>
<div id="ref-ho_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[28] </div><div
class="csl-right-inline">Ho, J., Jain, A. and Abbeel, P. 2020. Denoising
diffusion probabilistic models. <em>Advances in Neural Information
Processing Systems</em>. 33, (2020), 6840–6851.</div>
</div>
<div id="ref-hochreiter_1997" class="csl-entry" role="listitem">
<div class="csl-left-margin">[29] </div><div
class="csl-right-inline">Hochreiter, S. and Schmidhuber, J. 1997.
<span>Long Short-Term Memory</span>. <em>Neural Computation</em>. 9, 8
(Nov. 1997), 1735–1780. DOI:https://doi.org/<a
href="https://doi.org/10.1162/neco.1997.9.8.1735">10.1162/neco.1997.9.8.1735</a>.</div>
</div>
<div id="ref-hornik_1989" class="csl-entry" role="listitem">
<div class="csl-left-margin">[30] </div><div
class="csl-right-inline">Hornik, K., Stinchcombe, M. and White, H. 1989.
Multilayer feedforward networks are universal approximators. <em>Neural
Networks</em>. 2, 5 (1989), 359–366. DOI:https://doi.org/<a
href="https://doi.org/10.1016/0893-6080(89)90020-8">10.1016/0893-6080(89)90020-8</a>.</div>
</div>
<div id="ref-ioffe_2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[31] </div><div
class="csl-right-inline">Ioffe, S. and Szegedy, C. 2015. <a
href="https://proceedings.mlr.press/v37/ioffe15.html">Batch
normalization: Accelerating deep network training by reducing internal
covariate shift</a>. <em>Proceedings of the 32nd international
conference on machine learning</em> (Lille, France, 2015),
448–456.</div>
</div>
<div id="ref-jackson_1998" class="csl-entry" role="listitem">
<div class="csl-left-margin">[32] </div><div
class="csl-right-inline">Jackson, P. 1998. <em>Introduction to expert
systems</em>. Addison-Wesley Longman Publishing Co., Inc.</div>
</div>
<div id="ref-jumper_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[33] </div><div
class="csl-right-inline">Jumper, J. et al. 2021. Highly accurate protein
structure prediction with AlphaFold. <em>Nature</em>. 596, 7873 (Aug.
2021), 583–589. DOI:https://doi.org/<a
href="https://doi.org/10.1038/s41586-021-03819-2">10.1038/s41586-021-03819-2</a>.</div>
</div>
<div id="ref-kandinsky_1977" class="csl-entry" role="listitem">
<div class="csl-left-margin">[34] </div><div
class="csl-right-inline">Kandinsky, W. and Sadleir, M. 1977.
<em>Concerning the spiritual in art</em>. Dover Publications.</div>
</div>
<div id="ref-karpathy_micrograd" class="csl-entry" role="listitem">
<div class="csl-left-margin">[35] </div><div
class="csl-right-inline">Karpathy, A. 2020. <a
href="https://github.com/karpathy/micrograd"><span>Micrograd</span></a>.</div>
</div>
<div id="ref-kim_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[36] </div><div
class="csl-right-inline">Kim, H., Jhoo, H.Y., Park, E. and Yoo, S. 2019.
<a href="https://doi.org/10.1109/ICCV.2019.00915">Tag2Pix: Line art
colorization using text tag with SECat and changing loss</a>. <em>2019
IEEE/CVF international conference on computer vision (ICCV)</em> (2019),
9055–9064.</div>
</div>
<div id="ref-kingma_2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[37] </div><div
class="csl-right-inline">Kingma, D.P. and Ba, J. 2014. Adam: A method
for stochastic optimization. <em>arXiv preprint arXiv:1412.6980</em>.
(2014).</div>
</div>
<div id="ref-kingma_2013" class="csl-entry" role="listitem">
<div class="csl-left-margin">[38] </div><div
class="csl-right-inline">Kingma, D.P. and Welling, M. 2013.
Auto-encoding variational bayes. <em>arXiv preprint
arXiv:1312.6114</em>. (2013).</div>
</div>
<div id="ref-krizhevsky_2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">[39] </div><div
class="csl-right-inline">Krizhevsky, A., Sutskever, I. and Hinton, G.E.
2012. <a
href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">ImageNet
classification with deep convolutional neural networks</a>. <em>Advances
in neural information processing systems</em> (2012).</div>
</div>
<div id="ref-larid_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[40] </div><div
class="csl-right-inline">Laird, J.E. 2019. <em>The soar cognitive
architecture</em>. The MIT Press.</div>
</div>
<div id="ref-lecun_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[41] </div><div class="csl-right-inline">Le
Cun, Y. 2019. <em><a
href="https://www.odilejacob.fr/catalogue/sciences-humaines/questions-de-societe/quand-la-machine-apprend_9782738149312.php">Quand
la machine apprend: La r<span>é</span>volution des neurones artificiels
et de l’apprentissage profond</a></em>. Odile Jacob.</div>
</div>
<div id="ref-lecun_1989" class="csl-entry" role="listitem">
<div class="csl-left-margin">[42] </div><div
class="csl-right-inline">LeCun, Y., Boser, B., Denker, J.S., Henderson,
D., Howard, R.E., Hubbard, W. and Jackel, L.D. 1989. <span
class="nocase">Backpropagation Applied to Handwritten Zip Code
Recognition</span>. <em>Neural Computation</em>. 1, 4 (Dec. 1989),
541–551. DOI:https://doi.org/<a
href="https://doi.org/10.1162/neco.1989.1.4.541">10.1162/neco.1989.1.4.541</a>.</div>
</div>
<div id="ref-lecun_1998" class="csl-entry" role="listitem">
<div class="csl-left-margin">[43] </div><div
class="csl-right-inline">Lecun, Y., Bottou, L., Bengio, Y. and Haffner,
P. 1998. Gradient-based learning applied to document recognition.
<em>Proceedings of the IEEE</em>. 86, 11 (1998), 2278–2324.
DOI:https://doi.org/<a
href="https://doi.org/10.1109/5.726791">10.1109/5.726791</a>.</div>
</div>
<div id="ref-lieto_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[44] </div><div
class="csl-right-inline">Lieto, A. 2021. <em><a
href="https://doi.org/10.4324/9781315460536">Cognitive design for
artificial minds (1st ed.)</a></em>. Routledge.</div>
</div>
<div id="ref-lim_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[45] </div><div
class="csl-right-inline">Lim, J.H. and Ye, J.C. 2017. Geometric gan.
<em>arXiv preprint arXiv:1705.02894</em>. (2017).</div>
</div>
<div id="ref-liu_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[46] </div><div
class="csl-right-inline">Liu, Y., Qin, Z., Wan, T. and Luo, Z. 2018.
Auto-painter: Cartoon image generation from sketch by using conditional
wasserstein generative adversarial networks. <em>Neurocomputing</em>.
311, (2018), 78–87. DOI:https://doi.org/<a
href="https://doi.org/10.1016/j.neucom.2018.05.045">10.1016/j.neucom.2018.05.045</a>.</div>
</div>
<div id="ref-loshchilov_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[47] </div><div
class="csl-right-inline">Loshchilov, I. and Hutter, F. 2017. Decoupled
weight decay regularization. <em>arXiv preprint arXiv:1711.05101</em>.
(2017).</div>
</div>
<div id="ref-luong_2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[48] </div><div
class="csl-right-inline">Luong, M.-T., Pham, H. and Manning, C.D. 2015.
Effective approaches to attention-based neural machine translation.
<em>arXiv preprint arXiv:1508.04025</em>. (2015).</div>
</div>
<div id="ref-mccarthy_1978" class="csl-entry" role="listitem">
<div class="csl-left-margin">[49] </div><div
class="csl-right-inline">McCarthy, J. 1978. <a
href="https://doi.org/10.1145/800025.1198360">History of LISP</a>.
<em>History of programming languages</em>. Association for Computing
Machinery. 173–185.</div>
</div>
<div id="ref-dartmouth_2006" class="csl-entry" role="listitem">
<div class="csl-left-margin">[50] </div><div
class="csl-right-inline">McCarthy, J., Minsky, M.L., Rochester, N. and
Shannon, C.E. 2006. A proposal for the dartmouth summer research project
on artificial intelligence, august 31, 1955. <em>AI Magazine</em>. 27, 4
(2006), 12. DOI:https://doi.org/<a
href="https://doi.org/10.1609/aimag.v27i4.1904">10.1609/aimag.v27i4.1904</a>.</div>
</div>
<div id="ref-minsky_1969" class="csl-entry" role="listitem">
<div class="csl-left-margin">[51] </div><div
class="csl-right-inline">Minsky, M. and Papert, S. 1969.
<em>Perceptrons: An introduction to computational geometry</em>. MIT
Press.</div>
</div>
<div id="ref-miyato_2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[52] </div><div
class="csl-right-inline">Miyato, T., Kataoka, T., Koyama, M. and
Yoshida, Y. 2018. Spectral normalization for generative adversarial
networks. <em>arXiv preprint arXiv:1802.05957</em>. (2018).</div>
</div>
<div id="ref-mumford_2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">[53] </div><div
class="csl-right-inline">Mumford, M., Medeiros, K. and Partlow, P. 2012.
Creative thinking: Processes, strategies, and knowledge. <em>The Journal
of Creative Behavior</em>. 46, (Mar. 2012). DOI:https://doi.org/<a
href="https://doi.org/10.1002/jocb.003">10.1002/jocb.003</a>.</div>
</div>
<div id="ref-newell_1959" class="csl-entry" role="listitem">
<div class="csl-left-margin">[54] </div><div
class="csl-right-inline">Newell, A., Shaw, J.C. and Simon, H.A. 1959.
<em><a href="https://doi.org/10.1037/13117-003">The processes of
creative thinking</a></em>. RAND Corporation.</div>
</div>
<div id="ref-nichol_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[55] </div><div
class="csl-right-inline">Nichol, A.Q. and Dhariwal, P. 2021. Improved
denoising diffusion probabilistic models. <em>International conference
on machine learning</em> (2021), 8162–8171.</div>
</div>
<div id="ref-paintman" class="csl-entry" role="listitem">
<div class="csl-left-margin">[56] </div><div
class="csl-right-inline">Paintman: <a
href="http://www.retasstudio.net/products/paintman/"><em>http://www.retasstudio.net/products/paintman/</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-pytorch" class="csl-entry" role="listitem">
<div class="csl-left-margin">[57] </div><div
class="csl-right-inline">Paszke, A. et al. 2019. PyTorch: An imperative
style, high-performance deep learning library. <em>Proceedings of the
33rd international conference on neural information processing
systems</em>. Curran Associates Inc.</div>
</div>
<div id="ref-paintschainer_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[58] </div><div
class="csl-right-inline">Pelica paint: 2017. <a
href="https://petalica-paint.pixiv.dev/index_en.html"><em>https://petalica-paint.pixiv.dev/index_en.html</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-photoshop" class="csl-entry" role="listitem">
<div class="csl-left-margin">[59] </div><div
class="csl-right-inline">Photoshop: <a
href="https://www.adobe.com/products/photoshop.html"><em>https://www.adobe.com/products/photoshop.html</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-qian_1999" class="csl-entry" role="listitem">
<div class="csl-left-margin">[60] </div><div
class="csl-right-inline">Qian, N. 1999. On the momentum term in gradient
descent learning algorithms. <em>Neural Networks</em>. 12, 1 (1999),
145–151. DOI:https://doi.org/<a
href="https://doi.org/10.1016/S0893-6080(98)00116-6">10.1016/S0893-6080(98)00116-6</a>.</div>
</div>
<div id="ref-rombach_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[61] </div><div
class="csl-right-inline">Rombach, R., Blattmann, A., Lorenz, D., Esser,
P. and Ommer, B. 2021. <a
href="https://arxiv.org/abs/2112.10752">High-resolution image synthesis
with latent diffusion models</a>.</div>
</div>
<div id="ref-ronneberger_2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[62] </div><div
class="csl-right-inline">Ronneberger, O., Fischer, P. and Brox, T. 2015.
U-net: Convolutional networks for biomedical image segmentation.
<em>Medical image computing and computer-assisted intervention–MICCAI
2015: 18th international conference, munich, germany, october 5-9, 2015,
proceedings, part III 18</em> (2015), 234–241.</div>
</div>
<div id="ref-rosenblatt_1958" class="csl-entry" role="listitem">
<div class="csl-left-margin">[63] </div><div
class="csl-right-inline">Rosenblatt, F. 1958. <span class="nocase">The
perceptron: A probabilistic model for information storage and
organization in the brain.</span> <em>Psychological Review</em>. 65, 6
(1958), 386–408. DOI:https://doi.org/<a
href="https://doi.org/10.1037/h0042519">10.1037/h0042519</a>.</div>
</div>
<div id="ref-rumelhart_1986" class="csl-entry" role="listitem">
<div class="csl-left-margin">[64] </div><div
class="csl-right-inline">Rumelhart, D.E., Hinton, G.E. and Williams,
R.J. 1986. Learning representations by back-propagating errors.
<em>Nature</em>. 323, 6088 (Oct. 1986), 533–536. DOI:https://doi.org/<a
href="https://doi.org/10.1038/323533a0">10.1038/323533a0</a>.</div>
</div>
<div id="ref-russell_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[65] </div><div
class="csl-right-inline">Russell, S.J. and Norvig, P. 2009.
<em>Artificial intelligence: A modern approach</em>. Pearson.</div>
</div>
<div id="ref-saito_2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[66] </div><div
class="csl-right-inline">Saito, M. and Matsui, Y. 2015. <a
href="https://doi.org/10.1145/2820903.2820907">Illustration2Vec: A
semantic vector representation of illustrations</a>. <em>SIGGRAPH asia
2015 technical briefs</em> (New York, NY, USA, 2015), 5:1–5:4.</div>
</div>
<div id="ref-sangkloy_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[67] </div><div
class="csl-right-inline">Sangkloy, P., Lu, J., Fang, C., Yu, F. and
Hays, J. 2017. <a
href="https://doi.org/10.1109/CVPR.2017.723">Scribbler: Controlling deep
image synthesis with sketch and color</a>. <em>2017 <span>IEEE</span>
conference on computer vision and pattern recognition, <span>CVPR</span>
2017, honolulu, HI, USA, july 21-26, 2017</em> (2017), 6836–6845.</div>
</div>
<div id="ref-senior_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[68] </div><div
class="csl-right-inline">Senior, A.W. et al. 2020. Improved protein
structure prediction using potentials from deep learning.
<em>Nature</em>. 577, 7792 (Jan. 2020), 706–710. DOI:https://doi.org/<a
href="https://doi.org/10.1038/s41586-019-1923-7">10.1038/s41586-019-1923-7</a>.</div>
</div>
<div id="ref-silver_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[69] </div><div
class="csl-right-inline">Silver, D. et al. 2016. Mastering the game of
go with deep neural networks and tree search. <em>Nature</em>. 529, 7587
(Jan. 2016), 484–489. DOI:https://doi.org/<a
href="https://doi.org/10.1038/nature16961">10.1038/nature16961</a>.</div>
</div>
<div id="ref-simonyan_2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[70] </div><div
class="csl-right-inline">Simonyan, K. and Zisserman, A. 2014. Very deep
convolutional networks for large-scale image recognition. <em>arXiv
preprint arXiv:1409.1556</em>. (2014).</div>
</div>
<div id="ref-mnist" class="csl-entry" role="listitem">
<div class="csl-left-margin">[71] </div><div
class="csl-right-inline">The MNIST database: <a
href="http://yann.lecun.com/exdb/mnist/"><em>http://yann.lecun.com/exdb/mnist/</em></a>.
Accessed: 2023-02-07.</div>
</div>
<div id="ref-vaswani_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[72] </div><div
class="csl-right-inline">Vaswani, A., Shazeer, N., Parmar, N.,
Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I.
2017. Attention is all you need. <em>Advances in neural information
processing systems</em>. 30, (2017).</div>
</div>
<div id="ref-vinyals_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[73] </div><div
class="csl-right-inline">Vinyals, O. et al. 2019. Grandmaster level in
StarCraft II using multi-agent reinforcement learning. <em>Nature</em>.
575, 7782 (Nov. 2019), 350–354. DOI:https://doi.org/<a
href="https://doi.org/10.1038/s41586-019-1724-z">10.1038/s41586-019-1724-z</a>.</div>
</div>
<div id="ref-wolf_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[74] </div><div
class="csl-right-inline">Wolf, M.J., Miller, K. and Grodzinsky, F.S.
2017. Why we should have seen that coming: Comments on microsoft’s tay
"experiment," and wider implications. <em>SIGCAS Comput. Soc.</em> 47, 3
(Sep. 2017), 54–64. DOI:https://doi.org/<a
href="https://doi.org/10.1145/3144592.3144598">10.1145/3144592.3144598</a>.</div>
</div>
<div id="ref-aston_zhang_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[75] </div><div
class="csl-right-inline">Zhang, A., Lipton, Z.C., Li, M. and Smola, A.J.
2021. Dive into deep learning. <em>arXiv preprint arXiv:2106.11342</em>.
(2021).</div>
</div>
<div id="ref-zhang_ji_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[76] </div><div
class="csl-right-inline">Zhang, L., Ji, Y., Lin, X. and Liu, C. 2017. <a
href="https://doi.org/10.1109/ACPR.2017.61">Style transfer for anime
sketches with enhanced residual u-net and auxiliary classifier GAN</a>.
<em>2017 4th IAPR asian conference on pattern recognition (ACPR)</em>
(2017), 506–511.</div>
</div>
<div id="ref-zhang_richard_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[77] </div><div
class="csl-right-inline">Zhang, R., Zhu, J.-Y., Isola, P., Geng, X.,
Lin, A.S., Yu, T. and Efros, A.A. 2017. Real-time user-guided image
colorization with learned deep priors. <em>ACM Trans. Graph.</em> 36, 4
(Jul. 2017). DOI:https://doi.org/<a
href="https://doi.org/10.1145/3072959.3073703">10.1145/3072959.3073703</a>.</div>
</div>
</div>
</body>
</html>
