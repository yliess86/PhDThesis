<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Yliess Hati" />
  <meta name="keywords" content="keyword" />
  <title>AI-Assisted Creative Expression: a Case for Automatic Lineart Colorization</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title"><p>AI-Assisted Creative Expression: a Case for
Automatic Lineart Colorization</p></h1>
<p class="author">Yliess Hati</p>
</header>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Contents</h2>
<ul>
<li><a href="#list-of-abbreviations" id="toc-list-of-abbreviations">List
of Abbreviations</a></li>
<li><a href="#acronym-list" id="toc-acronym-list">Acronyms</a></li>
<li><a href="#abstract" id="toc-abstract">Abstract</a></li>
<li><a href="#aknowledgements"
id="toc-aknowledgements">Aknowledgements</a></li>
<li><a href="#sec:I" id="toc-sec:I">Introduction</a>
<ul>
<li><a href="#sec:I.1" id="toc-sec:I.1">Motivations</a></li>
<li><a href="#sec:I.2" id="toc-sec:I.2">Problem Statement</a></li>
<li><a href="#sec:I.3" id="toc-sec:I.3">Contributions</a></li>
<li><a href="#sec:I.4" id="toc-sec:I.4">Concerns</a></li>
<li><a href="#sec:I.5" id="toc-sec:I.5">Outline</a></li>
</ul></li>
<li><a href="#sec:II" id="toc-sec:II">Theoretical Background</a>
<ul>
<li><a href="#sec:II.1" id="toc-sec:II.1">Machine Learning</a>
<ul>
<li><a href="#sec:II.1.1" id="toc-sec:II.1.1">Supervised
Learning</a></li>
<li><a href="#sec:II.1.2" id="toc-sec:II.1.2">Optimization</a></li>
<li><a href="#sec:II.1.3" id="toc-sec:II.1.3">Backpropagation</a></li>
</ul></li>
<li><a href="#sec:II.2" id="toc-sec:II.2">Artificial Neural Networks</a>
<ul>
<li><a href="#sec:II.2.1" id="toc-sec:II.2.1">Perceptron</a></li>
<li><a href="#sec:II.2.2" id="toc-sec:II.2.2">Multi-Layer
Perceptron</a></li>
<li><a href="#sec:II.2.3" id="toc-sec:II.2.3">Convolutional Neural
Network</a></li>
<li><a href="#sec:II.2.4" id="toc-sec:II.2.4">Transformers</a></li>
</ul></li>
<li><a href="#sec:II.3" id="toc-sec:II.3">Summary</a></li>
</ul></li>
<li><a href="#sec:III" id="toc-sec:III">Related Work</a>
<ul>
<li><a href="#sec:III.1" id="toc-sec:III.1">Classical Approaches to
Automatic Lineart Colorization</a>
<ul>
<li><a href="#sec:III.1.1"
id="toc-sec:III.1.1">Delaunay-Triangulation</a></li>
<li><a href="#sec:III.1.2" id="toc-sec:III.1.2">Assistive
Completion</a></li>
</ul></li>
<li><a href="#sec:III.2" id="toc-sec:III.2">Generative Neural
Networks</a>
<ul>
<li><a href="#sec:III.2.1" id="toc-sec:III.2.1">Autoencoders</a></li>
<li><a href="#sec:III.2.2" id="toc-sec:III.2.2">Variational
Autoencoders</a></li>
<li><a href="#sec:III.2.3" id="toc-sec:III.2.3">Generative Adversarial
Networks</a></li>
<li><a href="#sec:III.2.4" id="toc-sec:III.2.4">Denoising Diffusion
Models</a></li>
<li><a href="#sec:III.2.5" id="toc-sec:III.2.5">Conditional
Generation</a></li>
</ul></li>
<li><a href="#sec:III.3" id="toc-sec:III.3">Deep Learning Approaches to
Automatic Lineart Colorization</a>
<ul>
<li><a href="#sec:III.3.1" id="toc-sec:III.3.1">Automatic
Colorization</a></li>
<li><a href="#sec:III.3.2" id="toc-sec:III.3.2">Example-Based</a></li>
<li><a href="#sec:III.3.3" id="toc-sec:III.3.3">Tags</a></li>
<li><a href="#sec:III.3.4" id="toc-sec:III.3.4">Natural Language
Prompt</a></li>
<li><a href="#sec:III.3.5" id="toc-sec:III.3.5">Color Hints</a></li>
</ul></li>
<li><a href="#sec:III.4" id="toc-sec:III.4">Summary</a></li>
</ul></li>
<li><a href="#sec:IV" id="toc-sec:IV">Contributions in Automatic Lineart
Colorization</a>
<ul>
<li><a href="#sec:IV.1" id="toc-sec:IV.1">Method and Implementation</a>
<ul>
<li><a href="#sec:IV.1.1" id="toc-sec:IV.1.1">Synthetic Dataset
Pipeline</a></li>
<li><a href="#sec:IV.1.4" id="toc-sec:IV.1.4">Evaluation
Metrics</a></li>
<li><a href="#sec:IV.1.5" id="toc-sec:IV.1.5">Reproducibility</a></li>
</ul></li>
<li><a href="#sec:IV.2" id="toc-sec:IV.2">PaintsTorch: User-Guided
Lineart Colorization</a>
<ul>
<li><a href="#sec:IV.2.1" id="toc-sec:IV.2.1">Introduction</a></li>
<li><a href="#sec:IV.2.2" id="toc-sec:IV.2.2">Method</a></li>
<li><a href="#sec:IV.2.3" id="toc-sec:IV.2.3">Results</a></li>
<li><a href="#sec:IV.2.4" id="toc-sec:IV.2.4">Summary</a></li>
</ul></li>
<li><a href="#sec:IV.3" id="toc-sec:IV.3">StencitTorch: Human-Machine
Colaboration Summary</a>
<ul>
<li><a href="#sec:IV.3.1" id="toc-sec:IV.3.1">Introduction</a></li>
<li><a href="#sec:IV.3.2" id="toc-sec:IV.3.2">Method</a></li>
<li><a href="#sec:IV.3.3" id="toc-sec:IV.3.3">Results</a></li>
<li><a href="#sec:IV.3.4" id="toc-sec:IV.3.4">Summary</a></li>
</ul></li>
<li><a href="#sec:IV.4" id="toc-sec:IV.4">StablePaint: Conditional
Denoising Diffusion</a>
<ul>
<li><a href="#sec:IV.4.1" id="toc-sec:IV.4.1">Introduction</a></li>
<li><a href="#sec:IV.4.2" id="toc-sec:IV.4.2">Method</a></li>
<li><a href="#sec:IV.4.3" id="toc-sec:IV.4.3">Intermediate
Results</a></li>
<li><a href="#sec:IV.4.4" id="toc-sec:IV.4.4">Summary</a></li>
</ul></li>
</ul></li>
<li><a href="#sec:V" id="toc-sec:V">Conclusion</a>
<ul>
<li><a href="#sec:ethical-and-societal-impact"
id="toc-sec:ethical-and-societal-impact">Ethical and Societal Impact</a>
<ul>
<li><a href="#benefits" id="toc-benefits">Benefits</a></li>
<li><a href="#ethical-concerns" id="toc-ethical-concerns">Ethical
Concerns</a></li>
<li><a href="#societal-impact" id="toc-societal-impact">Societal
Impact</a></li>
<li><a href="#limitations" id="toc-limitations">Limitations</a></li>
<li><a href="#regulations-and-guidelines"
id="toc-regulations-and-guidelines">Regulations and Guidelines</a></li>
<li><a href="#summary" id="toc-summary">Summary</a></li>
</ul></li>
<li><a href="#sec:conclusion"
id="toc-sec:conclusion">Conclusion</a></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul></li>
</ul>
</nav>
<h2 class="unnumbered" id="list-of-abbreviations">List of
Abbreviations</h2>
<h1 id="acronym-list">Acronyms</h1>
<ul>
<li><strong>AD</strong>: Automatic Differentiation</li>
<li><strong>AE</strong>: Autoencoder</li>
<li><strong>AI</strong>: Artificial Intelligence</li>
<li><strong>ANN</strong>: Artificial Neural Network</li>
<li><strong>AST</strong>: Abstract Syntax Tree</li>
<li><strong>CNN</strong>: Convolutional Neural Network</li>
<li><strong>CS</strong>: Computer Science</li>
<li><strong>CV</strong>: Computer Vision</li>
<li><strong>DAG</strong>: Directed Acyclic Graph</li>
<li><strong>DDM</strong>: Denoising Diffusion Model</li>
<li><strong>DDPM</strong>: Denoising Diffusion Probabilistic Model</li>
<li><strong>DL</strong>: Deep Learning</li>
<li><strong>DoG</strong>: Difference of Gaussians</li>
<li><strong>FID</strong>: Fréchet Inception Distance</li>
<li><strong>GAN</strong>: Generative Adversarial Network</li>
<li><strong>GD</strong>: Gradient Descent</li>
<li><strong>GPU</strong>: Graphical Processing Unit</li>
<li><strong>KL-Divergence</strong>: Kullback-Leibler Divergence</li>
<li><strong>LLM</strong>: Large Language Model</li>
<li><strong>LPIPS</strong>: Learned Perceptual Image Patch
Similarity</li>
<li><strong>ML</strong>: Machine Learning</li>
<li><strong>MLP</strong>: Multi-Layer Perceptron</li>
<li><strong>MNIST</strong>: Modified National Institute of Standards and
Technology</li>
<li><strong>MOS</strong>: Mean Opinion Score</li>
<li><strong>MSE</strong>: Mean Squared Error</li>
<li><strong>NLP</strong>: Natural Language Processing</li>
<li><strong>NN</strong>: Neural Network</li>
<li><strong>NPU</strong>: Neural Processing Unit</li>
<li><strong>PSNR</strong>: Peak Signal-to-Noise Ratio</li>
<li><strong>ReLU</strong>: Rectified Linear Unit</li>
<li><strong>SGD</strong>: Stochastic Gradient Descent</li>
<li><strong>TPU</strong>: Tensor Processing Unit</li>
<li><strong>VAE</strong>: Variational Autoencoder</li>
<li><strong>VI</strong>: Variational Inference</li>
<li><strong>ViT</strong>: Vision Transformer</li>
<li><strong>WGAN</strong>: Wasserstein Generative Adversarial
Network</li>
<li><strong>WGAN-GP</strong>: Wasserstein Generative Adversarial Network
with Gradient Penalty</li>
<li><strong>cGAN</strong>: Conditional Generative Adversarial
Network</li>
<li><strong>cWGAN</strong>: Conditional Wasserstein Generative
Adversarial Network</li>
<li><strong>cWGAN-GP</strong>: Conditional Wasserstein Generative
Adversarial Network with Gradient Penalty</li>
<li><strong>xDoG</strong>: extended Difference of Gaussians</li>
</ul>
<h2 class="unnumbered" id="abstract">Abstract</h2>

<h2 class="unnumbered" id="aknowledgements">Aknowledgements</h2>
<!-- ===================== [START] PART INTRODUCTION ===================== -->
<h1 id="sec:I">Introduction</h1>
<p>Humans can perceive and understand the world. We can accomplish a
wide range of complex tasks through the combination of visual
recognition, scene understanding, and communication. The ability to
quickly and accurately extract information from a single image is a
testament to the complexity and sophistication of the human brain and is
often taken for granted. One objective of the Artificial Intelligence
(AI) field’s is to empower machines with such human-like abilities, one
of them being creativity, being able to produce something original and
worthwhile <span class="citation" data-cites="mumford_2012">[<a
href="#ref-mumford_2012" role="doc-biblioref">54</a>]</span>.</p>
<p>Computational creativity is a multidisciplinary field – AI, cognitive
psychology, philosophy, and art – that seeks to understand, simulate,
replicate, or enhance human creativity. One definition of creativity
<span class="citation" data-cites="newell_1959">[<a
href="#ref-newell_1959" role="doc-biblioref">55</a>]</span> is the
ability to produce something that is novel and useful, demands that we
reject common beliefs, results from intense motivation and persistence,
or comes from clarifying a vague problem. Top-down approaches to this
definition use a mix of explicit formulations of recipes and randomness
such as procedural generation. On the opposite, bottom-up approaches use
Artificial Neural Networks (ANN) to learn patterns and heuristics from
large datasets to enable non-linear generation.</p>
<p>We are currently witnessing the beginning of a new era where the gap
between machines and humans is starting to blur. Recent breakthroughs in
the field of AI, more specifically in Deep Learning (DL), are giving
computers the ability to perceive and understand our world, but also to
interact with our environment using natural interactions such as speech
and natural language. ANNs, once mocked by the AI community <span
class="citation" data-cites="lecun_2019">[<a href="#ref-lecun_2019"
role="doc-biblioref">44</a>]</span>, are now trainable using Gradient
Descent (GD) <span class="citation" data-cites="rumelhart_1986">[<a
href="#ref-rumelhart_1986" role="doc-biblioref">69</a>]</span> thanks to
the massive availability of data and the processing power of modern
hardware accelerators such as Graphical Processing Units (GPU), Tensor
Processing Units (TPU), and Neural Processing Units (NPU).</p>
<p>Neural Networks (NN), those trainable general function approximators,
gave rise to the field of generative NNs. Specialized DL architectures
such as Variational Autoencoders (VAE) <span class="citation"
data-cites="kingma_2013">[<a href="#ref-kingma_2013"
role="doc-biblioref">43</a>]</span>, Generative Adversarial Networks
(GAN) <span class="citation" data-cites="goodfellow_2014">[<a
href="#ref-goodfellow_2014" role="doc-biblioref">26</a>]</span>,
Denoising Diffusion Models (DDM) <span class="citation"
data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">35</a>]</span>, and Large Language Models (LLM)
<span class="citation" data-cites="vaswani_2017 brown_2020">[<a
href="#ref-brown_2020" role="doc-biblioref">8</a>, <a
href="#ref-vaswani_2017" role="doc-biblioref">81</a>]</span> are used to
generate creative artifacts such as text, audio, images, and videos of
unprecedented quality and complexity.</p>
<p>This dissertation aims at exploring how one could train and use
generative NN to create AI-powered tools capable of enhancing human
creative expression. The task of automatic lineart colorization is
chosen to to illustrate this process throughout the entire thesis
dissertation.</p>
<figure id="fig:steps">
<img src="./figures/motivations_steps.svg"
alt="Standard illustration workflow from left to right: sketching, inking, coloring, and pros-processing. Credits: Taira Akitsu" />
<figcaption>Figure 1: Standard illustration workflow from left to right:
sketching, inking, coloring, and pros-processing. Credits: Taira
Akitsu</figcaption>
</figure>
<h2 id="sec:I.1">Motivations</h2>
<p>Lineart colorization is an essential aspect of the work of artists,
illustrators, and animators. The task of manually coloring lineart can
be time-consuming, repetitive, and exhausting, particularly in the
animation industry, where every frame of an animated product must be
colored and shaded. This process is typically done using image editing
software such as Photoshop <span class="citation"
data-cites="photoshop">[<a href="#ref-photoshop"
role="doc-biblioref">63</a>]</span>, Clip Studio PAINT <span
class="citation" data-cites="clipstudiopaint">[<a
href="#ref-clipstudiopaint" role="doc-biblioref">15</a>]</span>, and
PaintMan <span class="citation" data-cites="paintman">[<a
href="#ref-paintman" role="doc-biblioref">57</a>]</span>. Automating the
colorization process can greatly improve the workflow of these creative
professionals and has the potential to lower the barrier for newcomers
and amateurs. Such a system was integrated into Clip Studio PAINT <span
class="citation" data-cites="clipstudiopaint">[<a
href="#ref-clipstudiopaint" role="doc-biblioref">15</a>]</span>,
demonstrating the growing significance of automatic colorization in the
field.</p>
<p>The most common digital illustration process can be broken down into
four distinct stages: sketching, inking, coloring, and post-processing
(see <a href="#fig:steps">Fig 1</a>). As demonstrated by the work of
Kandinsky <span class="citation" data-cites="kandinsky_1977">[<a
href="#ref-kandinsky_1977" role="doc-biblioref">39</a>]</span>, the
colorization process can greatly impact the overall meaning of a piece
of art through the introduction of various color schemes, shading, and
textures. These elements of the coloring process present significant
challenges for the Computer Vision (CV) task of automatic lineart
colorization, particularly in comparison to its grayscale counterpart
<span class="citation"
data-cites="furusawa_2O17 hensman_2017 zhang_richard_2017">[<a
href="#ref-furusawa_2O17" role="doc-biblioref">23</a>, <a
href="#ref-hensman_2017" role="doc-biblioref">32</a>, <a
href="#ref-zhang_richard_2017" role="doc-biblioref">89</a>]</span>.
Without the added semantic information provided by textures and shadows,
inferring materials and 3D shapes from black and white linearts is
difficult.</p>
<h2 id="sec:I.2">Problem Statement</h2>
<p>One major challenge of automatic lineart colorization is the
availability of qualitative public datasets. Illustrations do not always
come with their corresponding lineart. The few datasets available for
the task are lacking consistency in the quality of the illustrations,
gathering images from different types, mediums and styles. For those
reasons, online scrapping and synthetic lineart extraction is the method
of choice for many of the contributions in the field <span
class="citation" data-cites="ci_2018 zhang_richard_2017">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>, <a
href="#ref-zhang_richard_2017" role="doc-biblioref">89</a>]</span>.</p>
<p>Previous works in automatic lineart colorization are based on the GAN
<span class="citation" data-cites="goodfellow_2014">[<a
href="#ref-goodfellow_2014" role="doc-biblioref">26</a>]</span>
architecture. They can generate unperfect but high-quality illustrations
in a quasi realtime setting. They achieve user control and guidance via
different means, color hints <span class="citation"
data-cites="frans_2017 liu_2017 sangkloy_2016 paintschainer_2018 ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>, <a
href="#ref-frans_2017" role="doc-biblioref">21</a>, <a
href="#ref-liu_2017" role="doc-biblioref">48</a>, <a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>, <a
href="#ref-sangkloy_2016" role="doc-biblioref">71</a>]</span>, style
transfer <span class="citation" data-cites="zhang_ji_2017">[<a
href="#ref-zhang_ji_2017" role="doc-biblioref">86</a>]</span>, tagging
<span class="citation" data-cites="kim_2019">[<a href="#ref-kim_2019"
role="doc-biblioref">41</a>]</span>, and more recently natural language
<span class="citation" data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">35</a>]</span>. One common pattern in these methods
is the use of a feature extractor such as Illustration2Vec <span
class="citation" data-cites="saito_2015">[<a href="#ref-saito_2015"
role="doc-biblioref">70</a>]</span> allowing to compensate for the lack
of semantic descriptors by injecting its feature vector into the
models.</p>
<h2 id="sec:I.3">Contributions</h2>
<p>This work focuses on the use of color hints in the form of user
strokes as it fits the natural digital artist workflow and does not
involve learning and mastering a new skill. While previous works offers
improving quality compared to classical CV techniques, they are still
subject to noisy training data, artifacts, a lack of variety, and a lack
of fidelity in the user intent. In this dissertation we explore the
importance of a clean, qualitative and consistent dataset. We
investigate how to better capture the user intent via natural artistic
controls and how to reflect them into the generated model artifact while
preserving or improving its quality. We also look at how the creative
process can be transformed into a dynamic iterative workflow where the
user collaborates with the machine to refine and carry out variations of
his artwork.</p>
<p>Here is a brief enumeration of this thesis’s contributions:</p>
<ul>
<li>A recipe for curating datasets for the task of automatic lineart
colorization <span class="citation" data-cites="hati_2019 hati_2023">[<a
href="#ref-hati_2019" role="doc-biblioref">29</a>, <a
href="#ref-hati_2023" role="doc-biblioref">30</a>]</span></li>
<li>Three generative models:
<ul>
<li>PaintsTorch <span class="citation" data-cites="hati_2019">[<a
href="#ref-hati_2019" role="doc-biblioref">29</a>]</span>, a double GAN
generator that improved generation quality compared to previous work
while allowing realtime interaction with the user.</li>
<li>StencilTorch <span class="citation" data-cites="hati_2023">[<a
href="#ref-hati_2023" role="doc-biblioref">30</a>]</span>, an upgrade
upon PaintsTorch, shifting the colorization problem to in-painting
allowing for human collaboration to emerge as a natural workflow where
the input of a first pass becomes the potential input for a second.</li>
<li>StablePaint, an exploration of DDM bringing more variety into the
generated outputs allowing for variation exploration while conserving
the iterative workflow introduced by StencilTorch at the cost of
inference speed.</li>
</ul></li>
<li>An advised reflection on current generative AI ethical and societal
impact.</li>
</ul>
<h2 id="sec:I.4">Concerns</h2>
<p>Recent advances in generative AI for text, image, audio, and video
synthesis are raising important ethical and societal concerns,
especially because of its availability and ease of use. Models such as
Stable Diffusion <span class="citation" data-cites="rombach_2021">[<a
href="#ref-rombach_2021" role="doc-biblioref">66</a>]</span> and more
recently Chat-GPT <span class="citation" data-cites="openai_2023">[<a
href="#ref-openai_2023" role="doc-biblioref">11</a>]</span> are
disturbing our common beliefs and relation with copyright, creativity,
the distribution of fake information and so on.</p>
<p>One of the main issues with generative AI is the potential for model
fabulation. Generative models can create entirely new, synthetic data
that is indistinguishable from real data. This can lead to the
dissemination of false information and the manipulation of public
opinion. Additionally, there are ambiguities surrounding the ownership
and copyright of the generated content, as it is unclear who holds the
rights to the generated images and videos. Training data is often
obtained via online scrapping and thus copyright ownership is not
propagated. This is especially true for commercial applications.</p>
<p>Another important concern is the potential for biases and
discrimination. These models are trained on large amounts of data, and
if the data is not diverse or representative enough, the model may
perpetuate or even amplify existing biases. The Microsoft Tay Twitter
bot <span class="citation" data-cites="wolf_2017">[<a
href="#ref-wolf_2017" role="doc-biblioref">83</a>]</span> scandal is an
outcome of such a phenomenon. This initially innocent chatbot has been
easily turned into a racist bot perpetuating hate speech. The task was
made easier because of the inherently biased dataset it was trained
on.</p>
<p>In this work, we are committed to addressing and raising awareness
for these concerns. The illustrations used for training our models and
for our experiments are only used for educational and research purposes.
We only provide recipes for reproducibility and do not distribute the
dataset nor the weights resulting from model training, only the code. We
hope this will not ensure that our work is used ethically and
responsibly but limit its potential misuse.</p>
<h2 id="sec:I.5">Outline</h2>
<p>The first part of this thesis (chapters <a
href="#sec:introduction">1</a>-<a href="#sec:methodology">3</a>)
provides context to the recent advances in generative AI and introduces
the CV task of user-guided automatic lineart colorization, its
challenges, and our contributions to the field. It then provides
additional background, from DL first principles to current architectures
used in modern generative NN, and introduces the methodology used
throughout the entire document. This part should be accessible to the
majority, experts and non-experts, and serve as an introduction to the
field.</p>
<p>The second part (chapters <a href="#sec:contrib-1">4</a>-<a
href="#sec:contrib-4">7</a>) presents our contributions, some of which
have previously been presented in <span class="citation"
data-cites="hati_2019 hati_2023">[<a href="#ref-hati_2019"
role="doc-biblioref">29</a>, <a href="#ref-hati_2023"
role="doc-biblioref">30</a>]</span>. It introduces into detail our
recipe for sourcing and curating consistent and qualitative datasets for
automatic lineart colorization, PaintsTorch <span class="citation"
data-cites="hati_2019">[<a href="#ref-hati_2019"
role="doc-biblioref">29</a>]</span> our first double generator GAN
conditioned on user strokes, StencilTorch <span class="citation"
data-cites="hati_2023">[<a href="#ref-hati_2023"
role="doc-biblioref">30</a>]</span> our in-painting reformulation
introducing the use of masks to allow the emergence of iterative
workflow and collaboration with the machine, and finally StablePaint, an
exploration of the use of DDM models for variations qualitative
exploration.</p>
<p>The third and final part (chapters <a
href="#sec:ethdical-and-societal-impact">7</a>-<a
href="#sec:conclusion">8</a>) offers a detailed reflection on this
thesis’s contributions and more generally about the field of generative
AI ethical and societal impact, identifies the remaining challenges and
discusses future work.</p>
<p>The code base for the experiments and contributions is publicly
available on GitHub at <a
href="https://github.com/yliess86">https://github.com/yliess86</a>.</p>
<!-- ===================== [END] PART INTRODUCTION ===================== -->
<!-- ===================== [START] PART BACKGROUND ===================== -->
<h1 id="sec:II">Theoretical Background</h1>
<p>This chapter introduces the reader to the field of Deep Learning (DL)
from first principles to the current architectures used in modern
generative AI. The first section (sections <a href="#sec:core">1</a>-<a
href="#sec:generative">2</a>) are discussing the first principles of
modern DL from the early Perceptron to more modern frameworks such as
Large Language Models (LLM).</p>
<figure id="fig:timeline">
<img src="./figures/boai_timeline.svg"
alt="A brief timeline of the History of Artificial Intelligence (AI)." />
<figcaption>Figure 2: A brief timeline of the History of Artificial
Intelligence (AI).</figcaption>
</figure>
<h2 id="sec:II.1">Machine Learning</h2>
<p>This section introduces the technical background necessary to
understand this thesis dissertation. It introduces Neural Networks (NN)
from first principles. A more detailed and complete introduction to the
field can be found in “the Deep Learning book” by Ian Goodfellow et al
<span class="citation" data-cites="goodfellow_2016">[<a
href="#ref-goodfellow_2016" role="doc-biblioref">25</a>]</span> or in
“Dive into Deep Learning” by Aston Zhang et al. <span class="citation"
data-cites="aston_zhang_2021">[<a href="#ref-aston_zhang_2021"
role="doc-biblioref">85</a>]</span>.</p>
<h3 id="sec:II.1.1">Supervised Learning</h3>
<p>In Machine Learning (ML), problems are often formulated as
data-driven learning tasks, where a computer is used to find a mapping
<span class="math inline">\(f: X \rightarrow Y\)</span> from input space
<span class="math inline">\(X\)</span> to output space <span
class="math inline">\(Y\)</span>. For example, <span
class="math inline">\(X\)</span> could represent data about an e-mail
and <span class="math inline">\(Y\)</span> the probability of this
e-mail being spam. In practice, manually defining all the
characteristics of a function <span class="math inline">\(f\)</span>
that would satisfy this task is considered unpractical. It would require
one to manually describe all potential rules defining spam. In ML, the
supervised framework offers a practical solution consisting of acquiring
label data pairs, <span class="math inline">\((x, y) \in X \times
Y\)</span> for the current problem (see <a
href="#fig:dataflow">Fig 3</a>). In our case, this would require
gathering a dataset of e-mails and asking humans to label those as spam
or not.</p>
<p><strong>Objective Function</strong>: Let us consider such a training
dataset containing n independent pairs <span
class="math inline">\(\{(x_1, y_1), \dots, (x_n, y_n)\}\)</span> sampled
from the data distribution <span class="math inline">\(D\)</span>, <span
class="math inline">\((x_i, y_i) \sim D\)</span>. In ML, we seek for
learning a mapping <span class="math inline">\(f: X \rightarrow
Y\)</span> by searching the space of the candidates function class <span
class="math inline">\(\mathcal{F}\)</span>. Defining a scalar objective
function <span class="math inline">\(L(\hat{y}, y)\)</span> measuring
the distance from true label <span class="math inline">\(y\)</span> and
our prediction <span class="math inline">\(f(x_i) = \hat{y}_i\)</span>
given <span class="math inline">\(f \in \mathcal{F}\)</span>, the
ultimate objective is to find the function <span
class="math inline">\(f^* \in F\)</span> that best satisfy the following
minimization problem (see <a href="#eq:f_star_objective">Eq 1</a>):</p>
<p><span id="eq:f_star_objective"><span class="math display">\[
f^* = arg \; \underset{f \in \mathcal{F}}{min} \; E_{(x, y) \sim D}
L(\hat{y}, y)
\qquad{(1)}\]</span></span></p>
<p>The function <span class="math inline">\(f^*\)</span> must minimize
the expected loss <span class="math inline">\(L\)</span> over the entire
data distribution <span class="math inline">\(D\)</span>. Once such a
function is learned one can use it to perform inference and map any
element from the input space <span class="math inline">\(X\)</span> to
the output space <span class="math inline">\(Y\)</span>.</p>
<p>However, this minimization problem is intractable as it is impossible
to represent the entire distribution <span
class="math inline">\(D\)</span>. Fortunately, as every pair <span
class="math inline">\((x_i, y_i)\)</span> is independently sampled and
identically distributed, the objective can be approximated by sampling
and minimizing the loss over the training dataset (see <a
href="#eq:f_star_objective_approx">Eq 2</a>):</p>
<p><span id="eq:f_star_objective_approx"><span class="math display">\[
f^* \approx arg \; \underset{f \in \mathcal{F}}{min} \; \frac{1}{n}
\sum_{i=1}^{n} L(\hat{y}_i, y_i)
\qquad{(2)}\]</span></span></p>
<p><strong>Regularization</strong>: While simplifying the problem allows
us to perform loss minimization, this approximation comes at a cost.
This optimization problem can have multiple solutions, a set of
functions <span class="math inline">\(\{f_1, \dots, f_m\} \in F\)</span>
performing well on the given training set, but would behave differently
outside of the training data and outside of the data distribution. Those
functions would not necessarily be able to generalize. To mitigate those
concerns, we can introduce a regularization term <span
class="math inline">\(R\)</span> into the objective function (see <a
href="#eq:f_star_objective_regul">Eq 3</a>), a scalar function that is
independent of the data distribution and represent a preference on
certain function class.</p>
<p><span id="eq:f_star_objective_regul"><span class="math display">\[
f^* \approx arg \; \underset{f \in \mathcal{F}}{min} \; \frac{1}{n}
\sum_{i=1}^{n} L(\hat{y}_i, y_i) + R(f)
\qquad{(3)}\]</span></span></p>
<figure id="fig:dataflow">
<img src="./figures/core_nn_dataflow.svg"
alt="Supervised learning data flow. The dataset {(x_i, y_i)} \in D is used to train the model f \in \mathcal{F} to minimize an objective function with two terms, a data dependant loss L, and a regularization R measuring the system complexity." />
<figcaption>Figure 3: Supervised learning data flow. The dataset <span
class="math inline">\({(x_i, y_i)} \in D\)</span> is used to train the
model <span class="math inline">\(f \in \mathcal{F}\)</span> to minimize
an objective function with two terms, a data dependant loss <span
class="math inline">\(L\)</span>, and a regularization <span
class="math inline">\(R\)</span> measuring the system
complexity.</figcaption>
</figure>
<p>In the following, we investigate two examples where supervised
learning is first applied to a Neural Network (NN) regression problem,
and then a NN classification problem. The examples highlight the
objective functions composed by the loss and the regularization term for
regression and classification respectively.</p>
<p><strong>Regression Problem:</strong> Let us consider the distribution
<span class="math inline">\(D\)</span> represented by the <span
class="math inline">\(sin\)</span> function in the <span
class="math inline">\([-3 \pi; 3 \pi]\)</span> range (see <a
href="#fig:regression">Fig 4</a>). We sample <span
class="math inline">\(50\)</span> pairs <span
class="math inline">\((x_i, y_i)\)</span> with <span
class="math inline">\(X \in [-3 \pi; 3 \pi]\)</span> and <span
class="math inline">\(Y \in [-1; 1]\)</span>. Our objective is to learn
a regressor <span class="math inline">\(f_\theta\)</span>, a three
layers NN parametrized by its weights <span class="math inline">\(\{w_0,
W_1, w_2\} = \theta\)</span>. <span class="math inline">\(w_0\)</span>
contains <span class="math inline">\((1 \times 16) + 1\)</span> weights,
<span class="math inline">\(W_1\)</span>, <span
class="math inline">\((16 \times 16) + 1\)</span>, and <span
class="math inline">\(w_2\)</span>, <span class="math inline">\((16
\times 1) + 1\)</span>. In this case, the function space is limited to
the three layers NN family with <span class="math inline">\(291\)</span>
parameters <span class="math inline">\(\mathcal{F}\)</span>.</p>
<figure id="fig:regression">
<img src="./figures/core_nn_regression.svg"
alt="Neural Network (NN) regression example. The model f_\theta is fit on the training set (X, Y) \in D representing the sin function in the range [-3 \pi; 3 \pi]." />
<figcaption>Figure 4: Neural Network (NN) regression example. The model
<span class="math inline">\(f_\theta\)</span> is fit on the training set
<span class="math inline">\((X, Y) \in D\)</span> representing the <span
class="math inline">\(sin\)</span> function in the range <span
class="math inline">\([-3 \pi; 3 \pi]\)</span>.</figcaption>
</figure>
<p>To achieve this goal using supervised learning, we can optimize the
following objective function (see <a
href="#eq:reg_sin_objective">Eq 4</a>):</p>
<p><span id="eq:reg_sin_objective"><span class="math display">\[
f^* = arg \; \underset{\theta}{min} \; \frac{1}{n} \sum_{i=1}^{n}
(f_\theta(x_i) - y_i)^2 + \lambda ||\theta||_2^2
\qquad{(4)}\]</span></span></p>
<p>where the loss is the Mean Squared Error (MSE) <span
class="math inline">\(||.||_2^2\)</span> between the ground-truth <span
class="math inline">\(y_i\)</span> and the prediction <span
class="math inline">\(\hat{y_i} = f_\theta(x_i)\)</span>, and the
weighted regularization term <span class="math inline">\(\lambda
||\theta||_2^2\)</span> to penalize the model for having large weights
and converge to a simpler solution.</p>
<p><strong>Classification Problem:</strong> Let us consider the
distribution <span class="math inline">\(D\)</span> representing the 2d
positions of two clusters <span class="math inline">\({0, 1} \in
K\)</span> of moons (see <a href="#fig:classification">Fig 5</a>). We
sample <span class="math inline">\(250\)</span> moon <span
class="math inline">\((x_i, y_i)\)</span> with <span
class="math inline">\(X \in [-1; 1]\)</span> and <span
class="math inline">\(Y \in [-1; 1]\)</span>. Our objective is to learn
a classifier <span class="math inline">\(f_\theta\)</span>, a three
layers NN parametrized by its weights <span class="math inline">\(\{w_0,
W_1, w_2\} = \theta\)</span>. <span class="math inline">\(w_0\)</span>
contains <span class="math inline">\((1 \times 32) + 1\)</span> weights,
<span class="math inline">\(W_1\)</span>, <span
class="math inline">\((32 \times 32) + 1\)</span>, and <span
class="math inline">\(w_2\)</span>, <span class="math inline">\((32
\times 1) + 1\)</span>. In this case, the function space is limited to
the three layers NN family with <span
class="math inline">\(1,091\)</span> parameters <span
class="math inline">\(\mathcal{F}\)</span>.</p>
<figure id="fig:classification">
<img src="./figures/core_nn_classification.svg"
alt="Neural Network (NN) classification example. The model f_\theta is trained to classify moons based on their positions. The decision boundary is shown." />
<figcaption>Figure 5: Neural Network (NN) classification example. The
model <span class="math inline">\(f_\theta\)</span> is trained to
classify moons based on their positions. The decision boundary is
shown.</figcaption>
</figure>
<p>To achieve this goal using supervised learning, we can optimize an
objective function similar to the regression problem (see <a
href="#eq:reg_sin_objective">Eq 4</a>) using the cross-entropy as the
loss function (see <a href="#eq:cross_entropy">Eq 5</a>), measuring the
classification discordance.</p>
<p><span id="eq:cross_entropy"><span class="math display">\[
\mathcal{L} (\hat{y}, y) = \sum_{k=1}^{K} y_k \; log \; \hat{y}_k
\qquad{(5)}\]</span></span></p>
<h3 id="sec:II.1.2">Optimization</h3>
<p>In ML, supervised problems can be reduced to an optimization problem
where the computer has to find a set of parameters, weights <span
class="math inline">\(\theta\)</span>, for a given function class <span
class="math inline">\(\mathcal{F}\)</span> by optimizing an objective
function <span class="math inline">\(\theta^* = arg \; min_\theta
\mathcal{C(\theta)}\)</span> made out of two components, a
data-dependant loss <span class="math inline">\(L\)</span> and a
regularization <span class="math inline">\(R\)</span>.</p>
<p><strong>Random Search:</strong> One way to find such a function <span
class="math inline">\(f_\theta\)</span> that satisfies this objective is
to estimate the objective function for a set of random parameter
initializations and take the one that minimizes <span
class="math inline">\(C\)</span> the most. This <span
class="math inline">\(\theta\)</span> setting can then be refined by
applying random perturbations to the parameters and repeating the
operation. This is possible due to the fact that we can computer <span
class="math inline">\(C(\theta)\)</span> for any value of <span
class="math inline">\(\theta\)</span> taking the average loss for a
given dataset. However, such an approach to optimization is unpractical.
NN often comes with millions or billions of parameters <span
class="math inline">\(\theta\)</span> making random-search
intractable.</p>
<p><strong>First Order Derivation:</strong> A more efficient approach is
to make the objective function <span class="math inline">\(C\)</span>
and the model <span class="math inline">\(f_\theta\)</span>
differentiable. This constraint allows us to compute the gradient of the
cost <span class="math inline">\(C\)</span> with respect to the model’s
parameters <span class="math inline">\(\theta\)</span>. The value <span
class="math inline">\(\nabla_\theta C\)</span> can be obtained using
backpropagation (discussed in the next sub-section
Sec <strong>¿sec:backpropagation?</strong>). This vector of first order
derivatives indicates the direction from which we need to move the
weights <span class="math inline">\(\theta\)</span> away. By taking
small iterative steps toward the negative direction of the gradients, we
can improve <span class="math inline">\(\theta\)</span>. This algorithm
is called GD. In practice, due to the very large size of the datasets
(<span class="math inline">\(14,197,122\)</span> images for ImageNet
<span class="citation" data-cites="deng_2009">[<a href="#ref-deng_2009"
role="doc-biblioref">16</a>]</span>), the objective gradient is
approximated using a small subset of the training data for each step
referred to as a minibatch. This approximation of the GD is called
Stochastic Gradient Descent (SGD).</p>
<p>One critical aspect of the SGD algorithm is the hyperparameter <span
class="math inline">\(\epsilon\)</span>, the learning rate. It controls
the size of the step we take toward the negative gradients. If it is too
height or too low, the optimization may not converge toward an
acceptable local minimum. A toy example is provided in <a
href="#fig:toysgd">Fig 6</a> where different learning rates are used to
find the minimum of the square function <span class="math inline">\(y =
x^2\)</span>.</p>
<figure id="fig:toysgd">
<img src="./figures/core_nn_sgd.svg"
alt="Toy example where different learning rates \epsilon are used to find the minimum of the square function y = x^2 using the Gradient Descent (GD) algorithm starting from x = -1. Some learning rate setup result in situations where the optimization does not converge to the solution. A learning rate \epsilon = 2 diverges toward infinity, \epsilon = 1 is stuck and bounces between two positions -1 and 1. However, a small learning rate \epsilon = 0.1 &lt; 1 converges towards the minimum y = 0. This example illustrates the impact of the hyperparameter \epsilon on GD." />
<figcaption>Figure 6: Toy example where different learning rates <span
class="math inline">\(\epsilon\)</span> are used to find the minimum of
the square function <span class="math inline">\(y = x^2\)</span> using
the Gradient Descent (GD) algorithm starting from <span
class="math inline">\(x = -1\)</span>. Some learning rate setup result
in situations where the optimization does not converge to the solution.
A learning rate <span class="math inline">\(\epsilon = 2\)</span>
diverges toward infinity, <span class="math inline">\(\epsilon =
1\)</span> is stuck and bounces between two positions <span
class="math inline">\(-1\)</span> and <span
class="math inline">\(1\)</span>. However, a small learning rate <span
class="math inline">\(\epsilon = 0.1 &lt; 1\)</span> converges towards
the minimum <span class="math inline">\(y = 0\)</span>. This example
illustrates the impact of the hyperparameter <span
class="math inline">\(\epsilon\)</span> on GD.</figcaption>
</figure>
<p><strong>First Order Derivation with Momentum:</strong> The DL
literature contains abundant work on first order optimizer variants
aiming for faster convergence such as SGD with Momentum <span
class="citation" data-cites="qian_1999">[<a href="#ref-qian_1999"
role="doc-biblioref">64</a>]</span>, Adagrad <span class="citation"
data-cites="duchi_2011">[<a href="#ref-duchi_2011"
role="doc-biblioref">19</a>]</span>, RMSProp <span class="citation"
data-cites="hinton_lecture6a">[<a href="#ref-hinton_lecture6a"
role="doc-biblioref">34</a>]</span>, Adam <span class="citation"
data-cites="kingma_2014">[<a href="#ref-kingma_2014"
role="doc-biblioref">42</a>]</span>, and its correction AdamW <span
class="citation" data-cites="loshchilov_2017">[<a
href="#ref-loshchilov_2017" role="doc-biblioref">49</a>]</span>. A toy
example is shown <a href="#fig:sgd_moments">Fig 7</a>.</p>
<p>The Momentum update <span class="citation" data-cites="qian_1999">[<a
href="#ref-qian_1999" role="doc-biblioref">64</a>]</span> introduces the
use of a momentum inspired by physics’ first principles to favor small
and consistent gradient directions. In this particular case, the
momentum is represented by a variable <span
class="math inline">\(v\)</span> updated to store an exponential
decaying sum of the previous gradients <span class="math inline">\(v :=
\alpha v + \nabla_\theta C(\theta)\)</span>. The weights are then
updated using negative <span class="math inline">\(v\)</span> as the
gradient direction instead of <span class="math inline">\(\nabla_\theta
C(\theta)\)</span>.</p>
<p>Other optimizers also make use of the second moment of the gradients.
Adagrad <span class="citation" data-cites="duchi_2011">[<a
href="#ref-duchi_2011" role="doc-biblioref">19</a>]</span> uses another
variable <span class="math inline">\(r\)</span> to store the second
moment <span class="math inline">\(r := r + \nabla_\theta C(\theta)
\odot \nabla_\theta C(\theta)\)</span> and modulate the update rule
toward the negative direction <span
class="math inline">\(\frac{1}{\delta + \sqrt{r}} \odot \nabla_\theta
C(\theta)\)</span> where <span class="math inline">\(\delta\)</span> is
a small value to avoid division by zero. Similarly, RMSProp <span
class="citation" data-cites="hinton_lecture6a">[<a
href="#ref-hinton_lecture6a" role="doc-biblioref">34</a>]</span>
maintains a running mean of the second moment <span
class="math inline">\(r := \rho r + (1 - \rho) \nabla_\theta C(\theta)
\odot \nabla_\theta C(\theta)\)</span>.</p>
<p>Finally Adam <span class="citation" data-cites="kingma_2014">[<a
href="#ref-kingma_2014" role="doc-biblioref">42</a>]</span>, and its
correction AdamW <span class="citation" data-cites="loshchilov_2017">[<a
href="#ref-loshchilov_2017" role="doc-biblioref">49</a>]</span>, are
applying both Momentum and RMSProp estimating the first and second
moment to make parameters with large gradients take small steps and
parameters with low gradients take larger ones. This has the advantage
to allow for bigger learning rates and faster convergence at the cost of
triple the amount of parameters to store during training.</p>
<figure id="fig:sgd_moments">
<img src="./figures/core_nn_sgd_moments.svg"
alt="This toy example illustrates the impact of the optimizer choice during objective minimization with first order methods. SGD, Momentum, Adagrad, RMSProp and Adam are tasked to find the minimum of a 1-dimensional mixture of Gaussians given the same starting point x = 1 and the same learning rate \epsilon = 0.5. In this particular setup, Moments and Adagrad find the solution, RMSProp explodes, and SGD and Adam are stuck into a local minimum." />
<figcaption>Figure 7: This toy example illustrates the impact of the
optimizer choice during objective minimization with first order methods.
SGD, Momentum, Adagrad, RMSProp and Adam are tasked to find the minimum
of a 1-dimensional mixture of Gaussians given the same starting point
<span class="math inline">\(x = 1\)</span> and the same learning rate
<span class="math inline">\(\epsilon = 0.5\)</span>. In this particular
setup, Moments and Adagrad find the solution, RMSProp explodes, and SGD
and Adam are stuck into a local minimum.</figcaption>
</figure>
<p><strong>Cross-Validation and HyperParameter Search:</strong> As
illustrated by the toy examples (see Figs <a href="#fig:toysgd">6</a>,
<a href="#fig:sgd_moments">7</a>), the training of NN using SGD is
highly dependent on the initial setting of hyperparameters. One could
ask if there is a rule for choosing such parameters. Unfortunately, this
is not the case. The field is highly empirical and driven by exploration
using the scientific method.</p>
<p>One common approach is to set up metrics to evaluate the performance
of the model during the optimization process. It is a good practice to
divide the dataset into validation folds that are different from the
training data to evaluate the generalization capabilities of the model.
This practice is referred to as <span
class="math inline">\(k\)</span>-fold cross-validation and is most of
the time in DL, because of the large datasets, reduced to a single fold,
called the validation set. By defining such a process, NN can be
compared in a controlled manner and the hyperparameter space can be
searched. Hyperparameter search is so important that it is a subfield of
its own. The broad DL literature however contains many examples of
initial parameters and architectures that can be used to bootstrap this
search.</p>
<h3 id="sec:II.1.3">Backpropagation</h3>
<p>In the previous sub-section (see
Sec <strong>¿sec:optimization?</strong>), we saw how to learn
parametrized functions <span class="math inline">\(f_\theta\)</span>
given a training dataset. By evaluating the gradients of the objective
function with respect to the model’s parameters, it is possible to
obtain a good enough mapping <span class="math inline">\(f_\theta: X
\rightarrow Y\)</span>. In this sub-section, we discuss backpropagation,
the recursive algorithm used to efficiently compute those gradients
exploiting the chain rule <span class="math inline">\(\frac{\partial
z}{\partial x} = \frac{\partial z}{\partial y} \cdot \frac{\partial
y}{\partial x}\)</span> with <span class="math inline">\(z\)</span>
dependant on <span class="math inline">\(y\)</span> and <span
class="math inline">\(y\)</span> on <span
class="math inline">\(x\)</span>.</p>
<figure id="fig:dag">
<img src="./figures/core_nn_dag.svg"
alt="Illustration of reverse mode Automatic Differentiation (AD). This Directed Acyclic Graph (DAG) shows the forward pass in green and backward in red. The gradient of an activation is computed by multiplying the local gradient of a node by its output gradient computed in the previous step when following backward differentiation \frac{\partial C}{\partial x} = \frac{\partial z}{\partial x} \cdot \frac{\partial C}{\partial z} where \frac{\partial z}{\partial x} is the location derivative and \frac{\partial C}{\partial z} the output one." />
<figcaption>Figure 8: Illustration of reverse mode Automatic
Differentiation (AD). This Directed Acyclic Graph (DAG) shows the
forward pass in green and backward in red. The gradient of an activation
is computed by multiplying the local gradient of a node by its output
gradient computed in the previous step when following backward
differentiation <span class="math inline">\(\frac{\partial C}{\partial
x} = \frac{\partial z}{\partial x} \cdot \frac{\partial C}{\partial
z}\)</span> where <span class="math inline">\(\frac{\partial z}{\partial
x}\)</span> is the location derivative and <span
class="math inline">\(\frac{\partial C}{\partial z}\)</span> the output
one.</figcaption>
</figure>
<p><strong>Automatic Differentiation:</strong> In mathematics, AD
describes the set of techniques used to evaluate the derivative of a
function and exploits the fact that any complex computation can be
transformed into a sequence of elementary operations and functions with
known symbolic derivatives. By applying the chain rule recursively to
this sequence of operations, one can automatically compute the
derivatives with precision at the cost of storage.</p>
<p>We distinguish two modes of operation for AD, forward mode
differentiation, and reverse mode differentiation. In forward mode, the
derivatives are computed after applying each elementary operation and
function in order using the chain rule. It requires storing the
gradients along the way and carrying them until the last computation.
This mode is preferred when the size of the outputs exceeds the size of
the inputs. This is generally not the case for NN where the input, an
image for example, is larger than the output, a scalar for the objective
function. On the opposite, reverse mode differentiation traverses the
sequence of operations from end to start using the chain rule and
requires storing the output of the operations instead. This method is
preferred when the size of the inputs exceeds the outputs. This mode
thus has to happen in two passes, a forward pass where one computes the
output of every operation in the order, and a backward pass, where the
sequence of operations is traversed in backward order to compute the
derivatives.</p>
<p><strong>Computation Graph:</strong> A NN can be defined as a
succession of linear transformations followed by non-linear activations
(discussed in the next section Sec <strong>¿sec:nn?</strong>). Those
elementary operations are differentiable and when thinking of the data
flow can be viewed as a computation DAG to which backpropagation,
reverse mode differentiation, can be applied.</p>
<p>In modern DL frameworks <span class="citation"
data-cites="pytorch tensorflow">[<a href="#ref-tensorflow"
role="doc-biblioref">1</a>, <a href="#ref-pytorch"
role="doc-biblioref">61</a>]</span>, the AD is centered on the
implementation of a Graph object with Nodes. Both entities possess a
<code>forward()</code> and a <code>backward()</code> function. The
forward pass calls the <code>forward()</code> function of each node of
the graph by traversing it in order while saving the node output for
differentiation. The backward pass traverses the graph recursively in
backward order calling the <code>backward()</code> function responsible
for computing the local gradient of the node operation and multiplying
it by its output gradient following the chain rule. Nodes are in most
frameworks referred to as Layers, the elementary building block of the
NN operation chain.</p>
<p>Fortunatly open-source implementations of such engines are already
available and extensively used by the DL community. They have the
adantage to work at the Tensor level, not at the Scalar level like
Micrograd, and offer support for accelerated hardware such as Graphical
Processing Units (GPU), Tensor Processing Units (TPU), and Neural
Processing Units (NPU). In this dissertation, most examples are using
the PyTorch <span class="citation" data-cites="pytorch">[<a
href="#ref-pytorch" role="doc-biblioref">61</a>]</span> framework, a
Python Tensor library written in C++ and equipped with a powerful eager
mode reverse AD engine.</p>
<p><strong>Eager or Graph Execution:</strong> Modern DL frameworks such
as PyTorch <span class="citation" data-cites="pytorch">[<a
href="#ref-pytorch" role="doc-biblioref">61</a>]</span> and Tensorflow
<span class="citation" data-cites="tensorflow">[<a
href="#ref-tensorflow" role="doc-biblioref">1</a>]</span> now propose
two execution modes. An eager mode, where the graph is built dynamically
and operations are applied immediately, and a graph mode where the
computational graph has to be defined beforehand. Both modes come with
advantages and inconveniences. Eager mode is useful for iterative
development and provides an intuitive interface similar to imperative
programming, it is easier to debug and offers natural control flows as
well as hardware acceleration support. On the other side, graph mode
allows for more efficient execution. The graph can be optimized by
applying operations similar to the ones used in programming language
Abstract Syntax Trees (AST). Graph edges can be merged into a single
fused operation, and execution can be optimized for parallelization. It
is often the preferred way for deployment where the execution time and
memory are at stake.</p>
<h2 id="sec:II.2">Artificial Neural Networks</h2>
<p>In the previous section, we described the general setup for ML, where
one has to fit a model from a given function family <span
class="math inline">\(f \in \mathcal{F}\)</span> on a given dataset
<span class="math inline">\((X, Y) \in D\)</span> optimized using +sdg
and backpropagation. This section begins discussing a particular class
of parameterized function <span class="math inline">\(f_\theta\)</span>
called Neural Networks (NN).</p>
<h3 id="sec:II.2.1">Perceptron</h3>
<p>The Perceptron, introduced by Frank Rosenblatt in 1958 <span
class="citation" data-cites="rosenblatt_1958">[<a
href="#ref-rosenblatt_1958" role="doc-biblioref">68</a>]</span>, is the
building block of Neural Networks (NN). It was introduced as a
simplified model of the human neuron, containing three parts: dendrites
handling incoming signals from other neurons, a soma with a nucleus
responsible for signal aggregation, and an axone responsible for the
transmission of the processed signal to other neurons. When the signal
aggregation in the soma reaches a predefined threshold, the neuron
activates. This phenomenon is called an action potential. Although this
is not an accurate representation of the modern neuroscience state of
knowledge, this simplified model was believed to be accurate at the
time.</p>
<figure id="fig:perceptron">
<img src="./figures/core_nn_perceptron.svg"
alt="Diagram of a Perceptron with three inputs \{x_1; x_2; x_3\}. The perceptron computes an activated weighted sum of its inputs y = \sigma(\sum_{i=1}^{3} w_i \cdot x_i) where \sigma, the activation function is a threshold function." />
<figcaption>Figure 9: Diagram of a Perceptron with three inputs <span
class="math inline">\(\{x_1; x_2; x_3\}\)</span>. The perceptron
computes an activated weighted sum of its inputs <span
class="math inline">\(y = \sigma(\sum_{i=1}^{3} w_i \cdot x_i)\)</span>
where <span class="math inline">\(\sigma\)</span>, the activation
function is a threshold function.</figcaption>
</figure>
<p>Similarly, the Perceptron computes a weighted sum of its inputs and
activates if a certain threshold is reached (see <a
href="#fig:perceptron">Fig 9</a>). The Perceptron is parametrized by the
weights representing the importance attributed to the incoming inputs
and are part of the parameters <span
class="math inline">\(\theta\)</span> that are trained on a given
dataset. It can be viewed as a learned linear regressor followed by a
non-linear activation, historically a threshold function, a function
<span class="math inline">\(\sigma\)</span> that activates <span
class="math inline">\(\sigma(x) = 1\)</span> when <span
class="math inline">\(x &gt; 0.5\)</span> and <span
class="math inline">\(\sigma(x) = 0\)</span> otherwise.</p>
<p>The objective of a perceptron is to learn a hyperplane, a plane with
<span class="math inline">\(n - 1\)</span> dimensions where <span
class="math inline">\(n\)</span> is the number of inputs, that can
perform binary classification, separate two classes. However, as
mentioned by Marvin L. Minsky and al. in their controversial book
Perceptrons <span class="citation" data-cites="minsky_1969">[<a
href="#ref-minsky_1969" role="doc-biblioref">52</a>]</span>, a
hyperplane regressor cannot solve a simple XOR problem (see <a
href="#fig:xor">Fig 10</a>).</p>
<figure id="fig:xor">
<img src="./figures/core_nn_xor.svg"
alt="Illustration of the Perceptron’s decision hyperplane when trained to solve the AND problem on the left, the OR problem in the middle, and the XOR problem on the right. The first two problems are linearly sperable, thus adapted for a Perceptron. However, a single perceptron cannot solve the XOR problem as it is not linearly separable." />
<figcaption>Figure 10: Illustration of the Perceptron’s decision
hyperplane when trained to solve the AND problem on the left, the OR
problem in the middle, and the XOR problem on the right. The first two
problems are linearly sperable, thus adapted for a Perceptron. However,
a single perceptron cannot solve the XOR problem as it is not linearly
separable.</figcaption>
</figure>
<h3 id="sec:II.2.2">Multi-Layer Perceptron</h3>
<p>The real value of the Perceptron comes when assembled into a
hierarchical and layer-wise architecture, a Neural Network (NN). By
repeating matrix multiplications (linear transformations) and
non-linearities the network is able to handle non-linear problems and
act as a universal function approximator <span class="citation"
data-cites="hornik_1989">[<a href="#ref-hornik_1989"
role="doc-biblioref">36</a>]</span>. This arrangement of layered
perceptrons is called a Multi-Layer Perceptron (MLP) (see <a
href="#fig:mlp">Fig 11</a>).</p>
<figure id="fig:mlp">
<img src="./figures/core_nn_mlp.svg" style="width:90.0%"
alt="Diagram of a 3-layer Multi-Layer Perceptron (MLP). When using the matrix formulation, this arrangement of neurons can be summarized into a single expression y = \sigma(\sigma(x \cdot W_1^T) \cdot W_2^T) \cdot W_3^T." />
<figcaption>Figure 11: Diagram of a 3-layer Multi-Layer Perceptron
(MLP). When using the matrix formulation, this arrangement of neurons
can be summarized into a single expression <span class="math inline">\(y
= \sigma(\sigma(x \cdot W_1^T) \cdot W_2^T) \cdot
W_3^T\)</span>.</figcaption>
</figure>
<p>A MLP with Identity as its activation function is useless as its
chain of linear transformations can be collapsed into a single one.
Since the advent of the Perceptron, the literature has moved away from
using threshold functions as activations. Common activation functions
are the sigmoid <span class="math inline">\(\sigma(x) = \frac{1}{1 +
e^{-x}}\)</span>, tanh <span class="math inline">\(tanh(x) = \frac{e^{z}
- e^{-z}}{e^{z} + e^{-z}}\)</span>, Rectified Linear Unit (ReLU) <span
class="math inline">\(ReLU(x) = max(x, 0)\)</span> functions and
variants presenting additional properties such as infinite continuity,
gradient smoothness, and more (see <a
href="#fig:activations">Fig 12</a>).</p>
<figure id="fig:activations">
<img src="./figures/core_nn_activations.svg"
alt="Activation functions. Sigmoid \sigma(x) = \frac{1}{1 + e^{-x}} acts as a filter y \in [0; 1], tanh tanh(x) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} acts as a normalization compressor y \in [-1; 1], ReLU ReLU(x) = max(x, 0) folds all negatives down to zero y \in [0; +\infty]." />
<figcaption>Figure 12: Activation functions. Sigmoid <span
class="math inline">\(\sigma(x) = \frac{1}{1 + e^{-x}}\)</span> acts as
a filter <span class="math inline">\(y \in [0; 1]\)</span>, tanh <span
class="math inline">\(tanh(x) = \frac{e^{z} - e^{-z}}{e^{z} +
e^{-z}}\)</span> acts as a normalization compressor <span
class="math inline">\(y \in [-1; 1]\)</span>, ReLU <span
class="math inline">\(ReLU(x) = max(x, 0)\)</span> folds all negatives
down to zero <span class="math inline">\(y \in [0;
+\infty]\)</span>.</figcaption>
</figure>
<p><strong>MNIST Classifier:</strong> A classic toy example showing the
capabilities of MLPs is the handwritten digit classification challenge
on the Modified National Institute of Standards and Technology (MNIST)
dataset <span class="citation" data-cites="mnist">[<a href="#ref-mnist"
role="doc-biblioref">80</a>]</span>. MNIST contains <span
class="math inline">\(60,000\)</span> training and <span
class="math inline">\(10,000\)</span> test examples. It has been written
by high school students and gather <span class="math inline">\(28 \times
28\)</span> centered black and white handwritten digits from <span
class="math inline">\(0\)</span> to <span
class="math inline">\(9\)</span> (see <a
href="#fig:mnist">Fig 13</a>).</p>
<figure id="fig:mnist">
<img src="./figures/core_nn_mnist.svg"
alt="First 27 handwritten digits from the Modified National Institute of Standards and Technology (MNIST) dataset. The digits are stored as 28 \times 28 centered black and white images." />
<figcaption>Figure 13: First <span class="math inline">\(27\)</span>
handwritten digits from the Modified National Institute of Standards and
Technology (MNIST) dataset. The digits are stored as <span
class="math inline">\(28 \times 28\)</span> centered black and white
images.</figcaption>
</figure>
<p>Training a MLP on such a challenge is simple and effective. With
little training, parameters (according to the DL standards), and no
hyperparameter tweaking, a vanilla 3-layer NN with ReLU activations can
achieve <span class="math inline">\(97.5%\)</span> accuracy on the test
set. The inputs however need to be transformed before ingestion by the
model as MLPs are constrained to <span
class="math inline">\(1\)</span>-dimensional input vectors.</p>
<figure id="fig:mnist_history">
<img src="./figures/core_nn_mnist_history.svg"
alt="Training history of a 3-layer Multi-Layer Perceptron (MLP) with 128 neurons in every layer on the MNIST dataset. The average loss (cross-entropy) on the left, and the accuracy on the right are displayed for the training, validation, and test splits." />
<figcaption>Figure 14: Training history of a 3-layer Multi-Layer
Perceptron (MLP) with <span class="math inline">\(128\)</span> neurons
in every layer on the MNIST dataset. The average loss (cross-entropy) on
the left, and the accuracy on the right are displayed for the training,
validation, and test splits.</figcaption>
</figure>
<h3 id="sec:II.2.3">Convolutional Neural Network</h3>
<p>While MLPs can be viewed as universal function approximators, they
scale poorly with respect to high dimensional inputs such as images,
videos, sound representations such as a spectrogram, volumetric data,
and long sequences. For example, if we consider a small RGB image of
size <span class="math inline">\(256 \times 256 \times 3\)</span>, the
input of a MLP would be a 1-dimensional vector of size <span
class="math inline">\(196,608\)</span>. The input layer of a MLP with
<span class="math inline">\(64\)</span> neurons would already mean that
the network contains more than <span
class="math inline">\(12,582,912\)</span> parameters. For this reason,
researchers have created specialized NNs with biases in their
architecture inspired by cognitive and biophysical mechanisms.
Convolutional Neural Networks (CNN) (ConvNets) are such a NN specialized
in handling spatially correlated data such as images.</p>
<figure id="fig:convolution">
<img src="./figures/core_nn_convolution.svg"
alt="Illustration of a single 3 \times 3 \times 3 filter convolution in the middle applied to a 8 \times 8 \times 3 input tensor on the left. The result is a 6 \times 6 \times 1 activation map on the right. The filter receptive field is drawn in dashed lines. This convolution is applied in valid model, no passing was applied to the input resulting in a lower resolution output tensor." />
<figcaption>Figure 15: Illustration of a single <span
class="math inline">\(3 \times 3 \times 3\)</span> filter convolution in
the middle applied to a <span class="math inline">\(8 \times 8 \times
3\)</span> input tensor on the left. The result is a <span
class="math inline">\(6 \times 6 \times 1\)</span> activation map on the
right. The filter receptive field is drawn in dashed lines. This
convolution is applied in valid model, no passing was applied to the
input resulting in a lower resolution output tensor.</figcaption>
</figure>
<p><strong>Convolution:</strong> The core component of a ConvNet is the
convolution operation. A +CNN operates by convolving (rolling) a set of
parametrized filters on the input. If we reconsider our <span
class="math inline">\(W_1 \times H_1 \times D_1 = 256 \times 256 \times
3\)</span>, convolving a single filter of size <span
class="math inline">\(F_W \times F_H \times D_1 = 3 \times 3 \times
3\)</span> would require sliding the filter across the entire input
image tensor and computing the dot product of the overlapping tensor
chunk and the filter. This operation results in what is called an
activation map, or feature map. The filter can be convolved in different
configurations. The stride <span class="math inline">\(S\)</span>
defines the hop size when rolling the filter over the input, and the
padding <span class="math inline">\(P\)</span> defines the additional
border added to the input tensor in order to parkour the input border
(<span class="math inline">\(252\)</span> unique positions for the
filter in the <span class="math inline">\(256\)</span> image, <span
class="math inline">\(256\)</span> positions with a padding of <span
class="math inline">\(1\)</span> on each side of the input). A CNN
convolves multiple parametrized filters <span
class="math inline">\(K\)</span> in a single convolution operation.
Given a convolution setting, the operation requires <span
class="math inline">\(F_H \times F_W \times D_1 \times D_2\)</span>
parameters and outputs a feature map tensor of size <span
class="math inline">\(W_2 = (W_1 - F_W + 2P_W) / S + 1\)</span>, <span
class="math inline">\(H_2 = (H_1 - F_H + 2P_H) / S + 1\)</span>, and
<span class="math inline">\(D_2 = K\)</span> (see <a
href="#fig:convolution">Fig 15</a>). The different filters are
responsible for looking for the activation of different patterns in the
input. The Convolution layer introduces the notion of weight sharing
enabled by the sliding filter (neurons) and reduces computation by a
large margin in comparison to a standard MLP layer.</p>
<p><strong>Pooling:</strong> It is common to follow convolution layers
by pooling layers to reduce the dimensionality when growing the ConvNet
deeper. The pooling layer reduces its input by applying a reduction
operation. The reduction operation can be taking the <code>max</code>,
<code>min</code>, or <code>average</code>, of a rolling window. This
operation does not involve any additional parameter and is applied
channel-wise. If we consider a max-pooling operation with a <span
class="math inline">\(2 \times 2\)</span> kernel and a stride of <span
class="math inline">\(2\)</span>, the output becomes half the size of
the input. It also has the benefit of making the CNN more robust to
scale and translation. It is sometimes more strategic to make use of
stride instead of adding pooling layers. It has the same benefit of
reducing the feature map size while avoiding an additional
operation.</p>
<figure id="fig:convnet">
<img src="./figures/core_nn_convnet.svg"
alt="Illustration of a small Convolutional Neural Networks (CNN) containing a convolution (conv) layer, a max-pooling (maxpool), and another convolution followed by another max-pooling. The last feature map is then flattened into a 1-dimensional vector and used as the input for the Multi-Layer Perceptron (MLP) classifier." />
<figcaption>Figure 16: Illustration of a small Convolutional Neural
Networks (CNN) containing a convolution (conv) layer, a max-pooling
(maxpool), and another convolution followed by another max-pooling. The
last feature map is then flattened into a <span
class="math inline">\(1\)</span>-dimensional vector and used as the
input for the Multi-Layer Perceptron (MLP) classifier.</figcaption>
</figure>
<p><strong>ConvNet:</strong> Finally, a CNN is assembled by stacking
multiple convolution layers and pooling layers. When the feature maps
are small enough, the final feature map is flattened and passed to an
additional MLP in charge of the classification or regression. This
combination of a parametric convolutional feature extractor and a MLP is
what we call a ConvNet.</p>
<figure id="fig:vgg_activations">
<img src="./figures/core_nn_vgg_activations.svg"
alt="Visualization of VGG16’s four first activation maps (feature maps). The input image is left and the activations are shown in order of the layers top to bottom and left to right. Credit https://images.all4ed.org/" />
<figcaption>Figure 17: Visualization of VGG16’s four first activation
maps (feature maps). The input image is left and the activations are
shown in order of the layers top to bottom and left to right. Credit <a
href="https://images.all4ed.org/">https://images.all4ed.org/</a></figcaption>
</figure>
<p><strong>Feature Maps:</strong> The feature maps learned by a CNN are
hierarchical. In the first layers, the learned filters are focusing on
simple features such as lines, diagonals, and arcs, and act as edge
detectors. The deeper the layers are, the more complex the features are
because they are resulting from a succession of combinations from
previous activations (see <a
href="#fig:vgg_activations">Fig 17</a>).</p>
<p><strong>Finetuning:</strong> Training CNNs on bigger and more diverse
datasets allows learning more general filters increasing the likelihood
that the network will perform on out-of-domain data. In practice, CNNs
are not often trained from scratch. Such a process requires the use of
expensive dedicated hardware and hours of training. However, thanks to
the open-source mindset of the DL field, big actors often share the
weights of such models referred to as pretrained models, or foundation
models <span class="citation" data-cites="foundation_2021">[<a
href="#ref-foundation_2021" role="doc-biblioref">7</a>]</span>.</p>
<p>Foundation models can be further refined through smaller training on
smaller and specialized datasets containing few good-quality examples.
This process is called finetuning and is less expensive and
time-consuming than full training. One method for finetuning consists in
removing the classification head of a pre-trained model such as VGG16
<span class="citation" data-cites="simonyan_2014">[<a
href="#ref-simonyan_2014" role="doc-biblioref">74</a>]</span> and
replacing it with a new one adapted to the number of classes required
for the task. The pretrained weights are then frozen (not updated during
training), and the new weights are trained following a standard
supervised-learning procedure.</p>
<p><strong>MNIST Classifier:</strong> Let us reconsider the MNIST toy
classification example and replace the MLP with a CNN. The model is
divided in two sections, the feature extractor made out of two
convolutional and max-pooling layers with <span class="math inline">\(5
\times 5\)</span> filters, the middle layer responsible for flattening
the feature maps down to a <span
class="math inline">\(1\)</span>-dimensional vector fed to the
classifier head, a <span class="math inline">\(3\)</span>-layer MLP
similar to the first one. The training procedure is left unchanged, the
number of parameters is approximately similar, a little less for the
CNN, and the number of epochs is the same. The input is however not
flattened as the CNN consumes a full image tensor.</p>
<p>The CNN is able to achieve a <span class="math inline">\(99%\)</span>
accuracy on the test set early during training (epoch <span
class="math inline">\(5\)</span>). The CNN is a more robust,
specialized, and thus more efficient architecture for handling images.
The training history can be observed in <a
href="#fig:mnist_convnet_history">Fig 18</a>.</p>
<figure id="fig:mnist_convnet_history">
<img src="./figures/core_nn_mnist_convnet_history.svg"
alt="Training history of a Convolutional Neural Network (CNN) made out of a sequence of two convolutions followed by max-pooling and a 3-layer Multi-Layer Perceptron (MLP) classifier on the MNIST dataset. The average loss (cross-entropy) on the left, and the accuracy on the right are displayed for the training, validation, and test splits." />
<figcaption>Figure 18: Training history of a Convolutional Neural
Network (CNN) made out of a sequence of two convolutions followed by
max-pooling and a 3-layer Multi-Layer Perceptron (MLP) classifier on the
MNIST dataset. The average loss (cross-entropy) on the left, and the
accuracy on the right are displayed for the training, validation, and
test splits.</figcaption>
</figure>
<h3 id="sec:II.2.4">Transformers</h3>
<p>When considering sequences such as text, audio, video, or a single
image divided into a sequence of <span class="math inline">\(n \times
n\)</span> blocks, MLPs and CNNs fail at capturing long-term
relationships being subject to vanishing and exploding gradients. When
taking a guess from a long sequence of information, we humans do not
attribute as much weight to every bit of the sequence. We selectively
gather and retain information that we feel serves our decision-making
called a task-dependent context. This selective mechanism is called
attention and has been the subject of implementation attempts, first in
the field of Natural Language Processing (NLP) <span class="citation"
data-cites="bahdanau_2014 luong_2015 vaswani_2017">[<a
href="#ref-bahdanau_2014" role="doc-biblioref">5</a>, <a
href="#ref-luong_2015" role="doc-biblioref">51</a>, <a
href="#ref-vaswani_2017" role="doc-biblioref">81</a>]</span>, and later
transposed to field of Computer Vision (CV) <span class="citation"
data-cites="dosovitskiy_2020 caron_2021">[<a href="#ref-caron_2021"
role="doc-biblioref">10</a>, <a href="#ref-dosovitskiy_2020"
role="doc-biblioref">18</a>]</span>.</p>
<p>This section focuses on visual applications of the attention
mechanism only as this thesis is about image generation, and more
specifically a particular type of attention that made the success of the
modern Transformer <span class="citation" data-cites="vaswani_2017">[<a
href="#ref-vaswani_2017" role="doc-biblioref">81</a>]</span>
architecture called Self-Attention.</p>
<p><strong>Self-Attention:</strong> Computing self-attention requires
the creation of three vectors for each element of the sequence in latent
space. For every element <span class="math inline">\(x_i\)</span>, we
create a query vector <span class="math inline">\(q_i\)</span>, a key
vector <span class="math inline">\(k_i\)</span>, and a value vector
<span class="math inline">\(v_i\)</span>. Those vectors are obtained by
multiplying the latent input vectors <span
class="math inline">\(x_i\)</span> by trainable embedding weight
matrices <span class="math inline">\(W_q\)</span>, <span
class="math inline">\(W_k\)</span>, and <span
class="math inline">\(W_v\)</span>. The generated output embeddings are
usually chosen to be smaller than the original input size to save
computation.</p>
<p>The query and key vectors are then combined using a dot product to
compute a score <span class="math inline">\(s_{i,j} = q_i \cdot
k_j\)</span> matching every query vector with every key. This score
determines how much focus is put on the element at position <span
class="math inline">\(j\)</span> in order to encode the element at
position <span class="math inline">\(i\)</span>. The scores are then
scaled by dividing them by the square root of the key vectors dimensions
<span class="math inline">\(\sqrt{d_k}\)</span> and normalizing them
using a softmax so they add up to one and form probability vectors. This
value describes how much each element attends to the others.</p>
<p>Finally, we apply and make the element attend to each other, by
multiplying the softmax vectors with the value vectors and aggregating
them with a sum. This final vector representing the new context for
which attention is used can be propagated to the next layers of the
NN.</p>
<p>When summarized and expressed in matrix form, by stacking the element
vectors <span class="math inline">\(x_i\)</span> into the matrix <span
class="math inline">\(X\)</span>, the self-attention mechanism can be
summarized into the following formula:</p>
<p><span id="eq:nn_sa_formula"><span class="math display">\[
\begin{aligned}
Q = (x \cdot W_q), \; K = (X \cdot W_k), \; V = (X \cdot W_v) \\
Z = softmax(\frac{Q \cdot K^T}{\sqrt{d_k}}) \cdot V
\end{aligned}
\qquad{(6)}\]</span></span></p>
<p>One other advantage of using such a technique is explainability. We
can explore the “reasoning” of the NN by looking at the attention scores
and observing how the sequence elements attend to each other.</p>
<p><strong>Multihead Self-Attention:</strong> In their paper <span
class="citation" data-cites="vaswani_2017">[<a href="#ref-vaswani_2017"
role="doc-biblioref">81</a>]</span>, the authors further refine this
notion of self-attention. Let us consider one self-attention module and
call it an attention head. They propose to stack multiple attention
heads and demonstrate improved performances. This notion enables models
equipped with multi-head attention to possess an ensemble of query, key,
and value subspaces with different representations. To optimize for
parallelism every head computation is done at the same time by combining
the weight matrices.</p>
<figure id="fig:core_nn_posenc">
<img src="./figures/core_nn_posenc.svg"
alt="Sinusoidal positional encoding for Transformers. The positional encoding is precomputed as a n \times d_x matrix, queried and added to the input embeddings before traversing the transformer blocks, with n the sequence length, and d_x the dimension of an embedding vector." />
<figcaption>Figure 19: Sinusoidal positional encoding for Transformers.
The positional encoding is precomputed as a <span
class="math inline">\(n \times d_x\)</span> matrix, queried and added to
the input embeddings before traversing the transformer blocks, with
<span class="math inline">\(n\)</span> the sequence length, and <span
class="math inline">\(d_x\)</span> the dimension of an embedding
vector.</figcaption>
</figure>
<p><strong>Positional Encoding:</strong> There is however an issue.
Contrary to CNNs exploiting the spatially local correlation of
information, self-attention does not encode the order of the vectors. To
alleviate this problem, Vaswani et al. <span class="citation"
data-cites="vaswani_2017">[<a href="#ref-vaswani_2017"
role="doc-biblioref">81</a>]</span> propose to add (concatenate) a
vector to each embedding input. This vector is designed to follow a
specific pattern learned by the model that will help encode the position
of information along its flow in the network. This is called a
positional encoding vector. Most positional encodings introduced by the
community follow some kind of general Fourier pattern that can scale
with the length of the sequence (see <a
href="#fig:core_nn_posenc">Fig 19</a>).</p>
<figure id="fig:core_nn_transformer_block">
<img src="./figures/core_nn_transformer_block.svg" style="width:80.0%"
alt="Illustrated Transformer block. The input is a sequence of embeddings x_i. The embeddings are augmented with positional encoding and sent to the transformer blocks. A block contains a succession of a first block of self-attention, residual connections, layer normalization, and a second block of feed-forward layer, residual connections, and layer normalization. The blocks are repeated N times." />
<figcaption>Figure 20: Illustrated Transformer block. The input is a
sequence of embeddings <span class="math inline">\(x_i\)</span>. The
embeddings are augmented with positional encoding and sent to the
transformer blocks. A block contains a succession of a first block of
self-attention, residual connections, layer normalization, and a second
block of feed-forward layer, residual connections, and layer
normalization. The blocks are repeated <span
class="math inline">\(N\)</span> times.</figcaption>
</figure>
<p><strong>Transformers:</strong> A Transformer architecture is defined
by a succession of transformer blocks (see <a
href="#fig:core_nn_transformer_block">Fig 20</a>). Those blocks follow a
similar pattern and are using tricks from previous work such as layer
normalization <span class="citation" data-cites="ba_2016">[<a
href="#ref-ba_2016" role="doc-biblioref">4</a>]</span> and residual
connections <span class="citation" data-cites="he_2016">[<a
href="#ref-he_2016" role="doc-biblioref">31</a>]</span> to allow to
train deeper networks with a large number of blocks. A block is made out
of a multi-head self-attention layer, followed by a residual injection
and a normalization layer followed by a feed-forward layer and another
residual injection and normalization. The input embeddings are first
augmented with positional encoding and go through every block one by
one. This architecture is flexible and has first been presented as a
sequence-to-sequence network with encoder blocks and decoder blocks.</p>
<figure id="fig:core_nn_patches">
<img src="./figures/core_nn_patches.svg"
alt="256 \times 256 image processed into 32 \times 32 patches with a no overlap. Each patch can be processed by a CNN or a MLP after flattening and their embedding used in a Transformer network which is thus called a Vision Transformer (ViT)" />
<figcaption>Figure 21: <span class="math inline">\(256 \times
256\)</span> image processed into <span class="math inline">\(32 \times
32\)</span> patches with a no overlap. Each patch can be processed by a
CNN or a MLP after flattening and their embedding used in a Transformer
network which is thus called a Vision Transformer (ViT)</figcaption>
</figure>
<p><strong>Vision Transformers:</strong> The success of the Transformer
architecture <span class="citation" data-cites="vaswani_2017">[<a
href="#ref-vaswani_2017" role="doc-biblioref">81</a>]</span> in the NLP
community has pushed the CV community to explore how one can use this
kind of NNs for vision tasks. It turns out that by expressing images as
sequences of patches, <span class="math inline">\(n \times n\)</span>
image blocks (see <a href="#fig:core_nn_patches">Fig 21</a>), and
embedding them, transformers can exceed the performance of a CNN for
classification <span class="citation" data-cites="dosovitskiy_2020">[<a
href="#ref-dosovitskiy_2020" role="doc-biblioref">18</a>]</span>, but
also present emerging faculties resulting from the use of self-attention
such as segmentation and saliency maps <span class="citation"
data-cites="caron_2021">[<a href="#ref-caron_2021"
role="doc-biblioref">10</a>]</span>. This architecture is referred to as
a ViT.</p>
<p><strong>MNIST Classifier:</strong> Let us reconsider the MNIST
classification toy example and train a ViT classifier. We first define
the <code>MultiHeadAttention</code> module responsible for computing the
attention maps and applying them to a sequence of image embedding
patches. It is common practice to remove the bias from the linear
projections (<code>Linear</code>) as the layer normalization
(<code>LayerNorm</code>) already learn how to shift the
distribution.</p>
<p>The training history of the ViT network with a patch size of <span
class="math inline">\(32 \times 32\)</span>, an embedding dimension of
<span class="math inline">\(128\)</span>, and <span
class="math inline">\(8\)</span> self-attention heads can be observed in
<a href="#fig:core_nn_vit_history">Fig 22</a>. The network has been
trained for <span class="math inline">\(10\)</span> epochs using the
standard Adam policy and a learning rate of <span
class="math inline">\(1e^{-2}\)</span>. This simple ViT achieve <span
class="math inline">\(98%\)</span> accuracy on the MNIST test set.
Results can be improved by replacing the MLP patch embedding by a CNN or
by augmenting the network size and tweaking its hyperparameters.</p>
<figure id="fig:core_nn_vit_history">
<img src="./figures/core_nn_vit_history.svg"
alt="Vision Transformer (ViT) training history. The ViT is trained to classifier the handwritten digit MNIST dataset for 10 epochs. The train, validation, test loss (cross-entropy) and accuracy are shown. The model reaches a 98% accuracy on the test set." />
<figcaption>Figure 22: Vision Transformer (ViT) training history. The
ViT is trained to classifier the handwritten digit MNIST dataset for
<span class="math inline">\(10\)</span> epochs. The train, validation,
test loss (cross-entropy) and accuracy are shown. The model reaches a
<span class="math inline">\(98%\)</span> accuracy on the test
set.</figcaption>
</figure>
<p>As shown in <a href="#fig:core_nn_vit_mha">Fig 23</a>, the attention
heads learn an ensemble of different attention mappings. The attention
maps can be observed using the output from the attention scores after
the softmax and before being applied to the projected values called
attention maps. The attention map is of size <span
class="math inline">\(B \times N \times S \times S\)</span> where <span
class="math inline">\(B\)</span> is the batch size, <span
class="math inline">\(N\)</span> the number of attention heads, and
<span class="math inline">\(S\)</span> the sequence length, in our case
the total number of patches.</p>
<figure id="fig:core_nn_vit_mha">
<img src="./figures/core_nn_vit_mha.svg"
alt="Visualization of a MNIST trained Vision Transformer (ViT) classifier multi-head attention maps. Each map corresponds to the attention map of a single head resulting from the multi-head self-attention computation." />
<figcaption>Figure 23: Visualization of a MNIST trained Vision
Transformer (ViT) classifier multi-head attention maps. Each map
corresponds to the attention map of a single head resulting from the
multi-head self-attention computation.</figcaption>
</figure>
<h2 id="sec:II.3">Summary</h2>
<p>Deep Learning (DL) is one of the most active fields of Computer
Science (CS). It has evolved through a complex and controversial history
and is now pervasive in our environments ranging from personal
assistants, autonomous cars, medical aids, recommender systems, search
engines, and more. The recent public success of generative AI such as
Dall-E <span class="citation" data-cites="openai_2023">[<a
href="#ref-openai_2023" role="doc-biblioref">11</a>]</span>, Stable
Diffusion <span class="citation" data-cites="rombach_2021">[<a
href="#ref-rombach_2021" role="doc-biblioref">66</a>]</span>, and
Chat-GPT <span class="citation" data-cites="openai_2023">[<a
href="#ref-openai_2023" role="doc-biblioref">11</a>]</span> has put the
research in creative tool building to the front of the scene. Those AI
tools enable the mass population to enhance its creative capabilities
but also ring the bell in the artist’s community concerning the ethical
and societal impact of such tools often trained on proprietary data from
which the rights are violated.</p>
<p>This thesis explores the building of such tools and focuses on the
training of conditional Generative Adversarial Networks (GAN) and <span
class="plural full">ddm</span> for user-guided automatic lineart
colorization.</p>
<!-- ===================== [END] PART BACKGROUND ===================== -->
<!-- ===================== [START] PART RELATED WORK ===================== -->
<h1 id="sec:III">Related Work</h1>
<p>In this chapter, we discuss related work for the task of automatic
colorization. The related contributions can be divided into two
categories of methods, classical algorithmic methods (see
Sec <strong>¿sec:rel-algo?</strong>) offering tools for flat
colorization mostly based on contour filling, and more recently Deep
Learning (DL)-based methods (see Sec <strong>¿sec:rel-deep?</strong>)
enabling end-to-end methods to color illustrations with inferred
semantic, lighting, and texture information. DL techniques offer
unprecedented means for model conditioning ranging from example-based,
tags and labels, natural language prompts and color hints in the form of
natural scribble lines or strokes.</p>
<h2 id="sec:III.1">Classical Approaches to Automatic Lineart
Colorization</h2>
<h3 id="sec:III.1.1">Delaunay-Triangulation</h3>
<p>Delaunay-triangulation is used by Wilkie et al. <span
class="citation" data-cites="wilkie_2020">[<a href="#ref-wilkie_2020"
role="doc-biblioref">59</a>]</span> and Parakkat et al. <span
class="citation" data-cites="parakkat_2022">[<a
href="#ref-parakkat_2022" role="doc-biblioref">60</a>]</span> to enable
controlled flat colorization of sketches that can contain unclosed
regions making it robust to variations in the input sketches contrary to
early work requiring the use of external tools to close them <span
class="citation" data-cites="gangnet_1994 sasaki_2017 liu_2019">[<a
href="#ref-gangnet_1994" role="doc-biblioref">24</a>, <a
href="#ref-liu_2019" role="doc-biblioref">47</a>, <a
href="#ref-sasaki_2017" role="doc-biblioref">72</a>]</span> some being
ML driven <span class="citation" data-cites="sasaki_2017 liu_2019">[<a
href="#ref-liu_2019" role="doc-biblioref">47</a>, <a
href="#ref-sasaki_2017" role="doc-biblioref">72</a>]</span>. While such
approaches do not provide lighting nor texture information to the
generated colored illustration, they can be used to initialize others
similar to the work of Frans et al. <span class="citation"
data-cites="frans_2017">[<a href="#ref-frans_2017"
role="doc-biblioref">21</a>]</span> although they use a DL approach.
They can also be used to build color-filling helper tools which are
handy for the animation industry where every frame of the show has to be
colored consistently.</p>
<p><strong><span class="citation" data-cites="gangnet_1994">[<a
href="#ref-gangnet_1994" role="doc-biblioref">24</a>]</span> Gagnet et
al.:</strong> Gagnet et al. presents a gap-closing method for cel
coloring in computer-assisted cartoon animation. It is based on
connected component analysis inside square stamps that are located near
potential gaps. This method is used to automatically close the regions
defined by freehand strokes.</p>
<p><strong><span class="citation" data-cites="sasaki_2017">[<a
href="#ref-sasaki_2017" role="doc-biblioref">72</a>]</span> Sasaki et
al.:</strong> In their paper, Sasaki et al. propose a novel data-driven
approach for automatically detecting and completing gaps in line
drawings without the need for user input. The proposed method is based
on a CNN which can learn the necessary heuristics for realistic line
completion from a dataset of line drawings. The results of the proposed
method are evaluated qualitatively and quantitatively and are shown to
significantly outperform the state-of-the-art.</p>
<p><strong><span class="citation" data-cites="liu_2019">[<a
href="#ref-liu_2019" role="doc-biblioref">47</a>]</span> Liu et
al.:</strong> In their contribution, SketchGAN, Liu et al. propose a new
GAN based approach for jointly completing and recognizing incomplete
hand-drawn sketches. The proposed method consists of a cascade
Encode-Decoder network to complete the input sketch iteratively and an
auxiliary sketch recognition task to recognize the completed sketch.
Experiments on the Sketchy database benchmark show that the proposed
joint learning approach outperforms the state-of-the-art methods in
sketch completion and recognition tasks. Further experiments on several
sketch-based applications validate the performance of the proposed
method.</p>
<figure id="fig:core_rel_delaunay">
<img src="./figures/core_rel_delaunay.png"
alt="Delaunay-triangulation for lineart and sketch segmentation-based flat colorization [59, 60]. Top is work from Wilkie et al. [59] and bottom from Parakkat et al. [60]." />
<figcaption>Figure 24: Delaunay-triangulation for lineart and sketch
segmentation-based flat colorization <span class="citation"
data-cites="wilkie_2020 parakkat_2022">[<a href="#ref-wilkie_2020"
role="doc-biblioref">59</a>, <a href="#ref-parakkat_2022"
role="doc-biblioref">60</a>]</span>. Top is work from Wilkie et al.
<span class="citation" data-cites="wilkie_2020">[<a
href="#ref-wilkie_2020" role="doc-biblioref">59</a>]</span> and bottom
from Parakkat et al. <span class="citation"
data-cites="parakkat_2022">[<a href="#ref-parakkat_2022"
role="doc-biblioref">60</a>]</span>.</figcaption>
</figure>
<p><strong><span class="citation" data-cites="wilkie_2020">[<a
href="#ref-wilkie_2020" role="doc-biblioref">59</a>]</span> Wilkie et
al.:</strong> In their work, Wilkie et al. introduced a
Delaunay-triangulation-based algorithm for interactive colorizations of
minimalist sketches. The aim is to reduce user intervention and make
interaction as easy and natural as with the flood-fill algorithm while
providing the ability to color regions with open contours. This is
accomplished in two steps: 1) an automatic segmentation step that
divides the input sketch into regions based on the underlying Delaunay
structure, and 2) an interactive grouping of neighboring regions based
on user input, which generates the final colored sketch. Results show
that the method is as natural as a bucket fill tool and powerful enough
to color minimalist sketches.</p>
<p><strong><span class="citation" data-cites="parakkat_2022">[<a
href="#ref-parakkat_2022" role="doc-biblioref">60</a>]</span> Parakkat
et al.:</strong> In their paper, Parakkat et al. introduced Delaunay
Painting, a novel and easy-to-use method for flat-coloring contour
sketches with gaps. It starts by creating a Delaunay triangulation of
the input contours and then proceeds to iteratively fill triangles with
the appropriate colors based on dynamic flow calculations and energy
minimization of contour curves. Additionally, a color diffusion
framework is used to automate the coloring of small internal regions.
The method is robust and enables any coloring strategy regardless of the
order of interactions. It has applications in medical imaging and sketch
segmentation.</p>
<h3 id="sec:III.1.2">Assistive Completion</h3>
<p>The literature also explored building assistive completion tools
<span class="citation" data-cites="sykora_2009 beck_2018">[<a
href="#ref-beck_2018" role="doc-biblioref">20</a>, <a
href="#ref-sykora_2009" role="doc-biblioref">77</a>]</span> for helping
in filling flat color areas, closing gaps in sketches, and more. While
they propose to enhance the natural workflow of digital painting, they
only focus on flat colorization and do not provide the mean of
specifying lighting and textures.</p>
<figure id="fig:core_rel_lazybrush">
<img src="./figures/core_rel_lazybrush.png"
alt="Flat colorization from Sykora et al. [77]. The color intents are loosely defined by the end-user using color strokes. The algorithm can generate quality colorization even with imprecise stroke positioning." />
<figcaption>Figure 25: Flat colorization from Sykora et al. <span
class="citation" data-cites="sykora_2009">[<a href="#ref-sykora_2009"
role="doc-biblioref">77</a>]</span>. The color intents are loosely
defined by the end-user using color strokes. The algorithm can generate
quality colorization even with imprecise stroke
positioning.</figcaption>
</figure>
<p><strong><span class="citation" data-cites="sykora_2009">[<a
href="#ref-sykora_2009" role="doc-biblioref">77</a>]</span> Sykora et
al.:</strong> Sykora et al. introduced LazyBrush, a novel interactive
painting tool that is both simple and flexible. It is designed to
address the needs of professional ink-and-paint illustrators and offers
comparable or even less manual effort than previous custom-tailored
approaches. LazyBrush is not sensitive to the imprecise placement of
color strokes and is incorporated into an optimization framework based
on a Potts energy model with several interesting theoretical properties.
It is demonstrated to be a useful tool in practical scenarios such as
the ink-and-paint production pipeline.</p>
<p><strong><span class="citation" data-cites="beck_2018">[<a
href="#ref-beck_2018" role="doc-biblioref">20</a>]</span> Beck et
al.:</strong> Beck et al. present a novel algorithm for semi-supervised
colorization of line-art images. The algorithm involves two steps: 1) a
geometric analysis of the stroke contours and their closing by
splines/segments, and 2) a colorization step based on the filling of the
corresponding connected components. The algorithm offers a fast and
efficient colorization of line-art images with a similar quality as
previous state-of-the-art algorithms, but with a lower algorithmic
complexity, allowing for more user interactivity.</p>
<h2 id="sec:III.2">Generative Neural Networks</h2>
<p>In this section, we extend our Deep Learning (DL) architecture
toolbox with generative AI architectures such as the Autoencoder (AE)
(see Sec <strong>¿sec:ae?</strong>), the Variational Autoencoder (VAE)
(see Sec <strong>¿sec:vae?</strong>), the Generative Adversarial Network
(GAN) (see Sec <strong>¿sec:gan?</strong>), the Denoising Diffusion
Model (DDM) (see Sec <strong>¿sec:ddm?</strong>), and Large Language
Models (LLM) (see Sec <strong>¿sec:llm?</strong>) with a strong focus on
image generation. Similarly to the previous sections, the Modified
National Institute of Standards and Technology (MNIST) dataset is used
for illustrative purposes.</p>
<p>This section does not only discuss the technical details of those
architectures but also compares them on three criteria, generation
inference speed, generation variance, and generation quality and
complexity.</p>
<h3 id="sec:III.2.1">Autoencoders</h3>
<p>Autoencoders (AE) are part of a family of feedforward NNs for which
the input tensor is the same as the output tensor. They encode
(compress), the input into a low-dimensional code in a latent space, and
then decode (reconstruct) the original input from this compressed
representation (see <a href="#fig:gai_autoencoder">Fig 26</a>). An AE is
built using two network parts, an encoder <span
class="math inline">\(E\)</span>, NN that reduces the input dimension, a
decoder <span class="math inline">\(D\)</span> that recovers the input
<span class="math inline">\(x\)</span> from the reduced tensor <span
class="math inline">\(z\)</span>, and a reconstruction objective. This
architecture can be viewed as a dimensionality reduction technique but
can be used as a generative model. By feeding the decoder <span
class="math inline">\(D\)</span> with arbitrary latent codes <span
class="math inline">\(z\)</span>, one can generate unseen data points
<span class="math inline">\(\hat{x}\)</span> similar to the training
distribution by interpolation. Additional training objectives can be
used to disentangle the latent representation so that the data points
are organized mindfully in the latent space, semantically for
example.</p>
<figure id="fig:gai_autoencoder">
<img src="./figures/core_gai_autoencoder.svg"
alt="+ae architecture. The encoder compresses the input into a latent code that is reconstructed using the decoder. architecture." />
<figcaption>Figure 26: <span class="full">+ae architecture. The encoder
compresses the input into a latent code that is reconstructed using the
decoder.</span> architecture.</figcaption>
</figure>
<p><strong>Properties:</strong> Compared to a traditional compression
method, AEs are tied to their training data. They are trained to learn
data-specific features useful for in-domain compression not for
out-of-domain. An AE trained on MNIST cannot be used for compressing
photos of faces. Such architecture cannot be considered a lossless
compression algorithm. The reconstruction is most of the time degraded.
One strong advantage of using an AE is that they do not require complex
data preparation. They are part of the unsupervised training family,
where labeled data is not needed for training, and in this case,
self-supervised learning where the target output is built synthetically
from the input.</p>
<p><strong>MNIST Digit Image Generation:</strong> Let us consider MNIST
and train a small Autoencoder (AE) to compress handwritten digits to
<span class="math inline">\(32\)</span> latent codes. Our AE is made out
of a small MLP encoder and decoder both with two inner layers with a
hidden dimension of <span class="math inline">\(128\)</span>.</p>
<p>The model is trained on <span class="math inline">\(10\)</span>
epochs with no hyperparameter tuning using the
<code>binar_cross_entropy</code> objective function as the dataset
contains black and white images normalized in the <span
class="math inline">\([0; 1]\)</span> range.</p>
<figure id="fig:gai_autoencoder_history">
<img src="./figures/core_gai_autoencoder_history.svg"
alt="Training history of a small 2-layer Autoencoder (AE). The binary cross entropy loss is shown on the left, a training sample in the middle, and its corresponding reconstruction on the right." />
<figcaption>Figure 27: Training history of a small <span
class="math inline">\(2\)</span>-layer Autoencoder (AE). The binary
cross entropy loss is shown on the left, a training sample in the
middle, and its corresponding reconstruction on the right.</figcaption>
</figure>
<p>The result of the training can be observed in <a
href="#fig:gai_autoencoder_history">Fig 27</a>. Despite little
degradation, our model can reconstruct the handwritten digits from their
latent code. The degradation is minimized by the fact that we are
dealing with a toy dataset. The phenomenon can be observed by reducing
the number of parameters of the network or the size of the latent space.
To reconstruct the images, we first need to get a latent code, either by
encoding an existing image, or by randomly initializing a latent vector
in a reasonable range, and providing it to the decoder.</p>
<figure id="fig:gai_autoencoder_latent">
<img src="./figures/core_gai_autoencoder_latent.svg" style="width:60.0%"
alt="Trained Autoencoder (AE) 2-dimensional latent space visualization. The data points represent the encoded latent code of images from the MNIST dataset and are colored based on their corresponding label (digit). The latent space is not organized in a way that allows us to visually separate these classes." />
<figcaption>Figure 28: Trained Autoencoder (AE) <span
class="math inline">\(2\)</span>-dimensional latent space visualization.
The data points represent the encoded latent code of images from the
MNIST dataset and are colored based on their corresponding label
(digit). The latent space is not organized in a way that allows us to
visually separate these classes.</figcaption>
</figure>
<p>The <span class="math inline">\(2\)</span>-dimensional latent space
can be observed in <a href="#fig:gai_autoencoder_latent">Fig 28</a>. Our
latent space is not organized in a way that we can visually distinguish
between the digit classes. This clearly demonstrates a lack of
structural organization preventing the AE from being used as a generator
by sampling its latent space.</p>
<figure id="fig:gai_autoencoder_latent_sampling">
<img src="./figures/core_gai_autoencoder_latent_sampling.svg"
alt="Trained Autoencoder (AE) 2-dimensional latent space sampling visualization. The decoder is used for generation by sampling the latent space in a grid pattern." />
<figcaption>Figure 29: Trained Autoencoder (AE) <span
class="math inline">\(2\)</span>-dimensional latent space sampling
visualization. The decoder is used for generation by sampling the latent
space in a grid pattern.</figcaption>
</figure>
<h3 id="sec:III.2.2">Variational Autoencoders</h3>
<p>Due to a lack of latent space regularization as shown in the previous
sub-section, AE cannot be used without any hacking to generate, or
produce unseen samples. A vanilla AE does not encode any structure on
the latent space. It is trained only for reconstruction and is thus
subject to high overfitting resulting in a meaningless structural
organization of the latent codes. The Variational Autoencoder (VAE)
architecture <span class="citation" data-cites="kingma_2013">[<a
href="#ref-kingma_2013" role="doc-biblioref">43</a>]</span> is one
answer to this issue. It can be viewed as a special AE hacked by adding
a regularization objective enabling generation by exploring the learned
and structured latent space (see <a href="#fig:gai_vae">Fig 30</a>).</p>
<figure id="fig:gai_vae">
<img src="./figures/core_gai_vae.svg"
alt="+vae architecture. The encoder compresses the input and regresses the latent distribution parameters \mu and \rho from which a latent code is sampled using the reparametrization trick with a surrogate parameter \epsilon sampled from the standard Gaussian distribution and then decoded to recover the input using the decoder. architecture." />
<figcaption>Figure 30: <span class="full">+vae architecture. The encoder
compresses the input and regresses the latent distribution parameters
<span class="math inline">\(\mu\)</span> and <span
class="math inline">\(\rho\)</span> from which a latent code is sampled
using the reparametrization trick with a surrogate parameter <span
class="math inline">\(\epsilon\)</span> sampled from the standard
Gaussian distribution and then decoded to recover the input using the
decoder.</span> architecture.</figcaption>
</figure>
<p><strong>Regularization:</strong> VAEs are topologically similar to
AE. They possess an encoder to compress the input into a latent code,
and a decoder to reconstruct the signal from it. However, instead of
encoding the input as a single point, it encodes it as a distribution in
the latent space. In practice, the distribution used is chosen to be
close to a normal distribution. The encoder is changed to output the
parameters of this distribution, the mean <span
class="math inline">\(\mu\)</span>, and the variance <span
class="math inline">\(\sigma^2\)</span>. <span
class="math inline">\(\sigma^2\)</span> is often replaced by a proxy
<span class="math inline">\(\rho = log(\sigma^2)\)</span> to enforce
positivity and stability. The new inference scheme is changed for <span
class="math inline">\(\hat{x} = D(z)\)</span>, where the latent code
<span class="math inline">\(z \sim \mathcal{N}(E(x)_\mu,
exp(E(x)_\rho))\)</span>.</p>
<p><strong>Probabilistic Formulation:</strong> Let us consider the VAE
as a probabilistic model. <span class="math inline">\(x\)</span>, our
data, is generated from the latent variable <span
class="math inline">\(z\)</span> that cannot be observed. In this
framework, the generation steps are the following: <span
class="math inline">\(z\)</span> is sampled from the prior distribution
<span class="math inline">\(p(z)\)</span>, and <span
class="math inline">\(x\)</span> is sampled from the conditional
likelihood <span class="math inline">\(x \sim p(x | z)\)</span>. In this
setting, the probabilistic decoder is <span class="math inline">\(p(x |
z)\)</span>, and the probabilistic encoder is <span
class="math inline">\(p(z | x)\)</span>. The Bayes theorem allows
expressing a natural relation between the prior <span
class="math inline">\(p(z)\)</span>, the likelihood <span
class="math inline">\(p(x | z)\)</span>, and the posterior <span
class="math inline">\(p(z|x)\)</span>:</p>
<p><span id="eq:vae_bayes"><span class="math display">\[
p(z | x) = \frac{p(x | z)p(z)}{p(x)} = \frac{p(x | z)p(z)}{\int p(x | u)
p(u) du}
\qquad{(7)}\]</span></span></p>
<p>A standard Gaussian distribution is often assumed for the prior <span
class="math inline">\(p(z)\)</span>, and a parametric Gaussian for the
likelihood <span class="math inline">\(p(x | z)\)</span> with its mean
being defined by a deterministic function <span class="math inline">\(f
\in F\)</span> and a positive constant <span class="math inline">\(c
\cdot I\)</span> for the covariance. In this setting:</p>
<p><span id="eq:vae_gaussian"><span class="math display">\[
\begin{aligned}
p(z)     &amp;\sim \mathcal{N}(0, I) \\
p(x | z) &amp;\sim \mathcal{N}(f(z), cI), \; f \in F, \; c &gt; 0
\end{aligned}
\qquad{(8)}\]</span></span></p>
<p>These equations (see Eqns <a href="#eq:vae_bayes">7</a>, <a
href="#eq:vae_gaussian">8</a>) define a classical Bayesian Inference
problem. This problem is however intractable because of the
denominator’s integral <span class="math inline">\(\int p(x | u) p(u)
du\)</span> and thus requires the use of approximation techniques.</p>
<p><strong>Variational Inference</strong>: In statistics, Variational
Inference (VI) is one of the techniques used to approximate complex
distributions. It consists in setting a parametrized distribution
family, in our case Gaussians with its mean and covariance, and
searching for the best approximation of the target distribution in this
family. To search for the best candidate, we use the Kullback-Leibler
Divergence (KL-Divergence) between the approximation and the target and
minimize it with Gradient Descent (GD).</p>
<p>Let us approximate the posterior <span
class="math inline">\(p(z|x)\)</span> using VI with a Gaussian
distribution <span class="math inline">\(q_x(z)\)</span> with a mean
<span class="math inline">\(g(x) \in G\)</span> and covariance <span
class="math inline">\(h(x) \in H\)</span> where <span
class="math inline">\(q_x(z) \sim \mathcal{N}(g(x), h(x))\)</span>. we
can now look for the optimal <span class="math inline">\(g^*\)</span>
and <span class="math inline">\(h^*\)</span> minimizing the
KL-Divergence between the target and the approximation:</p>
<p><span id="eq:vae_objective"><span class="math display">\[
\begin{aligned}
(g^*, h^*) &amp;= \underset{(g, h) \in G \times H}{arg \; min} KL(q_x(z)
|| p(z | x)) \\
           &amp;= \underset{(g, h) \in G \times H}{arg \; min} (E_{z
\sim q_x} \; log \; q_x(z) - E_{z \sim q_x} \; log \; \frac{p(x | z)
p(z)}{p(x)}) \\
           &amp;= \underset{(g, h) \in G \times H}{arg \; min} (E_z \;
log \; q_x(z) - E_z \; log \; p(z) - E_z \; log \; p(x | z) + E_z \; log
\; p(x)) \\
           &amp;= \underset{(g, h) \in G \times H}{arg \; min} (E_z [log
\; p(x | z) - KL(q_x(z) || p(z)]) \\
           &amp;= \underset{(g, h) \in G \times H}{arg \; min} (E_z \;
log \; p(x | z) - KL(q_x(z) || p(z)) \\
           &amp;= \underset{(g, h) \in G \times H}{arg \; min} (E_z
[-\frac{||x - f(z)||^2}{2c}] - KL(q_x(z) || p(z)) \\
\end{aligned}
\qquad{(9)}\]</span></span></p>
<p>This rewrite of the objective equations demonstrates a natural
tradeoff between the data confidence <span class="math inline">\(E_z
[-\frac{||x - f(z)||^2}{2c}]\)</span> and the prior confidence <span
class="math inline">\(KL(q_x(z) || p(z))\)</span>. The first term
describes a reconstruction loss where the decoder parametrized by the
function <span class="math inline">\(f \in F\)</span> has to recover the
input <span class="math inline">\(x\)</span> from the latent code <span
class="math inline">\(z\)</span>, and the second term a regularization
objective between <span class="math inline">\(q_x(z)\)</span> and the
prior <span class="math inline">\(p(z)\)</span> which is gaussian. We
can view the constant <span class="math inline">\(c\)</span> as a
strength parameter that can adjust how we favor the regularization.</p>
<p><strong>Reparametrization Trick:</strong> The VAE architecture is
trained to find the parameters of the functions <span
class="math inline">\(f\)</span>, <span
class="math inline">\(g\)</span>, and <span
class="math inline">\(h\)</span> by minimizing the VI objective (see <a
href="#eq:vae_objective">Eq 9</a>). The encoder is charged to output two
vectors, one for representing <span class="math inline">\(g(x)\)</span>
the mean, in the case of a Gaussian distribution <span
class="math inline">\(\mu\)</span>, and the other representing the
variance of the distribution <span class="math inline">\(h(x)\)</span>,
<span class="math inline">\(\rho = log(\sigma^2)\)</span>. The latent
code <span class="math inline">\(z\)</span> is then sampled from the
Gaussian distribution <span class="math inline">\(z \sim
\mathcal{N}(\mu, \sigma)\)</span> and finally decoded to reconstruct the
original input <span class="math inline">\(x\)</span>.</p>
<p>There is however a catch. The sampling process is stochastic and thus
not differentiable. And we know that a NN needs to be differentiable to
be optimized using SGD. To solve this problem, Kingma et al. <span
class="citation" data-cites="kingma_2013">[<a href="#ref-kingma_2013"
role="doc-biblioref">43</a>]</span> propose to use what they call a
reparametrization trick. It consists in sampling a surrogate standard
Gaussian distribution <span class="math inline">\(\epsilon \sim
\mathcal{N}(0, I)\)</span> and scaling it by the output of the learned
encoder <span class="math inline">\(\mu\)</span> and <span
class="math inline">\(\sigma^2\)</span>. This the process becomes:</p>
<p><span id="eq:vae_reparametrization"><span class="math display">\[
\begin{aligned}
E(x)    &amp;= (\mu, \rho) \\
\hat{x} &amp;= D(\mu + \epsilon \; exp(\rho)), \; \epsilon \sim N(O, I)
\end{aligned}
\qquad{(10)}\]</span></span></p>
<p>Performing the latent sampling using the reparametrization trick (see
<a href="#eq:vae_reparametrization">Eq 10</a>) conserves the gradient
flow. The VAE can thus be trained to learn a structured latent space
that can be used to interpolate latent codes and decode them into
samples similar to the training distribution.</p>
<p><strong>MNIST Digit Image Generation:</strong> Let us reconsider our
toy example with training a VAE on the MNIST handwritten digit dataset.
There is almost nothing change in comparison to the AE.</p>
<p>The model is trained to minimize the objective functions. Satisfying
both the reconstruction loss and the KL-Divergence regularization is
harder than the task of the vanilla AE. Intuitively this has to result
in a structured latent space at the cost of a small reconstruction
quality degradation. The training history and reconstruction
capabilities are shown in <a href="#fig:gai_vae_history">Fig 31</a>.</p>
<figure id="fig:gai_vae_history">
<img src="./figures/core_gai_vae_history.svg"
alt="Training history of a small Variational Autoencoder (VAE). The combination of the binary cross entropy loss and the KL-Divergence regularization is shown on the left, a training sample in the middle, and its corresponding reconstruction on the right." />
<figcaption>Figure 31: Training history of a small Variational
Autoencoder (VAE). The combination of the binary cross entropy loss and
the KL-Divergence regularization is shown on the left, a training sample
in the middle, and its corresponding reconstruction on the
right.</figcaption>
</figure>
<p>As expected, the latent space (see <a
href="#fig:gai_vae_latent">Fig 32</a>) presents structural organization.
The specific digit classes are visible and separable.</p>
<figure id="fig:gai_vae_latent">
<img src="./figures/core_gai_vae_latent.svg" style="width:60.0%"
alt="Trained Variational Autoencoder (VAE) 2-dimensional latent space visualization. The data points represent the encoded latent code of images from the MNIST dataset and are colored based on their corresponding label (digit). The latent space is organized in a way that allows us to visually separate these classes." />
<figcaption>Figure 32: Trained Variational Autoencoder (VAE) <span
class="math inline">\(2\)</span>-dimensional latent space visualization.
The data points represent the encoded latent code of images from the
MNIST dataset and are colored based on their corresponding label
(digit). The latent space is organized in a way that allows us to
visually separate these classes.</figcaption>
</figure>
<p>The structure of the trained VAE latent space allows the generation
of new data points by sampling latent codes and decoding them using the
trained decoder (see <a
href="#fig:gai_vae_latent_sampling">Fig 33</a>).</p>
<figure id="fig:gai_vae_latent_sampling">
<img src="./figures/core_gai_vae_latent_sampling.svg"
alt="Trained Variational Autoencoder (VAE) 2-dimensional latent space sampling visualization. The decoder is used for generation by sampling the latent space in a grid pattern." />
<figcaption>Figure 33: Trained Variational Autoencoder (VAE) <span
class="math inline">\(2\)</span>-dimensional latent space sampling
visualization. The decoder is used for generation by sampling the latent
space in a grid pattern.</figcaption>
</figure>
<h3 id="sec:III.2.3">Generative Adversarial Networks</h3>
<p>While VAE allow learning structured latent spaces ready for sampling
and generation, they suffer from quality degradation. The Generative
Adversarial Network (GAN) architecture, introduced by Ian Goodfellow et
al. <span class="citation" data-cites="goodfellow_2014">[<a
href="#ref-goodfellow_2014" role="doc-biblioref">26</a>]</span>, is one
answer to this problem. GANs can generate high-quality images in
real-time settings.</p>
<p><strong>Vanilla GAN:</strong> In its vanilla formulation, a GAN
consists of a generator <span class="math inline">\(G\)</span> trained
to produce images <span class="math inline">\(G(z)\)</span> similar to
the training distribution given a latent code <span
class="math inline">\(z\)</span> which is then fed to a discriminator
<span class="math inline">\(D\)</span> trained to differentiate fake
images (generated images) from true images <span
class="math inline">\(x\)</span>. The two networks are jointly trained
in an end-to-end fashion to optimize a Min-Max objective (see <a
href="#eq:gan_minmax">Eq 11</a>) that can be split into two objectives,
one for the generator, and another for the discriminator (see <a
href="#eq:gan_minmax_split">Eq 12</a>).</p>
<p><span id="eq:gan_minmax"><span class="math display">\[
\underset{D}{min} \; \underset{G}{max} \; E_x \; log \; D(x) + E_z \;
log(1 - D(G(z))) \\
\qquad{(11)}\]</span></span></p>
<p><span id="eq:gan_minmax_split"><span class="math display">\[
\begin{aligned}
\frac{1}{n} \sum_{i=1}^{n} \; &amp;log \; D(x_i) + log(1 - D(G(z_i))) \\
\frac{1}{n} \sum_{i=1}^{n} \; &amp;log \; D(G(z_i))
\end{aligned}
\qquad{(12)}\]</span></span></p>
<p><strong>Vanishing Gradients and Mode Collapse</strong>: In practice,
the vanilla formulation is however unstable. The generator training
often saturates if it cannot keep up with the discriminator training
which is in most cases easier to satisfy. It suffers from vanishing
gradients where the loss signal becomes too small and gradients do not
propagate to layers resulting in the early stop of the generator
training. It is also subject to mode collapse where the generator finds
a simple solution fooling the discriminator and failing at generating
diverse enough outputs. Solutions such as the hinge loss <span
class="citation" data-cites="lim_2017">[<a href="#ref-lim_2017"
role="doc-biblioref">46</a>]</span>, the Wasserstein distance <span
class="citation" data-cites="arjovsky_2017">[<a
href="#ref-arjovsky_2017" role="doc-biblioref">3</a>]</span>, gradient
penalty <span class="citation" data-cites="ishaan_2017">[<a
href="#ref-ishaan_2017" role="doc-biblioref">28</a>]</span>, the use of
batch <span class="citation" data-cites="ioffe_2015">[<a
href="#ref-ioffe_2015" role="doc-biblioref">38</a>]</span> and spectral
<span class="citation" data-cites="miyato_2018">[<a
href="#ref-miyato_2018" role="doc-biblioref">53</a>]</span>
normalization can be found in the literature.</p>
<p><strong>Wasserstein GAN:</strong> The most famous improvement of the
vanilla GAN is the Wasserstein Generative Adversarial Network (WGAN)
<span class="citation" data-cites="arjovsky_2017">[<a
href="#ref-arjovsky_2017" role="doc-biblioref">3</a>]</span>. It both
resolves the mode collapse and limits the vanishing gradients issues.
The last activation of the discriminator is swapped from a sigmoid to a
linear activation. This small change turns the discriminator into a
critic network in charge of scoring the quality (fidelity to the
original distribution) of input images. The new simplified objectives
are shown in <a href="#eq:gan_wasserstein">Eq 13</a> and are often
combined with gradient clipping to satisfy a Lipschitz constraint.</p>
<p><span id="eq:gan_wasserstein"><span class="math display">\[
\begin{aligned}
\frac{1}{n} \sum_{i=1}^{n} \; &amp; D(x_i) - D(G(z_i)) \\
\frac{1}{n} \sum_{i=1}^{n} \; &amp; D(G(z_i))
\end{aligned}
\qquad{(13)}\]</span></span></p>
<p><strong>Gradient Penalty:</strong> In their contribution, Gulrajani
et al. <span class="citation" data-cites="gulrajani_2017">[<a
href="#ref-gulrajani_2017" role="doc-biblioref">27</a>]</span> propose
to replace gradient clipping with gradient penalty to enforce a
constraint on the critic such that its gradients with respect to the
inputs are unit vectors. The critic loss is thus augmented with an
additional term (see <a href="#eq:gan_gp">Eq 14</a>) where <span
class="math inline">\(\hat{x}\)</span> is sampled from a linear
interpolation between real and fake samples to satisfy the critic’s
Lipschitz constraint.</p>
<p><span id="eq:gan_gp"><span class="math display">\[
\lambda \; E_{\hat{x}} \; (||\nabla_{\hat{x}}D(\hat{x})||_2 - 1)^2
\qquad{(14)}\]</span></span></p>
<p><strong>MNIST Digit Image Generation:</strong> Let us reconsider the
MNIST dataset and train a GAN to enable qualitative generation by
providing a latent code to the generator. We first need to define the
generator <span class="math inline">\(G\)</span> model and the critic
network <span class="math inline">\(C\)</span>. In this case, we keep
the same architecture for both. They are <span
class="math inline">\(3\)</span>-layer NNs with a sigmoid activation for
the generator and a linear activation for the critic. For the sake of
the example and comparability with the previous generative architecture,
we choose a latent dimension of <span
class="math inline">\(2\)</span>.</p>
<p>Even though the formulation of the Minmax objective is common, both
networks require their own optimizer in practice. We thus define the
optimizer for the generator and the critic. Their hyperparameters can be
different and tweaked to further optimize the training. For the sake of
simplicity, we keep the default parameters in this toy example.</p>
<p>One generator training step consists in sampling a latent code <span
class="math inline">\(z\)</span>, using it to generate a handwritten
digit fake image <span class="math inline">\(G(z)\)</span> and computing
the average generator hinge/Wasserstein loss <span
class="math inline">\(ReLU(1 - C(G(z)))\)</span>. This loss is then used
to backpropagate gradients with respect to the generator’s parameters.
The generator’s weights are finally updated using the Adam optimizer
policy.</p>
<p>For stability purposes and to satisfy the <span
class="math inline">\(1\)</span>-Lipschitz constraint of the critic
network we implement a <code>grad_penalty</code> helper for computing
the gradient penalty term. It consists of sampling interpolations
between fake and real samples <span class="math inline">\(t = \alpha
x_{real} + (1 - \alpha) x_{fake}\)</span> with <span
class="math inline">\(\alpha \sim \mathcal{U}(0, 1)\)</span>, scoring
them using the critic network <span class="math inline">\(C\)</span>. We
then compute the gradients with respect to the critic’s parameters and
optimize them to be close to <span class="math inline">\(1\)</span>,
<span class="math inline">\(\; E_{\hat{x}} \;
(||\nabla_{\hat{x}}D(\hat{x})||_2 - 1)^2\)</span>.</p>
<p>One critic training step consists in sampling a latent code <span
class="math inline">\(z\)</span>, using it to generate a fake image
<span class="math inline">\(G(z)\)</span> and computing the three
average critic’s loss terms <span class="math inline">\(ReLU(1 +
C(G(z)))\)</span>, $ReLU(1 - C(x)), and <span
class="math inline">\(\lambda \nabla_{gp}\)</span> with <span
class="math inline">\(\lambda = 10\)</span> to minimize. The losses are
then backward and the resulting gradients are used to update the
critic’s weights using the Adam policy.</p>
<p>The overall training loop consists in alternating between generator
training steps and critic training steps until convergence. It is common
to train the critic for more cycles, especially at the beginning of
training where the training signal for the generator is not constructive
enough. In this toy example, we optimize the critic five times more than
the generator.</p>
<p>The training history monitoring the generator’s and critic’s loss
terms are shown in <a href="#fig:gai_gan_history">Fig 34</a>. GAN are in
practice hard to train and monitoring each term independently allows us
to diagnose mode collapse and vanishing gradients when the gradient
distributions are checked.</p>
<figure id="fig:gai_gan_history">
<img src="./figures/core_gai_gan_history.svg"
alt="Training history of a small Generative Adversarial Network (GAN). The objective terms are plotted separately and follow a hinge loss and Wasserstein objective. The loss for the generator is shown in orange and the losses for the critic in blue." />
<figcaption>Figure 34: Training history of a small Generative
Adversarial Network (GAN). The objective terms are plotted separately
and follow a hinge loss and Wasserstein objective. The loss for the
generator is shown in orange and the losses for the critic in
blue.</figcaption>
</figure>
<p>Overall, the GAN architecture is highly efficient at generating new
samples giving latent codes. The quality of the generation is superior
to those of VAE at the cost of a small loss in variation (see <a
href="#fig:gai_gan_latent_sampling">Fig 35</a>).</p>
<figure id="fig:gai_gan_latent_sampling">
<img src="./figures/core_gai_gan_latent_sampling.svg"
alt="Trained Generative Adversarial Network (GAN) 2-dimensional latent space sampling visualization. The generator is used for generation by sampling the latent space in a grid pattern." />
<figcaption>Figure 35: Trained Generative Adversarial Network (GAN)
<span class="math inline">\(2\)</span>-dimensional latent space sampling
visualization. The generator is used for generation by sampling the
latent space in a grid pattern.</figcaption>
</figure>
<h3 id="sec:III.2.4">Denoising Diffusion Models</h3>
<p>Generative Adversarial Network (GAN) architectures have been the
framework of choice when approaching image generation tasks for
real-time or near real-time applications with perceptual quality needs.
However, a more recent proposal named Denoising Diffusion Model (DDM)
<span class="citation" data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">35</a>]</span> is currently challenging this
position. They allow for better quality generation and free user-guided
controls such as in-paint, out-painting, super-resolution, and more at
the cost of inference time, at least concerning its vanilla
formulation.</p>
<p>The DDM architecture is inspired by non-equilibrium thermodynamics
and consists of a Markov chain on diffusion steps slowly adding Gaussian
noise to the data. The task is then to learn the reverse operation to
reconstruct the original data from noise. In this particular framework,
the latent code is as big as the input tensor and reversed using a fixed
procedure.</p>
<p><strong>Forward Diffusion:</strong> Let us consider a data point
sampled from real data distribution <span class="math inline">\(x_0 \sim
q(x)\)</span>. The forward diffusion process consists of adding small
and successive Gaussian noise to the initial data sample during <span
class="math inline">\(T\)</span> steps. The chained noisy
transformations produce <span class="math inline">\(x_1, \dots,
x_T\)</span> samples (see <a href="#eq:ddm_froward">Eq 15</a>). The step
size is given by a variance <span
class="math inline">\(\beta\)</span>-scheduler <span
class="math inline">\({\beta \in [0; 1]}_{t=1}^{T}\)</span>.</p>
<p><span id="eq:ddm_froward"><span class="math display">\[
\begin{aligned}
q(x_t | x_{t - 1}) = \mathcal{N}(x_t; \sqrt{1 -  \beta_t} x_{t - 1},
\beta_t I) \\
q(x_{1:T} | x_0) = \prod_{t = 1}^{T} q(x_t | x_{t - 1})
\end{aligned}
\qquad{(15)}\]</span></span></p>
<p>By adding noise to the input data in small enough steps, when <span
class="math inline">\(T \rightarrow +\infty\)</span>, <span
class="math inline">\(x_T\)</span> converge towards an isotropic
Gaussian distribution. One feature of the forward diffusion process is
that <span class="math inline">\(x\)</span> can be computed in a closed
form making use of the reparametrization trick introduced previously
(see Sec <strong>¿sec:vae?</strong>). Defining <span
class="math inline">\(\alpha_t = 1 - \beta_t\)</span> and <span
class="math inline">\(\bar{\alpha_t} = \prod_{i=1}^{t}
\alpha_i\)</span>,</p>
<p><span id="eq:ddm_froward_closed"><span class="math display">\[
\begin{aligned}
x_t &amp;= \sqrt{\alpha_t} x_{t - 1} + \sqrt{1 - \alpha_t}
\epsilon_{t-1} \\
    &amp;= \sqrt{\alpha_t \alpha_{t - 1}} x_{t - 2} + \sqrt{1 - \alpha_t
\alpha_{t - 1}} \epsilon_{t-2} \\
    &amp;= \dots \\
    &amp;= \sqrt{\bar{\alpha_t}} x_{0} + \sqrt{1 - \bar{\alpha_t}}
\epsilon_0
\end{aligned}
\qquad{(16)}\]</span></span></p>
<p>where <span class="math inline">\(\epsilon_t \sim \mathcal{N}(0,
I)\)</span> and <span class="math inline">\(\epsilon_{t-2}\)</span>
results from merging two Gaussian distributions, <span
class="math inline">\(\mathcal{N}(0, \sigma_1^2 I) + \mathcal{N}(0,
\sigma_2^2 I) = \mathcal{N}(0, (\sigma_1^2 + \sigma_2^2)
I)\)</span>.</p>
<p>To summarize, <span class="math inline">\(x_t\)</span> can be sampled
as follow:</p>
<p><span id="eq:ddm_froward_xt"><span class="math display">\[
x_t \sim q(x_t | x_0) = \mathcal{N}(x_T; \sqrt{\bar{\alpha_t}} x_0, (1 -
\bar{\alpha}_t) I)
\qquad{(17)}\]</span></span></p>
<p><strong>Beta Schedule:</strong> <span
class="math inline">\(\beta_t\)</span>, the variance parameter can be
fixed to a constant or chosen using a schedule over <span
class="math inline">\(T\)</span> timesteps (see <a
href="#fig:gai_ddpm_forward">Fig 36</a>). In the original paper <span
class="citation" data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">35</a>]</span> and follow-up contribution <span
class="citation" data-cites="nichol_2021">[<a href="#ref-nichol_2021"
role="doc-biblioref">56</a>]</span>, the authors propose a linear (<span
class="math inline">\(\beta_1=1e^{-4}\)</span>, <span
class="math inline">\(\beta_T=0.02\)</span>), a quadratic, and a cosine
schedule. Their experiments show that the cosine schedule results are
better.</p>
<figure id="fig:gai_ddpm_forward">
<img src="./figures/core_gai_ddpm_forward.svg"
alt="Visualization of a Denoising Diffusion Probabilistic Model (DDPM) forward diffusion with 1,000 timesteps applied to a handwritten image of a digit extracted from the MNIST training dataset." />
<figcaption>Figure 36: Visualization of a Denoising Diffusion
Probabilistic Model (DDPM) forward diffusion with <span
class="math inline">\(1,000\)</span> timesteps applied to a handwritten
image of a digit extracted from the MNIST training dataset.</figcaption>
</figure>
<p><strong>Backward Diffusion:</strong> The backward diffusion process,
also called reverse diffusion, consists in learning the reverse mapping
<span class="math inline">\(q(x_{t-1} | x_t)\)</span>. By taking into
account that with enough steps <span class="math inline">\(T \rightarrow
+\infty\)</span>, the latent variable <span
class="math inline">\(x_T\)</span> follows an isotropic Gaussian
distribution, <span class="math inline">\(x_T\)</span> can be sampled
from <span class="math inline">\(\mathcal{0, I}\)</span> and <span
class="math inline">\(x_0\)</span> reconstructed by successively
applying this process resulting in <span
class="math inline">\(q(x_0)\)</span>, a novel data sample from the
training data distribution.</p>
<p>The reverse transformation <span class="math inline">\(q(x_{t-1} |
x_t)\)</span> is however intractable. It would require sampling the
entire data distribution. Similarly to the VAE, <span
class="math inline">\(q(x_{t-1} | x_t)\)</span> is approximated using a
parametrized model <span class="math inline">\(p_\theta\)</span>, in our
case a NN. For small enough steps, <span
class="math inline">\(p_\theta\)</span> can be chosen to be a Gaussian
distribution whose mean <span class="math inline">\(\mu_\theta\)</span>
and variance <span class="math inline">\(\Sigma_\theta\)</span> need to
be parameterized.</p>
<p><span id="eq:ddm_backward_param"><span class="math display">\[
\begin{aligned}
p_\theta(x_{t - 1} | x_t) = \mathcal{N}(x_{t - 1}; \mu_\theta(x_t, t),
\Sigma_\theta(x_t, t)) \\
p_\theta(x_{0:T}) = p_\theta(x_T) \prod_{t = 1}^{T} p_\theta(x_{t - 1} |
x_t)
\end{aligned}
\qquad{(18)}\]</span></span></p>
<p>We can then optimize the negative log-likelihood of the training
data. After a series of arrangements and simplifications, see the
original paper <span class="citation" data-cites="ho_2020">[<a
href="#ref-ho_2020" role="doc-biblioref">35</a>]</span> for full
derivation, the objective can be written as follow:</p>
<p><span id="eq:ddm_backward_elbo"><span class="math display">\[
\begin{aligned}
log \; p(x) &amp;\geq L_0 - L_T - \sum_{t=2}^{T} L_{t-1} \\
L_0 &amp;= E_{q(x_1 | x_0)} log \; p_\theta(x_0 | x_1) \\
L_T &amp;= D_{KL}(q(x_T | x_0) || p(x_T)) \\
L_t &amp;= E_{q(x_t | x_0)} \; D_{KL}(q(x_{t- 1 } | x_t, x_0) ||
p_\theta(x_{t - 1} | x_t))
\end{aligned}
\qquad{(19)}\]</span></span></p>
<p>where <span class="math inline">\(L_0\)</span> can be seen as a
reconstruction term, <span class="math inline">\(L_T\)</span> as a
similarity between <span class="math inline">\(x_T\)</span>’s
distribution and the standard Gaussian prior, and <span
class="math inline">\(L_t\)</span> the difference between the target
noise step and its estimation. It can be demonstrated that <span
class="math inline">\(q(x_{t - 1} | x_t)\)</span> can be made tractable
by conditioning it on <span class="math inline">\(x_0\)</span>, <span
class="math inline">\(q(x_{t - 1} | x_t, x_0)\)</span>. In this
setting:</p>
<p><span id="eq:ddm_backward_conditionned"><span class="math display">\[
\begin{aligned}
q(x_{t - 1} | x_t, x_0) &amp;= \mathcal{N}(x_{t - 1}; \tilde{\mu}(x_t,
x_0), \tilde{\beta_t} I) \\
\tilde{\beta_t} &amp;= [(1 - \bar{\alpha}_{t - 1}) \beta_t] / (1 -
\bar{\alpha}_t) \\
\tilde{\mu}(x_t, x_0) &amp;= [(\sqrt{\bar{\alpha}_{t - 1}} \beta_t) x_0
+ (\sqrt{\bar{\alpha}_t} (1 - \bar{\alpha}_{t - 1})) x_t] / (1 -
\bar{\alpha}_t)
\end{aligned}
\qquad{(20)}\]</span></span></p>
<p>Using the reparametrization trick in <a
href="#eq:ddm_froward_closed">Eq 16</a>, we can express <span
class="math inline">\(x_0\)</span> using <span
class="math inline">\(x_t\)</span> and <span
class="math inline">\(\epsilon \sim \mathcal{N}(0, I)\)</span>:</p>
<p><span id="eq:ddm_backward_x0"><span class="math display">\[
x_0 = (x_t - \sqrt{1 - \bar{\alpha}}_t \epsilon) / \sqrt{\bar{\alpha}_t}
\qquad{(21)}\]</span></span></p>
<p>Injecting <a href="#eq:ddm_backward_x0">Eq 21</a> in <a
href="#eq:ddm_backward_conditionned">Eq 20</a> allows us to express the
target mean <span class="math inline">\(\tilde{\mu}_t\)</span> to only
depend on <span class="math inline">\(x_t\)</span> and trained a NN to
approximate the noise <span class="math inline">\(\epsilon_\theta(x_t,
t)\)</span>:</p>
<p><span id="eq:ddm_backward_mutilde"><span class="math display">\[
\begin{aligned}
\tilde{\mu}_t(x_t)        &amp;= (x_t - \frac{\beta_t}{\sqrt{1 -
\bar{\alpha}}_t} \epsilon) / \sqrt{\alpha_t} \\
\tilde{\mu_\theta}_t(x_t) &amp;= (x_t - \frac{\beta_t}{\sqrt{1 -
\bar{\alpha}}_t} \epsilon_\theta(x_t, t)) / \sqrt{\alpha_t}
\end{aligned}
\qquad{(22)}\]</span></span></p>
<p>The <span class="math inline">\(L_t\)</span> denoising loss can thus
be expressed as follow:</p>
<p><span id="eq:ddm_backward_lt_rewrite"><span class="math display">\[
\begin{aligned}
L_t &amp;= E_{x_0, t, \epsilon} [\frac{1}{2 ||\Sigma_\theta(x_t,
t)||_2^2} ||\tilde{\mu}_t - \mu_\theta(x_t, t)||_2^2] \\
    &amp;= E_{x_0, t, \epsilon} [\frac{\beta_t^2}{2 \alpha_t (1 -
\bar{\alpha}_t) ||\Sigma_\theta(x_t, t)||_2^2} ||\epsilon_t -
\epsilon_\theta(x_t, t)||_2^2]
\end{aligned}
\qquad{(23)}\]</span></span></p>
<p>This tells us that the objective can be simplified to learning a
noise model to predict the noise <span
class="math inline">\(\epsilon\)</span> at timestep <span
class="math inline">\(t\)</span>. Ho et al. <span class="citation"
data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">35</a>]</span> propose to further simplify the
objective function <span class="math inline">\(L_t^{simple}\)</span> by
removing the weighting term and making the variance fixed.</p>
<p><span id="eq:ddm_backward_lt_simple"><span class="math display">\[
\begin{aligned}
L_t^{simple} &amp;= E_{x_0, t, \epsilon} [||\epsilon_t -
\epsilon_\theta(x_t, t)||_2^2] \\
x_t          &amp;= \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 -
\bar{\alpha}_t} \epsilon
\end{aligned}
\qquad{(24)}\]</span></span></p>
<p>Finally, when the model <span
class="math inline">\(\epsilon_\theta\)</span> is trained, we just need
to iteratively denoise the latent code <span class="math inline">\(z
\sim \mathcal{N}(0, I)\)</span> using:</p>
<p><span id="eq:ddm_backward_denoise"><span class="math display">\[
x_{t - 1} = \frac{1}{\sqrt{\alpha_t}} (x_t - \frac{1 - \alpha_t}{\sqrt{1
- \bar{\alpha}_t}} \epsilon\theta(x_t, t))
\qquad{(25)}\]</span></span></p>
<p><strong>MNIST Digit Image Generation:</strong> For the last time, let
us reconsider the MNIST handwritten digit dataset and train a Denoising
Diffusion Probabilistic Model (DDPM) to generate new data samples from
Gaussian distributed latent codes <span
class="math inline">\(z\)</span>. For this example, the data samples
must be normalized in the <span class="math inline">\([-1; 1]\)</span>
range.</p>
<p>A simple MLP does not perform well enough using this approach. Thus,
we implement a simple CNN. The network conditioning on the timestep
<span class="math inline">\(t\)</span> is omitted for the sake of
simplicity. It is however required for a faithful implementation using a
more common U-net architecture <span class="citation"
data-cites="ronneberger_2015">[<a href="#ref-ronneberger_2015"
role="doc-biblioref">67</a>]</span>.</p>
<p>The training procedure is straightforward and consists of sampling
<span class="math inline">\(t\)</span> and <span
class="math inline">\(\epsilon\)</span> values, and computing the
corresponding <span class="math inline">\(x_t\)</span> using forward
diffusion starting from the original images <span
class="math inline">\(x_0\)</span>. This step does not require any
gradient to be computed and stored. Next, the noise model is used to
predict the noise applied <span class="math inline">\(\epsilon\)</span>
and used to compute the objective function which is finally used for
backpropagation and SGD using the Adam policy. The training history is
shown in <a href="#fig:gai_ddpm_history">Fig 37</a>.</p>
<figure id="fig:gai_ddpm_history">
<img src="./figures/core_gai_ddpm_history.svg"
alt="Denoising Diffusion Probabilistic Model (DDPM) training history. The noise model is trained to generate the noise value used for a DDM reverse process and generate handwritten digits resembling those of the MNIST dataset." />
<figcaption>Figure 37: Denoising Diffusion Probabilistic Model (DDPM)
training history. The noise model is trained to generate the noise value
used for a DDM reverse process and generate handwritten digits
resembling those of the MNIST dataset.</figcaption>
</figure>
<p>The sampling procedure consists of applying reverse diffusion steps
in sequence going from a <span class="math inline">\(t = T\)</span>
timestep to <span class="math inline">\(t = 0\)</span>. The trained
noise model is used to infer the noise at each step of the process. The
final image is clipped in the <span class="math inline">\([-1;
1]\)</span> range and unnormalized <span class="math inline">\([0;
1]\)</span>. Generated samples are shown in Figs <a
href="#fig:gai_ddpm_latent_sampling">38</a>, <a
href="#fig:gai_ddpm_latent_samples">39</a>.</p>
<figure id="fig:gai_ddpm_latent_sampling">
<img src="./figures/core_gai_ddpm_latent_sampling.svg"
alt="Visualization of a sequence of Denoising Diffusion Probabilistic Model (DDPM) reverse diffusion steps applied to a Gaussian distributed latent code and a trained noise model. The result is a new data sample resembling the MNIST training data distribution" />
<figcaption>Figure 38: Visualization of a sequence of Denoising
Diffusion Probabilistic Model (DDPM) reverse diffusion steps applied to
a Gaussian distributed latent code and a trained noise model. The result
is a new data sample resembling the MNIST training data
distribution</figcaption>
</figure>
<figure id="fig:gai_ddpm_latent_samples">
<img src="./figures/core_gai_ddpm_latent_samples.svg"
alt="Selection of generated samples using a trained Denoising Diffusion Probabilistic Model (DDPM) noise model. The initial latent codes are reversed for 1,000 steps." />
<figcaption>Figure 39: Selection of generated samples using a trained
Denoising Diffusion Probabilistic Model (DDPM) noise model. The initial
latent codes are reversed for <span class="math inline">\(1,000\)</span>
steps.</figcaption>
</figure>
<h3 id="sec:III.2.5">Conditional Generation</h3>
<p>The Deep Learning community has come up with a variety of NN
architectures specialized in new sample generation such as the VAE <span
class="citation" data-cites="kingma_2013">[<a href="#ref-kingma_2013"
role="doc-biblioref">43</a>]</span>, the GAN <span class="citation"
data-cites="goodfellow_2014">[<a href="#ref-goodfellow_2014"
role="doc-biblioref">26</a>]</span>, and DDM <span class="citation"
data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">35</a>]</span>. When trained on a specific dataset,
they learn to reproduce the dataset distribution and generate unseen
data. However, their vanilla formulations do not allow the final user to
naturally control the output, let’s say pick the digit to generate by a
GAN trained on MNIST.</p>
<p><strong>Class Conditioning:</strong> To solve this problem, the
literature demonstrates that those same models can be conditioned on
additional inputs in several ways. The same GAN trained on MNIST can be
conditioned on the digit class for both its generator and critic
network. This enables the end user to choose a digit and generate
variations of this digit. In this particular case, injecting the one-hot
class vector to the latent code by concatenation is enough (see <a
href="#fig:gai_gan_class_cond">Fig 40</a>).</p>
<figure id="fig:gai_gan_class_cond">
<img src="./figures/core_gai_gan_class_cond.svg"
alt="Visualization of samples generated by a Generative Adversarial Network (GAN) conditioned on class digits. The images are arranged in order of the conditional digit and sampled using a uniform grid in latent space in the [-1; 1] \times [-1; 1] range." />
<figcaption>Figure 40: Visualization of samples generated by a
Generative Adversarial Network (GAN) conditioned on class digits. The
images are arranged in order of the conditional digit and sampled using
a uniform grid in latent space in the <span class="math inline">\([-1;
1] \times [-1; 1]\)</span> range.</figcaption>
</figure>
<p><strong>Semantic Conditioning:</strong> Using similar tricks semantic
maps, masks, and features can be used to condition the network to
generate images with additional controls for the end user. A gan can for
example be trained to generate Fashion MNIST clothes given a silhouette
mask. This mask can be generated from training data and injected in the
generator and critic networks (see <a
href="#fig:gai_gan_bbox_cond">Fig 41</a>). The example shown in <a
href="#fig:gai_gan_bbox_cond">Fig 41</a> illustrates this type of
conditioning with a naive and simple approach where the mask is
multiplied to the <span class="math inline">\(2\)</span>-dimensional
latent code and transformed via successive convolutional layers for the
generator and concatenated to the input image given to the critic
network.</p>
<figure id="fig:gai_gan_bbox_cond">
<img src="./figures/core_gai_gan_bbox_cond.svg"
alt="Generative Adversarial Network (GAN) trained on the Fashion MNIST dataset using silhouhette mask conditioning. Given a latent code and a silhouette mask, the generator can create fashion clothes images." />
<figcaption>Figure 41: Generative Adversarial Network (GAN) trained on
the Fashion MNIST dataset using silhouhette mask conditioning. Given a
latent code and a silhouette mask, the generator can create fashion
clothes images.</figcaption>
</figure>
<p>Alternative semantic conditioning can be used such as feature vectors
describing aspects of the data distribution, multiclass segmentation
masks to control the positioning and aspect of the generated output,
tags and more.</p>
<p><strong>Natural Language Prompts:</strong> Recently, natural language
has become one of the defacto approaches for conditioning generative
networks. Combine with the advances in language modeling with Large
Language Models (LLM) <span class="citation"
data-cites="devlin_2018 brown_2020 openai_2023">[<a
href="#ref-brown_2020" role="doc-biblioref">8</a>, <a
href="#ref-openai_2023" role="doc-biblioref">11</a>, <a
href="#ref-devlin_2018" role="doc-biblioref">17</a>]</span> and
multimodal embedding alignment [clip], GANs and DDMs <span
class="citation" data-cites="ramesh_2022 rombach_2021">[<a
href="#ref-ramesh_2022" role="doc-biblioref">65</a>, <a
href="#ref-rombach_2021" role="doc-biblioref">66</a>]</span> are trained
to follow natural language instructions called prompts. The community is
currently still learning how to engineer those prompts to enable
fine-grained control of the generated outputs.</p>
<h2 id="sec:III.3">Deep Learning Approaches to Automatic Lineart
Colorization</h2>
<h3 id="sec:III.3.1">Automatic Colorization</h3>
<p>The first category of Deep Learning (DL) techniques applied to the
task of automatic colorization <span class="citation"
data-cites="iizuka_2016 liu_2017 frans_2017 yoo_2019 su_2020">[<a
href="#ref-frans_2017" role="doc-biblioref">21</a>, <a
href="#ref-iizuka_2016" role="doc-biblioref">37</a>, <a
href="#ref-liu_2017" role="doc-biblioref">48</a>, <a href="#ref-su_2020"
role="doc-biblioref">76</a>, <a href="#ref-yoo_2019"
role="doc-biblioref">84</a>]</span> does not propose any user
conditioning input. They instead provide an automatic method for the
task resulting in common sense coloring learned from their respective
training datasets. While initial work and improvements focused on
grayscale image colorization <span class="citation"
data-cites="iizuka_2016 su_2020 yoo_2019">[<a href="#ref-iizuka_2016"
role="doc-biblioref">37</a>, <a href="#ref-su_2020"
role="doc-biblioref">76</a>, <a href="#ref-yoo_2019"
role="doc-biblioref">84</a>]</span> others are extending this work to
lineart or sketch colorization <span class="citation"
data-cites="liu_2017 frans_2017">[<a href="#ref-frans_2017"
role="doc-biblioref">21</a>, <a href="#ref-liu_2017"
role="doc-biblioref">48</a>]</span> which is considered more difficult
by the CV standards as black and white linearts do not provide any
semantic visual information such as texture and shadow. The introduction
of the DL learning frameworks in the task of automatic colorization
enables end-to-end models to color sketches from scratch and output
generated colored illustrations with inferred colors, textures and
lighting.</p>
<figure id="fig:core_rel_auto_grayscale">
<img src="./figures/core_rel_auto_grayscale.png"
alt="Automatic colorization of grayscale images from Iizuka et al. [37]. Grayscale images are on the top and their respective generated colored versions bottom." />
<figcaption>Figure 42: Automatic colorization of grayscale images from
Iizuka et al. <span class="citation" data-cites="iizuka_2016">[<a
href="#ref-iizuka_2016" role="doc-biblioref">37</a>]</span>. Grayscale
images are on the top and their respective generated colored versions
bottom.</figcaption>
</figure>
<p><strong><span class="citation" data-cites="iizuka_2016">[<a
href="#ref-iizuka_2016" role="doc-biblioref">37</a>]</span> Iizuka et
al.:</strong> In their paper, Iizuka et al. present a novel technique to
automatically colorize grayscale images using both global priors and
local image features. The proposed method uses a CNN to fuse local
information and global priors, which are computed using the entire
image. The entire framework is trained in an end-to-end fashion and is
capable of processing images of any resolution. The model is trained on
an existing scene classification database to learn discriminative global
priors. A user study is conducted to validate the proposed method and it
is shown to outperform the state-of-the-art. Extensive experiments on
different types of images demonstrate the realistic colorizations of the
proposed method.</p>
<p><strong><span class="citation" data-cites="liu_2017">[<a
href="#ref-liu_2017" role="doc-biblioref">48</a>]</span> Liu et
al.:</strong> Liu et al. investigate the sketch-to-image synthesis
problem using Conditional Generative Adversarial Network (cGAN). To
solve this problem, the auto-painter model is proposed, which is capable
of automatically generating compatible colors for a sketch, as well as
allowing users to indicate preferred colors. Experimental results on two
sketch datasets demonstrate the effectiveness of the auto-painter model
compared to existing image-to-image methods.</p>
<figure id="fig:core_rel_auto_lineart">
<img src="./figures/core_rel_auto_lineart.png"
alt="Two-stage anime lineart colorization from Frans et al. [21]. The linearts are on the top and the final and second stage colorization bottom." />
<figcaption>Figure 43: Two-stage anime lineart colorization from Frans
et al. <span class="citation" data-cites="frans_2017">[<a
href="#ref-frans_2017" role="doc-biblioref">21</a>]</span>. The linearts
are on the top and the final and second stage colorization
bottom.</figcaption>
</figure>
<p><strong><span class="citation" data-cites="frans_2017">[<a
href="#ref-frans_2017" role="doc-biblioref">21</a>]</span> Frans et
al.:</strong> In their contribution, Frans et al. propose a solution to
automatically colorize raw line art by leveraging two networks in
tandem. The first network is a color prediction network based solely on
outlines, while the second network is a shading network conditioned on
both outlines and a color scheme. Processing methods are used to limit
information passed in the color scheme, which improves generalization.
Finally, the research demonstrates natural-looking results when
colorizing outlines from scratch, as well as from a messy, user-defined
color scheme.</p>
<p><strong><span class="citation" data-cites="yoo_2019">[<a
href="#ref-yoo_2019" role="doc-biblioref">84</a>]</span> Yoo et
al.:</strong> In their work, Yoo et al. present a novel memory-augmented
colorization model called MemoPainter that is capable of producing
high-quality colorization with limited training data. It can capture
rare instances and successfully colorize them. Furthermore, a novel
threshold triplet loss is proposed to enable unsupervised training of
memory networks without the need for class labels. Experiments show that
the model is superior in both few-shot and one-shot colorization
tasks.</p>
<p><strong><span class="citation" data-cites="su_2020">[<a
href="#ref-su_2020" role="doc-biblioref">76</a>]</span> Su et
al.:</strong> Su et al. present a method for instance-aware image
colorization that leverages an object detector to obtain cropped object
images and a colorization network to extract object-level features. The
network fuses the object-level and image-level features to predict the
final colors, and the entire model is learned from a large-scale
dataset. Experimental results show that this model outperforms existing
methods and achieves state-of-the-art results.</p>
<h3 id="sec:III.3.2">Example-Based</h3>
<p>Another category of network conditioning for the task of automatic
lineart colorization is the use of examples <span class="citation"
data-cites="hensman_2017 furusawa_2017 zhang_ji_2017 shi_2020">[<a
href="#ref-furusawa_2017" role="doc-biblioref">22</a>, <a
href="#ref-hensman_2017" role="doc-biblioref">32</a>, <a
href="#ref-shi_2020" role="doc-biblioref">73</a>, <a
href="#ref-zhang_ji_2017" role="doc-biblioref">86</a>]</span>. Examples
can be used to generate small datasets from which a network can be
trained to replicate and learn the colors from <span class="citation"
data-cites="hensman_2017 furusawa_2017">[<a href="#ref-furusawa_2017"
role="doc-biblioref">22</a>, <a href="#ref-hensman_2017"
role="doc-biblioref">32</a>]</span> but also as stylistic targets <span
class="citation" data-cites="zhang_ji_2017 shi_2020">[<a
href="#ref-shi_2020" role="doc-biblioref">73</a>, <a
href="#ref-zhang_ji_2017" role="doc-biblioref">86</a>]</span>. The style
and content of the example image can be extracted using a pretrained NN
and the style transferred to the target lineart. The method has been
extended to allow the colorization of consistently animated frames <span
class="citation" data-cites="shi_2020">[<a href="#ref-shi_2020"
role="doc-biblioref">73</a>]</span>.</p>
<p><strong><span class="citation" data-cites="hensman_2017">[<a
href="#ref-hensman_2017" role="doc-biblioref">32</a>]</span> Hensman et
al.:</strong> In their contribution, Hensman et al. propose an approach
to colorizing manga using a Conditional Generative Adversarial Network
(cGAN), which requires only a single colorized reference image for
training, rather than hundreds or thousands of images. Additionally, a
method of segmentation and color correction is proposed to improve the
resolution and clarity of the output. The final results are sharp, clear
and in high resolution, and stay true to the character’s original color
scheme.</p>
<p><strong><span class="citation" data-cites="furusawa_2017">[<a
href="#ref-furusawa_2017" role="doc-biblioref">22</a>]</span> Furusawa
et al.:</strong> Furusawa et al. developed Comicolorization, a
semi-automatic colorization system for manga images, allowing users to
generate a plausible color version of a manga page with the same color
for the same character across multiple panels. The system utilizes color
features extracted from reference images to help colorize the target
character, employing adversarial loss to encourage the effect of the
color features. The system also provides users with the option to
interactively revise the colorization result. This is the first work to
address the colorization of an entire manga title, making it possible to
colorize the entire manga using desired colors for each panel.</p>
<figure id="fig:core_rel_styletransfer">
<img src="./figures/core_rel_styletransfer.png"
alt="Style transfer-based colorization from Zhang et al. [86]. The three examples contain the input lineart top left, the style target bottom left, and the resulting output on the right." />
<figcaption>Figure 44: Style transfer-based colorization from Zhang et
al. <span class="citation" data-cites="zhang_ji_2017">[<a
href="#ref-zhang_ji_2017" role="doc-biblioref">86</a>]</span>. The three
examples contain the input lineart top left, the style target bottom
left, and the resulting output on the right.</figcaption>
</figure>
<p><strong><span class="citation" data-cites="zhang_ji_2017">[<a
href="#ref-zhang_ji_2017" role="doc-biblioref">86</a>]</span> Zhang et
al.:</strong> Zhang et al. use neural style transfer to automatically
synthesize credible paintings from content and style images. However,
when attempting to apply a painting’s style to an anime sketch, these
methods fail to accurately transfer the style. their paper proposes a
method which combines a residual U-net and an auxiliary classifier GAN
to effectively apply the style to the sketch while maintaining quality
and colorization. The process is both automatic and fast.</p>
<p><strong><span class="citation" data-cites="shi_2020">[<a
href="#ref-shi_2020" role="doc-biblioref">73</a>]</span> Shi et
al.:</strong> In their contribution, Shi et al. propose a deep
architecture for automatically coloring line art videos according to the
colors of reference images. The architecture consists of a color
transform network and a temporal constraint network. The color transform
network utilizes non-local similarity matching to determine the region
correspondences between the target and reference images, and also
incorporates Adaptive Instance Normalization (AdaIN) to ensure global
color style consistency. The temporal constraint network learns the
spatiotemporal features through 3D convolution to ensure temporal
consistency of the target image and the reference image. Experiments
show that this method achieves the best performance on line art video
coloring compared to other state-of-the-art methods.</p>
<h3 id="sec:III.3.3">Tags</h3>
<p>Another method, closer to the use of natural language description, is
the use of tags (labels) [kim_2019]. Tags can be obtained from online
illustration galleries and used for training of both classifiers and
generative architectures. Their final use is however limited to the set
of tags made available during training.</p>
<figure id="fig:core_rel_tags">
<img src="./figures/core_rel_tags.png"
alt="Tag-based colorization sampled from Kim et al. [41]. The examples illustrate the ability to control the generated illustration output given a lineart and tags describing the end-user intent." />
<figcaption>Figure 45: Tag-based colorization sampled from Kim et al.
<span class="citation" data-cites="kim_2019">[<a href="#ref-kim_2019"
role="doc-biblioref">41</a>]</span>. The examples illustrate the ability
to control the generated illustration output given a lineart and tags
describing the end-user intent.</figcaption>
</figure>
<p><strong><span class="citation" data-cites="kim_2019">[<a
href="#ref-kim_2019" role="doc-biblioref">41</a>]</span> Kim et
al.:</strong> In their paper, Kim et al. proposed Tag2Pix, a line art
colorization method using a GAN approach to produce a quality colored
image from a grayscale line art and color tag information. It consists
of a generator network with convolutional layers for transforming the
input line art, a pre-trained semantic extraction network, and an
encoder for input color information. The discriminator is based on an
auxiliary classifier GAN to classify the tag information and the
genuineness. A novel two-step training method is proposed where the
generator and discriminator first learn the notion of object and shape
and then, based on that, learn colorization. The effectiveness of the
proposed method is demonstrated through quantitative and qualitative
evaluations.</p>
<h3 id="sec:III.3.4">Natural Language Prompt</h3>
<p>One of the most natural ways of describing one’s intent is the use of
natural language. Natural language can be compressed into meaningful
latent code sequences and used as a conditional signal to train
conditional generative models for the task of automatic colorization
<span class="citation" data-cites="zou_2019 chen_2018">[<a
href="#ref-chen_2018" role="doc-biblioref">12</a>, <a
href="#ref-zou_2019" role="doc-biblioref">91</a>]</span>. While language
can serve as a useful descriptor and add priors to the NN, it is hard to
produce a colored illustration representing the full extent of the user
intent. For example, positioning is hard. This issue can, however, be
solved by combination with other conditioning means such as
user-provided color strokes <span class="citation"
data-cites="zou_2019">[<a href="#ref-zou_2019"
role="doc-biblioref">91</a>]</span>.</p>
<p><strong><span class="citation" data-cites="chen_2018">[<a
href="#ref-chen_2018" role="doc-biblioref">12</a>]</span> Chen et
al.:</strong> Chen et al. investigate the Language-Based Image Editing
(LBIE) problem where a source image is to be edited based on a natural
language description. A generic modeling framework is proposed for two
sub-tasks of LBIE, specifically language-based image segmentation and
image colorization. This framework is based on recurrent attentive
models to fuse image and language features, and a termination gate is
used to determine how much information to extrapolate from the text
description. The effectiveness of the framework is evaluated on three
datasets: a synthetic CoSaL dataset to evaluate end-to-end performance,
ReferIt dataset for image segmentation, and the Oxford-102 Flowers
dataset for colorization.</p>
<figure id="fig:core_rel_prompt">
<img src="./figures/core_rel_prompt.png"
alt="Natural language prompts colorization from Zou et al. [91]. Their model can capture the user intent from natural language prompts and apply it to the given lineart." />
<figcaption>Figure 46: Natural language prompts colorization from Zou et
al. <span class="citation" data-cites="zou_2019">[<a
href="#ref-zou_2019" role="doc-biblioref">91</a>]</span>. Their model
can capture the user intent from natural language prompts and apply it
to the given lineart.</figcaption>
</figure>
<p><strong><span class="citation" data-cites="zou_2019">[<a
href="#ref-zou_2019" role="doc-biblioref">91</a>]</span> Zou et
al.:</strong> In their work, Zou et al. present a language-based system
for interactive colorization of scene sketches. The system is built upon
NNs trained on a large dataset of scene sketches and cartoon-style color
images with text descriptions. Given a scene sketch, the system allows
users to interactively localize and colorize specific foreground object
instances with language-based instructions. The effectiveness of the
approach is demonstrated through comprehensive experiments and
generalization user studies. The authors envision a combination of the
language-based interface with a traditional scribble-based interface for
a multimodal colorization system.</p>
<h3 id="sec:III.3.5">Color Hints</h3>
<p>Finally, generative models trained for automatic colorization can be
conditioned on user-provided color strokes or scribble lines <span
class="citation"
data-cites="kautz_2007 zhang_richard_2017 zhang_2018 petalicapaint_2023 paintschainer_2018 ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>, <a
href="#ref-kautz_2007" role="doc-biblioref">50</a>, <a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>, <a
href="#ref-petalicapaint_2023" role="doc-biblioref">62</a>, <a
href="#ref-zhang_2018" role="doc-biblioref">88</a>, <a
href="#ref-zhang_richard_2017" role="doc-biblioref">89</a>]</span>. They
are probably one of the most natural means of expressing the artist or
end user intent for colorization because of its resemblance to the
natural coloring workflow. Whether it is digital or manual painting, we
are used to applying color using successive and organized scribbles.
Contrary to natural language prompts, they have the power to be
precisely indicate positioning intent. Stroke hints have first been
introduced in the DL approaches for user-guided grayscale colorization
<span class="citation"
data-cites="kautz_2007 sangkloy_2016 zhang_richard_2017">[<a
href="#ref-kautz_2007" role="doc-biblioref">50</a>, <a
href="#ref-sangkloy_2016" role="doc-biblioref">71</a>, <a
href="#ref-zhang_richard_2017" role="doc-biblioref">89</a>]</span> and
then translated to lineart colorization <span class="citation"
data-cites="zhang_2018 petalicapaint_2023 paintschainer_2018 ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>, <a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>, <a
href="#ref-petalicapaint_2023" role="doc-biblioref">62</a>, <a
href="#ref-zhang_2018" role="doc-biblioref">88</a>]</span> taking
inspiration from early work <span class="citation"
data-cites="sykora_2009">[<a href="#ref-sykora_2009"
role="doc-biblioref">77</a>]</span>.</p>
<p><strong><span class="citation" data-cites="kautz_2007">[<a
href="#ref-kautz_2007" role="doc-biblioref">50</a>]</span> Kautz et
al.:</strong> In their contribution, Kautz et al. present an interactive
system for colorizing natural images of complex scenes. The system
separates the colorization procedure into two stages: Color labeling and
Color mapping. Color labeling groups similar pixels into coherent
regions based on a new algorithm that incorporates intensity-continuity
and texture-similarity constraints. The Color mapping stage assigns
colors to a few pixels in each region. An intuitive user interface is
designed to allow users to label, color and modify the results. The
system is demonstrated to produce vivid colorization effects with only a
modest amount of user input.</p>
<p><strong><span class="citation" data-cites="sangkloy_2016">[<a
href="#ref-sangkloy_2016" role="doc-biblioref">71</a>]</span> Sangkloy
et al.:</strong> In their work, Sangkloy et al. propose a deep
adversarial image synthesis architecture that is conditioned on sketched
boundaries and sparse color strokes to generate realistic cars,
bedrooms, or faces. This architecture is capable of generating
convincing images that satisfy both the color and the sketch constraints
of the user in real-time. It also allows for user-guided colorization of
grayscale images. This architecture is more realistic, diverse, and
controllable than other recent works on sketch to image synthesis.</p>
<figure id="fig:core_rel_strokes_grayscale">
<img src="./figures/core_rel_strokes_grayscale.png"
alt="User-guided grayscale image colorization using colored strokes as the conditioning signal used to vehiculate the user intent from Zhang et al. [89]. This conditioning in addition to the deep learned priors enables qualitative and real-time colorization." />
<figcaption>Figure 47: User-guided grayscale image colorization using
colored strokes as the conditioning signal used to vehiculate the user
intent from Zhang et al. <span class="citation"
data-cites="zhang_richard_2017">[<a href="#ref-zhang_richard_2017"
role="doc-biblioref">89</a>]</span>. This conditioning in addition to
the deep learned priors enables qualitative and real-time
colorization.</figcaption>
</figure>
<p><strong><span class="citation" data-cites="zhang_richard_2017">[<a
href="#ref-zhang_richard_2017" role="doc-biblioref">89</a>]</span> Zhang
et al.:</strong> Zhang et al. propose a DL based approach to user-guided
image colorization. The system employs a CNN to map a grayscale image
and sparse user “hints” to produce realistic colorization. The network
combines low-level cues with semantics learned from a large-scale data
set, and is capable of producing real-time colorization. The system also
provides users with likely colors to guide them in efficient input
selection. Furthermore, the framework can incorporate other user hints
to the desired colorization, showing an application to color histogram
transfer.</p>
<p><strong><span class="citation" data-cites="zhang_2018">[<a
href="#ref-zhang_2018" role="doc-biblioref">88</a>]</span> Zhang et
al.:</strong> In their paper, Zhang et al. propose a semi-automatic
learning-based framework to colorize sketches with proper color,
texture, and gradient. It divides the task into two stages: (1) a
drafting stage where the model guesses color regions and splashes a
variety of colors over the sketch, and (2) a refinement stage where the
model detects unnatural colors and artifacts and tries to refine the
result. An interactive software was developed to evaluate the model,
which allows users to iteratively edit and refine the colorization. An
extensive user study was conducted to evaluate the learning model and
the interactive system, and results showed that it outperforms existing
techniques and industrial applications in terms of visual quality, user
control, user experience, and other metrics.</p>
<p><strong><span class="citation" data-cites="petalicapaint_2023">[<a
href="#ref-petalicapaint_2023" role="doc-biblioref">62</a>]</span>
Petalica Paint:</strong> Petalica Paint, previously called PaintsChainer
<span class="citation" data-cites="paintschainer_2018">[<a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>]</span>, is a
line drawing colorizer built with Chainer. It uses a U-net-based CNN
trained to semi-automatically colorize sketches. The network is
conditioned on user-provided color stokes and can interpret the user
intent in the generated colored illustration. This is one of the first
publicly available and web-based deployed models which contributed to
its success.</p>
<figure id="fig:core_rel_strokes_lineart">
<img src="./figures/core_rel_strokes_lineart.png"
alt="User-guided anime lineart colorization using colored strokes as the conditioning signal used to vehiculate the user intent from Ci et al. [14]. This conditioning in addition to the deep learned priors enables qualitative and real-time colorization." />
<figcaption>Figure 48: User-guided anime lineart colorization using
colored strokes as the conditioning signal used to vehiculate the user
intent from Ci et al. <span class="citation" data-cites="ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>]</span>. This
conditioning in addition to the deep learned priors enables qualitative
and real-time colorization.</figcaption>
</figure>
<p><strong><span class="citation" data-cites="ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>]</span> Ci et
al.:</strong> Ci et al. propose a novel deep cGAN for the challenging
task of scribble-based anime line art colorization. The proposed model
integrates the framework with Conditional Wasserstein Generative
Adversarial Network with Gradient Penalty (cWGAN-GP) criteria and
perceptual loss to generate more natural and realistic images.
Additionally, a local features network is introduced to increase the
generalization capability over “in the wild” line arts. To support the
model, two datasets providing high-quality illustrations and authentic
linearts are collected. The results demonstrate that the proposed
approach produces more realistic and precise images than other
methods.</p>
<h2 id="sec:III.4">Summary</h2>
<p>The task of semi-automatic lineart colorization is currently
dominated by the DL community. Their offer unprecedented image quality
for generated colored illustrations from single input lineart with
various means of control among which scribble lines and color strokes
are the most popular although current trends tend to move towards the
use of natural language prompts. While such advances are undeniably
better in terms of perceptual quality and variability, they present
limitations. GAN approaches, most of them, are still subject to visual
artifacts and are biased towards the color of the used dataset, mostly
pastel colors because of the omnipresence of apparent skin in the
majority of the images. Additionally, the end-user is involved in the
creation process only once, at the beginning when producing the initial
prompts or color hints.</p>
<p>In this thesis, we explore how one can improve the current quality of
the generated images, remove artifacts, how to give the control back to
the end-user and best transcribe its colorization intent, and finally
explore the use of the DDM architecture to further increase the
generation quality but also add the ability to perform variation
exploration as DDM models offer more qualitative interpolation in
comparison to GANs.
<!-- ===================== [END] PART RELATED WORK ===================== --></p>
<!-- ===================== [START] PART CONTRIBUTIONS ===================== -->
<h1 id="sec:IV">Contributions in Automatic Lineart Colorization</h1>
<h2 id="sec:IV.1">Method and Implementation</h2>
<p>In this chapter, we discuss the methodology used throughout this
thesis dissertation. The dataset curation is described in
Sec <strong>¿sec:dataset-curation?</strong>, the objective evaluation
process in Sec <strong>¿sec:objective-eval?</strong>, the subjective
evaluation in Sec <strong>¿sec:subjective-eval?</strong>, the
implementation details in Sec <strong>¿sec:implementation?</strong>, and
the reproducibility in Sec <strong>¿sec:reproducibility?</strong>.</p>
<h3 id="sec:IV.1.1">Synthetic Dataset Pipeline</h3>
<p>The challenge of anime lineart colorization faces a lack of available
datasets with perceptive qualitative content. To acquire corresponding
pairs of lineart and illustrations, online scraping and synthetic
lineart extraction are the methods used by the majority of the
contributions in the literature <span class="citation"
data-cites="ci_2018 zhang_richard_2017 petalicapaint_2023 paintschainer_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>, <a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>, <a
href="#ref-petalicapaint_2023" role="doc-biblioref">62</a>, <a
href="#ref-zhang_richard_2017" role="doc-biblioref">89</a>]</span>.</p>
<figure id="fig:meta_danboruu_samples">
<img src="./figures/meta_danboruu_samples.png"
alt="Randomly sample illustrations extracted from the Danbooru dataset [2]. The dataset is mixing styles, quality, and image nature such as comic pages, photos, illustrations, and more." />
<figcaption>Figure 49: Randomly sample illustrations extracted from the
Danbooru dataset <span class="citation" data-cites="danbooru_2020">[<a
href="#ref-danbooru_2020" role="doc-biblioref">2</a>]</span>. The
dataset is mixing styles, quality, and image nature such as comic pages,
photos, illustrations, and more.</figcaption>
</figure>
<p>Currently, there are few public datasets available for the community
<span class="citation" data-cites="danbooru_2020 danboo_region_2020">[<a
href="#ref-danbooru_2020" role="doc-biblioref">2</a>, <a
href="#ref-danboo_region_2020" role="doc-biblioref">87</a>]</span>, the
content of which is not uniform in terms of perceptual quality, image
nature (e.g. comics pages, photos), and contains illustrations from
different artists with varied skill levels and styles (see <a
href="#fig:meta_danboruu_samples">Fig 49</a>). To address this, we have
curated a custom dataset.</p>
<p>Our dataset contains <span class="math inline">\(21,930\)</span>
scrapped anime-like illustrations for training and <span
class="math inline">\(3,545\)</span> for testing. Moreover, it is
manually filtered to ensure a consistent perceptive quality across the
samples and to remove inappropriate (e.g. gore, mature, and sexual)
content. In our work PaintsTorch <span class="citation"
data-cites="hati_2019">[<a href="#ref-hati_2019"
role="doc-biblioref">29</a>]</span>, we highlight the importance of the
dataset quality for the generation process.</p>
<p>It is essential to note that any dataset <span class="citation"
data-cites="danbooru_2020 danboo_region_2020">[<a
href="#ref-danbooru_2020" role="doc-biblioref">2</a>, <a
href="#ref-danboo_region_2020" role="doc-biblioref">87</a>]</span> used
for the challenge of anime-like line art colorization is biased. The
drawings are mainly of female characters with visible skin, a reflection
of the anime subculture and communities from which they are drawn. This
may account for the overall salmon watercolor tone attributed to the
illustrations produced by current works.</p>
<h4 id="sec:IV.1.2">Synthetic Lineart</h4>
<p>The lack of lineart and illustration pairs can be overcome using
synthetic generation. Previous work <span class="citation"
data-cites="ci_2018">[<a href="#ref-ci_2018"
role="doc-biblioref">14</a>]</span> proposed to use extended Difference
of Gaussians (xDoG) <span class="citation"
data-cites="winnermoller_2012">[<a href="#ref-winnermoller_2012"
role="doc-biblioref">82</a>]</span>, an extended edge detector derived
from Difference of Gaussians (DoG) that is less susceptible to noise and
can produce qualitative synthetic linearts from colored images, or
photographs (see <a href="#fig:met_xdog">Fig 50</a>).</p>
<figure id="fig:met_xdog">
<img src="./figures/met_xdog.svg"
alt="extended Difference of Gaussians (xDoG) applied to a colored photo using different parametrizations to illustrate output variations in the lines. The source image is on the left and the four synthetic lineart variations are on the right. Credit https://images.all4ed.org/." />
<figcaption>Figure 50: extended Difference of Gaussians (xDoG) applied
to a colored photo using different parametrizations to illustrate output
variations in the lines. The source image is on the left and the four
synthetic lineart variations are on the right. Credit <a
href="https://images.all4ed.org/">https://images.all4ed.org/</a>.</figcaption>
</figure>
<p>Using different initial parameters for xDoG, and different
thresholding techniques, we are able to generate a variety of plausible
binarized linearts from original colored illustrations.</p>
<h4 id="sec:IV.1.3">Copyright Policy</h4>
<p>The images scrapped online for curating our dataset do belong to
their original authors. They are not used for any commercial
applications not distributed to the public but are used for educational
and research purposes only based on the faire use policy. We thus share
our method for curating such a dataset and would advise any commercial
implementation to curate their own illustrations by employing artists or
compensating for the usage of their intellectual property.</p>
<h3 id="sec:IV.1.4">Evaluation Metrics</h3>
<p>The methods presented in this thesis dissertation are evaluated and
trained on our custom dataset containing illustrations scrapped from the
web and filtered manually, <span class="math inline">\(21,930\)</span>
for training, and <span class="math inline">\(3,545\)</span> for test.
The images are resized to <span class="math inline">\(512\)</span> on
their smallest side and randomly cropped to <span
class="math inline">\(512 \times 512\)</span> during training, and
center cropped only at test time.</p>
<p>The measure of perceptual quality is an entire research domain. The
GAN literature reports the use of both objective and subjective metrics.
The objective metrics offer a non-refutable measure of perceptual
quality and are often based on pre-trained image NNs. They however are
hard to interpret. A small variation in those metrics cool hide a big
change in quality. The subjective metrics are used to overcome those
issues and require performing a user-study. This section discusses the
metrics used in this thesis.</p>
<h4 id="sec:IV.1.4.1">Objective Evaluation</h4>
<p>In this dissertation, we measure the perceptual quality of a
generated colored illustration using the Fréchet Inception Distance
(FID) <span class="citation" data-cites="heusel_2017">[<a
href="#ref-heusel_2017" role="doc-biblioref">33</a>]</span> (see <a
href="#fig:met_fid">Fig 51</a>). This metric is based on the feature
learned by an ImageNet pre-trained NN such as InceptionNet <span
class="citation" data-cites="szegedy_2015">[<a href="#ref-szegedy_2015"
role="doc-biblioref">78</a>]</span>. We measure the similarity of a
given pair of fake and real images by comparing them in feature
space.</p>
<p>Instead of assessing images by their individual pixels (like what the
<span class="math inline">\(L_2\)</span> norm does), the FID compares
the average and variation of the deepest layer in Inception v3 <span
class="citation" data-cites="szegedy_2016">[<a href="#ref-szegedy_2016"
role="doc-biblioref">79</a>]</span>. These layers are closer to output
neurons focusing on object representation. Consequently, they imitate
the way humans judge similarities between images. The +FID between two
distributions <span class="math inline">\(r\)</span>, <span
class="math inline">\(f\)</span> is:</p>
<p><span id="eq:fid"><span class="math display">\[
D_F(r, f) = \sqrt{\underset{\gamma \in \Gamma(r, f)}{inf} \int ||x -
y||_2^2 d\gamma(x, y)}
\qquad{(26)}\]</span></span></p>
<p>where <span class="math inline">\(\Gamma(r, f)\)</span> is the set of
all couplings of <span class="math inline">\(r\)</span> and <span
class="math inline">\(f\)</span>, theur Wasserstein distance. When
modeling the data distribution of two datasets by two multivariate
Gaussians <span class="math inline">\(\mathcal{N}(\mu_r,
\Sigma_r)\)</span>, and <span class="math inline">\(\mathcal{N}(\mu_f,
\Sigma_f)\)</span>, this expression is solvable in closed-form:</p>
<p><span id="eq:fid_gauss"><span class="math display">\[
D_F(\mathcal{N}_r, \mathcal{N}_f)^2 = ||\mu_r - \mu_f||_2^2 +
tr(\Sigma_r + \Sigma_f - 2 (\Sigma_r^{\frac{1}{2}} \cdot \Sigma_f \cdot
\Sigma_r^{\frac{1}{2}})^{\frac{1}{2}})
\qquad{(27)}\]</span></span></p>
<p>The FID between real images (extracted from training data) and fake
images (generated) can thus be computed by sampling pairs of those <span
class="math inline">\((X_r, X_f)\)</span>, computing their inception
features <span class="math inline">\((f(X_r), f(X_f))\)</span>, feating
two multivariate Gaussians <span class="math inline">\((\mathcal{N}_r,
\mathcal{N}_f)\)</span>, and computing their similarity <span
class="math inline">\(D_F(\mathcal{N}_r, \mathcal{N}_f)\)</span> using
the formula shown in <a href="#eq:fid_gauss">Eq 27</a>.</p>
<figure id="fig:met_fid">
<img src="./figures/met_fid.svg"
alt="Fréchet Inception Distance (FID) measured between an input image and altered versions. Noise injection is shown on the left and blurring on the right. The more alteration there is to the original image, the further the FID grows away from 0. Credit https://images.all4ed.org/" />
<figcaption>Figure 51: Fréchet Inception Distance (FID) measured between
an input image and altered versions. Noise injection is shown on the
left and blurring on the right. The more alteration there is to the
original image, the further the FID grows away from <span
class="math inline">\(0\)</span>. Credit <a
href="https://images.all4ed.org/">https://images.all4ed.org/</a></figcaption>
</figure>
<h4 id="sec:IV.1.4.2">Subjective Evaluation</h4>
<p>Art is however a subjective matter. Depending on the cultural,
societal, and economical background of individuals, one may have a
different perspective on a given piece of art in comparison to another.
For this reason, a subjective evaluation of automatic lineart
colorization methods must be conducted. In this thesis with perform
subjective evaluation using the standard Mean Opinion Score (MOS)
approach where one has to rate images for their colorization quality
without knowing the method used to generate them from <span
class="math inline">\(1\)</span> to <span
class="math inline">\(5\)</span> (see <a href="#eq:mos">Eq 28</a>). A
score of <span class="math inline">\(1\)</span> means the colorization
is bad, and <span class="math inline">\(5\)</span> excellent. Studies in
human perception tests have shown that MOS is not a linear metric and
that people tend to avoid giving extremums such as <span
class="math inline">\(1\)</span> and <span
class="math inline">\(5\)</span>. An MOS value of <span
class="math inline">\(4\)</span> is thus considered good enough as a
target.</p>
<p><span id="eq:mos"><span class="math display">\[
MOS = \sum_{i=1}^{N} \frac{R_i}{N}, \; R_i \in \{1; 2; 3; 4; 5\}
\qquad{(28)}\]</span></span></p>
<p>Our MOS study included <span class="math inline">\(46\)</span>
individuals from <span class="math inline">\(16\)</span> to <span
class="math inline">\(30\)</span> years old, with <span
class="math inline">\(26%\)</span> women and <span
class="math inline">\(35%\)</span> experience in drawing, colorization
or a related subject. The study consisted in showing <span
class="math inline">\(20\)</span> illustrations randomly sampled from
our custom test set and colorized using different methods and
conditioned with their corresponding color hint method. The results of
the study are discussed in later chapters when presenting our methods
PaintsTorch <span class="citation" data-cites="hati_2019">[<a
href="#ref-hati_2019" role="doc-biblioref">29</a>]</span> (see
Sec <strong>¿sec:contrib-1?</strong>), and StencilTorch <span
class="citation" data-cites="hati_2023">[<a href="#ref-hati_2023"
role="doc-biblioref">30</a>]</span> (see
Sec <strong>¿sec:contrib-2?</strong>).</p>
<h3 id="sec:IV.1.5">Reproducibility</h3>
<p>For reproducibility and transparency, the implementation frameworks
and the devices used for measurement are discussed in this section. As a
reminder, the weights of our models and our custom datasets are not
shared for copyright policies for which the reasons are further
discussed in a specific chapter at this end of this document (see <a
href="#sec:ethical-and-societal-impact">Sec 6.1</a>).</p>
<h4 id="sec:IV.1.5.1">Implementation</h4>
<p>The code for this thesis’ projects are all available on GitHub at <a
href="https://github.com/yliess86">https://github.com/yliess86</a>.
Training, exploration, and testing codes are written in Python as it has
been designated as the DL language of choice by the community. Web
application demos have been written in pure HTML and Javascript.</p>
<p>The neural networks presented in this work are implemented using the
PyTorch library <span class="citation" data-cites="pytorch">[<a
href="#ref-pytorch" role="doc-biblioref">61</a>]</span>, one of the most
used Automatic Differentiation (AD) framework with out-of-the-box
support for DL accelerators. When required to export the trained models
for building web demos, the models are exported for TensorFlowJS <span
class="citation" data-cites="smilkov_2019">[<a href="#ref-smilkov_2019"
role="doc-biblioref">75</a>]</span> using the ONNX intermediate and
universal converter <span class="citation" data-cites="bai_2019">[<a
href="#ref-bai_2019" role="doc-biblioref">6</a>]</span>.</p>
<h4 id="sec:IV.1.5.2">Measurements</h4>
<p>All experimentations have been made possible thanks to the use of a
custom made Computer equipped with an AMD® Ryzen 9 5900x <span
class="math inline">\(12\)</span>-core processor and <span
class="math inline">\(24\)</span>-threads processor, <span
class="math inline">\(32\)</span> GB of DDR4 RAM, and a Nvidia GeForce
RTX 3090 GPU with 24 GB of dedicated v-RAM. The final training, scaling,
and hyperparameter tuning of our models have been performed on a
DGX1-station from Nvidia equipped with an Intel Xeon E5-2698 <span
class="math inline">\(20\)</span>-core processor, <span
class="math inline">\(512\)</span> GB of DDR4 RAM, and four V100 GPUs
with <span class="math inline">\(32\)</span> GB of v-RAM each.</p>
<h2 id="sec:IV.2">PaintsTorch: User-Guided Lineart Colorization</h2>
<figure id="fig:core_pt_teaser">
<img src="./figures/core_pt_teaser.png"
alt="User-guided colorization using color stroke conditioning generated by our method PaintsTorch. The input lineart and color stroke hint are shown on the left, and the generated colored illustration on the right." />
<figcaption>Figure 52: User-guided colorization using color stroke
conditioning generated by our method PaintsTorch. The input lineart and
color stroke hint are shown on the left, and the generated colored
illustration on the right.</figcaption>
</figure>
<h3 id="sec:IV.2.1">Introduction</h3>
<p>This chapter discusses our work on PaintsTorch <span class="citation"
data-cites="hati_2019">[<a href="#ref-hati_2019"
role="doc-biblioref">29</a>]</span>, a user-guided anime lineart
colorization tool with double Conditional Generative Adversarial
Networks (cGAN) (see <a href="#fig:core_pt_teaser">Fig 52</a>). While
the introduction of the GAN architecture <span class="citation"
data-cites="goodfellow_2014">[<a href="#ref-goodfellow_2014"
role="doc-biblioref">26</a>]</span> allowed the generation of
unprecedented quality illustration and intent conditioning <span
class="citation"
data-cites="zhang_2018 petalicapaint_2023 paintschainer_2018 ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>, <a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>, <a
href="#ref-petalicapaint_2023" role="doc-biblioref">62</a>, <a
href="#ref-zhang_2018" role="doc-biblioref">88</a>]</span>, they still
suffer from visual artifacts, color bleeding, and struggle with hint
positioning.</p>
<p>In this work, PaintsTorch <span class="citation"
data-cites="hati_2019">[<a href="#ref-hati_2019"
role="doc-biblioref">29</a>]</span>, our contributions are the
following:</p>
<ul>
<li>The introduction of a stricter dataset curation pipeline introduced
in the previous chapter on methodology (see
Sec <strong>¿sec:methodology?</strong>)</li>
<li>A new synthetic stroke generation scheme that is closer to the one
used during inference and provided by the end-user</li>
<li>We explore the use of a second generator network to enforce that the
colorization learned is linked to the lineart</li>
<li>A qualitative and quantitative evaluation comparing our work to
previous contributions</li>
<li>An interactive web application allowing users to naturally interact
with our model.</li>
</ul>
<h3 id="sec:IV.2.2">Method</h3>
<p>This section discusses the data generation pipeline used in
PaintsTorch <span class="citation" data-cites="hati_2019">[<a
href="#ref-hati_2019" role="doc-biblioref">29</a>]</span> (see
Sec <strong>¿sec:pt_synth?</strong>), the Conditional Wasserstein
Generative Adversarial Network (cWGAN) U-net-based model architecture
(see Sec <strong>¿sec:pt_arch?</strong>), the objective functions we aim
to optimize (see Sec <strong>¿sec:pt_losses?</strong>), and the training
regime (see Sec <strong>¿sec:pt_train?</strong>).</p>
<h4 id="sec:IV.2.2.1">Synthetic Inputs</h4>
<p>Because of the rarity of lineart and color illustration pairs, we
employ a synthetic data generation pipeline. Both the lineart and color
stoke hints are generated from scrapped illustrations curated for our
custom dataset.</p>
<p><strong>Synthetic Linearts:</strong> The linearts are generated by
applying the extended Difference of Gaussians (xDoG) <span
class="citation" data-cites="winnermoller_2012">[<a
href="#ref-winnermoller_2012" role="doc-biblioref">82</a>]</span> edge
detector algorithm to grayscaled versions of the given illustrations
with the following parametrization: <span class="math inline">\(\gamma =
0.95\)</span>, <span class="math inline">\(\phi = 1e^9\)</span>, <span
class="math inline">\(k = 4.5\)</span>, <span
class="math inline">\(\epsilon = -1e^{-1}\)</span>, and <span
class="math inline">\(\sigma \in \{ 0.3; 0.4, 0.5 \}\)</span>. Those
parameters are chosen to match the ones used by Ci et al. <span
class="citation" data-cites="ci_2018">[<a href="#ref-ci_2018"
role="doc-biblioref">14</a>]</span> as proven to be working for this
task and for a fair benchmark evaluation.</p>
<figure id="fig:core_pt_strokes">
<img src="./figures/core_pt_strokes.jpg"
alt="Synthetic colored strokes simulated using our method varying different brush parameters to add variation and make the model robust to stroke changes during inference. The generated illustration resulting from the color hints is on the left, and the synthetic color hint map on the right." />
<figcaption>Figure 53: Synthetic colored strokes simulated using our
method varying different brush parameters to add variation and make the
model robust to stroke changes during inference. The generated
illustration resulting from the color hints is on the left, and the
synthetic color hint map on the right.</figcaption>
</figure>
<p><strong>Synthetic Strokes:</strong> Previous work from Ci et al.
<span class="citation" data-cites="ci_2018">[<a href="#ref-ci_2018"
role="doc-biblioref">14</a>]</span> stated that training cGAN on
randomly sampled pixels as hints is enough to enable natural
interactions with colored strokes during inference. Their initial
assumptions are based on work from Zhang et al. <span class="citation"
data-cites="zhang_richard_2017">[<a href="#ref-zhang_richard_2017"
role="doc-biblioref">89</a>]</span>. They however dealt with grayscale
image colorization which is fundamentally different from lineart
colorization because of the shadow and texture information already
present in the input image. For this reason, we explore the use of
simulated natural color strokes for training and show that it yields
better results than random pixel activation hints.</p>
<p>We simulate natural colored strokes using the PIL drawing library and
a round brush. Our strokes are defined by four control parameters, the
number of strokes <span class="math inline">\(n_{strokes} \in [0;
10]\)</span>, the brush thickness <span class="math inline">\(t_{brush}
\in [1; 4]\)</span>, the number of waypoints defining a single stroke
path <span class="math inline">\(n_{points} \in [1; 5]\)</span>, and a
square boundary box within which the stroke line has to stay <span
class="math inline">\(w_{range} \in [0; 10]\)</span> (see <a
href="#fig:core_pt_strokes">Fig 53</a>). By applying the synthetic
stroke scheme, we aim at making the model more robust to real-world
strokes.</p>
<p><strong>Input Preprocessing:</strong> To add variety and robustness
to the model, the inputs are transformed and augmented. The illustration
is first used to generate the synthetic lineart in full scale. Both the
illustration and synthetic lineart are then resized to <span
class="math inline">\(512\)</span> on their smaller side before being
randomly cropped to <span class="math inline">\(512 \times 512\)</span>.
The illustration is then cloned and resized to <span
class="math inline">\(128 \times 128\)</span>, a fourth of its original
size, and then used to generate the synthetic colored strokes for which
an image of <span class="math inline">\(128 \times 128 \times 4\)</span>
is saved. The first three channels of the hint map are used to store the
stokes colors, and the last channel stores a mask of the hints (black
where no hint and white else). All the produced input content is
normalized in the <span class="math inline">\([-1; 1]\)</span> range for
stability except the hint mask channel which is normalized to the <span
class="math inline">\([0; 1]\)</span> range to ask as a filter.</p>
<h4 id="sec:IV.2.2.2">Model Architecture</h4>
<figure id="fig:core_pt_arch">
<img src="./figures/core_pt_arch.svg"
alt="PaintsTorch Conditional Wasserstein Generative Adversarial Network (cWGAN) architecture schematic." />
<figcaption>Figure 54: PaintsTorch Conditional Wasserstein Generative
Adversarial Network (cWGAN) architecture schematic.</figcaption>
</figure>
<p>PaintsTorch is a Conditional Wasserstein Generative Adversarial
Network (cWGAN) architecture (see <a
href="#fig:core_pt_arch">Fig 54</a>) inspired by the one proposed by Ci
et al. <span class="citation" data-cites="ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>]</span>. The generators
are identical <span class="math inline">\(G_1\)</span> and <span
class="math inline">\(G_2\)</span> are deep U-net <span class="citation"
data-cites="ronneberger_2015">[<a href="#ref-ronneberger_2015"
role="doc-biblioref">67</a>]</span> models with ResNeXt <span
class="citation" data-cites="he_2016">[<a href="#ref-he_2016"
role="doc-biblioref">31</a>]</span> blocks and dilated depth-wise
separable convolutions <span class="citation"
data-cites="chollet_2017">[<a href="#ref-chollet_2017"
role="doc-biblioref">13</a>]</span> to increase the receptive field of
the network. ShufflePixel blocks are used to prevent generation
artifacts such as checkboard patterns that are common in image
generation based on GANs. LeakyReLU activations are used throughout the
networks with a <span class="math inline">\(0.2\)</span> slope with the
exception of the output layer which is equipped with a Tanh activation
to satisfy the <span class="math inline">\([-1; 1]\)</span>
normalization constraint.</p>
<p>The critic network <span class="math inline">\(C\)</span> uses a
similar architecture as the SRGAN discriminator model <span
class="citation" data-cites="ledig_2017">[<a href="#ref-ledig_2017"
role="doc-biblioref">45</a>]</span> but with blocks that are similar to
the generators and without dilations.</p>
<p>Similarly to Ci et al. <span class="citation"
data-cites="ci_2018">[<a href="#ref-ci_2018"
role="doc-biblioref">14</a>]</span>, a local feature network <span
class="math inline">\(F_1\)</span> is used to bring back the missing
semantic information of the black and white lineart. <span
class="math inline">\(F_1\)</span> is the Illustration2Vec network <span
class="citation" data-cites="saito_2015">[<a href="#ref-saito_2015"
role="doc-biblioref">70</a>]</span> without the classification head. The
model was trained to tag anime illustrations in a supervised fashion.
Both the generators and critic networks are conditioned on the
Illustration2Vec features.</p>
<p>In PaintsTorch, we introduce a second generator <span
class="math inline">\(G_2\)</span> responsible for the reconstruction of
synthetic linearts from fake colored illustrations produced by the first
generator <span class="math inline">\(G_1\)</span>. This approach is
inspired by previous work on cross-domain training such as Zhu and al.
<span class="citation" data-cites="zhu_2017">[<a href="#ref-zhu_2017"
role="doc-biblioref">90</a>]</span> and their work on CycleGAN. The
second generator is learning an inverse mapping which aims to bring an
additional signal for training the first generator <span
class="math inline">\(G_1\)</span>.</p>
<h4 id="sec:IV.2.2.3">Objective Functions</h4>
<p>The objective functions optimized by PaintsTorch are the one of a
cWGAN with an additional term for the second generator being a simple
MSE between the regenerated lineart and the input one.</p>
<figure id="fig:core_pt_flow">
<img src="./figures/core_pt_flow.svg"
alt="PaintsTorch Conditional Wasserstein Generative Adversarial Network (cWGAN) data flow schematic." />
<figcaption>Figure 55: PaintsTorch Conditional Wasserstein Generative
Adversarial Network (cWGAN) data flow schematic.</figcaption>
</figure>
<p>The generator loss is a combination of three components: a content
loss, an adversarial one, and a reconstruction one:</p>
<p><span id="eq:pt_generator_loss"><span class="math display">\[L_G =
\lambda_{adv} \cdot L_{adv} + L_{cont} +
L_{recon}\qquad{(29)}\]</span></span></p>
<p>The adversarial component is computed using the local feature network
<span class="math inline">\(F_1\)</span> for conditioning and a
Wasserstein Generative Adversarial Network with Gradient Penalty
(WGAN-GP) objective <span class="citation"
data-cites="gulrajani_2017">[<a href="#ref-gulrajani_2017"
role="doc-biblioref">27</a>]</span> is used to minimized the critic
score when given a fake generated illustration:</p>
<p><span id="eq:pt_generator_loss_adv"><span
class="math display">\[L_{adv} = -E_{\hat{y}}[C(\hat{y},
F_1(x))]\qquad{(30)}\]</span></span></p>
<p>where <span class="math inline">\(\hat{y} = G_1(x, h,
F_1(x))\)</span> and <span class="math inline">\(\lambda_1 =
1e^{-4}\)</span>. The content loss is a perceptual loss relying on the
<span class="math inline">\(L_2\)</span> difference between the
generated output and the target CNN features maps computed from an
ImageNet <span class="citation" data-cites="deng_2009">[<a
href="#ref-deng_2009" role="doc-biblioref">16</a>]</span> pretrained
VGG16 network <span class="math inline">\(F_2\)</span> <span
class="citation" data-cites="simonyan_2014">[<a
href="#ref-simonyan_2014" role="doc-biblioref">74</a>]</span> using its
fourth activation layer:</p>
<p><span id="eq:pt_generator_loss_cont"><span
class="math display">\[L_{cont} = \frac{1}{chw} ||F_2(\hat{y}) -
F_2(y)||_2^2\qquad{(31)}\]</span></span></p>
<p>The reconstruction loss is the MSE between the input synthetic
lineart and the one reconstructed using the second generator <span
class="math inline">\(G_2\)</span> recovered from the generated colored
illustration. It is trying to learn an equivalent of xDoG and by doing
so, we hope to enforce an additional cycle consistency loss signal to
imporve the generation quality and compensate for the lack of semantic
information for the first network <span
class="math inline">\(G_1\)</span>:</p>
<p><span id="eq:pt_generator_loss_recon"><span
class="math display">\[L_{recon} = \frac{1}{hw} ||G_2(G_1(x, h, F_1(x)),
h, F_1(x)) - x||_2^2\qquad{(32)}\]</span></span></p>
<p>The critic <span class="math inline">\(C\)</span> loss is a
combination of two losses, the wasserstein loss and the gradient
penalty:</p>
<p><span id="eq:pt_critic_loss"><span class="math display">\[L_C = L_{w}
+ L_{gp}\qquad{(33)}\]</span></span></p>
<p>The wasserstein term is the critic loss used in the WGAN-GP paper
<span class="citation" data-cites="gulrajani_2017">[<a
href="#ref-gulrajani_2017" role="doc-biblioref">27</a>]</span>:</p>
<p><span id="eq:pt_critic_loss_wasserstein"><span
class="math display">\[L_{w} = E_{\hat{y} \sim P_G} [C(G_1(x, h,
F_1(x)), F_1(x))] - E_{y \sim P_r} [C(y,
F_1(x))]\qquad{(34)}\]</span></span></p>
<p>The gradient penalty is used to ensure that the critic network <span
class="math inline">\(C\)</span> satisfy a <span
class="math inline">\(1\)</span>-lipschitz constraint. The variant of
gradient penalty used in PaintsTorch has been introduced by Karras et
al. <span class="citation" data-cites="karras_2017">[<a
href="#ref-karras_2017" role="doc-biblioref">40</a>]</span> and propose
to add drift term to ensure stability during training with
hyperarameters <span class="math inline">\(\lambda_{gp} = 10\)</span>
and <span class="math inline">\(\epsilon_{drift} = 1e^{-3}\)</span>:</p>
<p><span id="eq:pt_critic_loss_gradient_penalty"><span
class="math display">\[L_{gp} = \lambda_{gp} \cdot E_{\hat{y}}
[(\nabla_{\hat{y}} C(\hat{y}, F_1(x)) - 1)^2] + \epsilon_{drift} \cdot
E_{y} [C(y, F_1(x))^2]\qquad{(35)}\]</span></span></p>
<p>The data flow used to compute the losses is summarized in <a
href="#fig:core_pt_flow">Fig 55</a>.</p>
<h4 id="sec:IV.2.2.4">Training</h4>
<p>The model is trained end-to-end using the Adam optimizer <span
class="citation" data-cites="kingma_2014">[<a href="#ref-kingma_2014"
role="doc-biblioref">42</a>]</span> with a learning rate of <span
class="math inline">\(\epsilon = 1e^{-4}\)</span>, <span
class="math inline">\(\beta_1 = 0.5\)</span>, and <span
class="math inline">\(\beta_2 = 0.9\)</span>. We first optimize for one
step for the critic network <span class="math inline">\(C\)</span>, then
the first generator <span class="math inline">\(G_1\)</span>, and
finally the second generator <span class="math inline">\(G_2\)</span>.
PaintsTorch is trained for <span class="math inline">\(100\)</span>
epochs on our custom dataset.</p>
<h3 id="sec:IV.2.3">Results</h3>
<p>In this section we present the objective and subjective evaluations
(see Sec <strong>¿sec:core_pt_reval?</strong>), perform a qualitative
visual analysis of the strengh and limitations of our contribution (see
Sec <strong>¿sec:core_pt_limits?</strong>).</p>
<h4 id="sec:IV.2.3.1">Evaluation</h4>
<p>Our model PaintsTorch is evaluated using subjective metrics, and
objective metrics against state-of-the-art methods <span
class="citation" data-cites="zhang_2018 paintschainer_2018 ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>, <a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>, <a
href="#ref-zhang_2018" role="doc-biblioref">88</a>]</span> on our <span
class="math inline">\(3,545\)</span> curated test set centerd cropped to
<span class="math inline">\(512 \times 512\)</span> images.</p>
<p><strong>Baseline:</strong> We evaluate PaintsTorch against previous
work: PaintsChainer Zhang et al. <span class="citation"
data-cites="zhang_2018">[<a href="#ref-zhang_2018"
role="doc-biblioref">88</a>]</span>, PaintsChainer <span
class="citation" data-cites="paintschainer_2018">[<a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>]</span>, and
Ci et al. <span class="citation" data-cites="ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>]</span> who has made
their code available and thus helped in the reproduction of their
work.</p>
<p><strong>Objective Evaluation:</strong> Contrary to previous work on
GANs and as stated by Ci et al. <span class="citation"
data-cites="ci_2018">[<a href="#ref-ci_2018"
role="doc-biblioref">14</a>]</span>, we do not evaluate the Peak
Signal-to-Noise Ratio (PSNR) which does not asses joint statistics
between the target and generated colored illustrations.</p>
<p>We instead evaulate the Fréchet Inception Distance (FID) of our
generated illustration against the targets. The FID measures the
intra-class dropping, diversity, and quality. A small value means that
the two distributions compared are similar. The results of the FID
evaluation are shown in <a href="#tbl:core_pt_fid">Tbl 1</a>. Our model
PaintsTorch outperform previous work with and without hint
conditioning.</p>
<div id="tbl:core_pt_fid">
<table style="width:78%;">
<caption>Table 1: Fréchet Inception Distance (FID) benchmark comparing
our work PaintsTorch against previous from Zhang et al. <span
class="citation" data-cites="zhang_2018">[<a href="#ref-zhang_2018"
role="doc-biblioref">88</a>]</span>, PaintsChainer <span
class="citation" data-cites="paintschainer_2018">[<a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>]</span>, and
Ci et al. <span class="citation" data-cites="ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>]</span>. Two
configurations are used, no hints, and hints to evaluate the models in
both conditions.</caption>
<colgroup>
<col style="width: 26%" />
<col style="width: 18%" />
<col style="width: 16%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: right;">No Hints</th>
<th style="text-align: right;">Hints</th>
<th style="text-align: right;">Mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><em>FID</em> <span
class="math inline">\(\downarrow\)</span></td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;">Zhang et al.</td>
<td style="text-align: right;">134.06</td>
<td style="text-align: right;">274.87</td>
<td style="text-align: right;">204.47</td>
</tr>
<tr class="odd">
<td style="text-align: left;">PaintsChainer</td>
<td style="text-align: right;">54.97</td>
<td style="text-align: right;">99.63</td>
<td style="text-align: right;">77.30</td>
</tr>
<tr class="even">
<td style="text-align: left;">Ci et al.</td>
<td style="text-align: right;">52.48</td>
<td style="text-align: right;">96.22</td>
<td style="text-align: right;">74.35</td>
</tr>
<tr class="odd">
<td style="text-align: left;">PaintsTorch</td>
<td style="text-align: right;"><strong>51.54</strong></td>
<td style="text-align: right;"><strong>95.71</strong></td>
<td style="text-align: right;"><strong>73.63</strong></td>
</tr>
</tbody>
</table>
</div>
<p>We additionally evaluate our work on a second feature-based metric,
Learned Perceptual Image Patch Similarity (LPIPS) measuring the
similarity of two images in feature space using different winodw sizes
and the features of an ImageNet <span class="citation"
data-cites="deng_2009">[<a href="#ref-deng_2009"
role="doc-biblioref">16</a>]</span> pretrained network such as VGG <span
class="citation" data-cites="simonyan_2014">[<a
href="#ref-simonyan_2014" role="doc-biblioref">74</a>]</span>. The
results shown in <a href="#tbl:core_pt_lpips">Tbl 2</a> shows that our
model is able to compete with previous work and produces relatively
better results with no hints.</p>
<div id="tbl:core_pt_lpips">
<table style="width:75%;">
<caption>Table 2: Learned Perceptual Image Patch Similarity (LPIPS)
benchmark comparing our work PaintsTorch against previous from Zhang et
al. <span class="citation" data-cites="zhang_2018">[<a
href="#ref-zhang_2018" role="doc-biblioref">88</a>]</span>,
PaintsChainer <span class="citation" data-cites="paintschainer_2018">[<a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>]</span>, and
Ci et al. <span class="citation" data-cites="ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>]</span>. Two
configurations are used, no hints, and hints to evaluate the models in
both conditions.</caption>
<colgroup>
<col style="width: 29%" />
<col style="width: 15%" />
<col style="width: 15%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: right;">No Hints</th>
<th style="text-align: right;">Hints</th>
<th style="text-align: right;">Mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><em>LPIPS</em> <span
class="math inline">\(\downarrow\)</span></td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;">Zhang et al.</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;"><strong>0.46</strong></td>
<td style="text-align: right;"><strong>0.37</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;">PaintsChainer</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.71</td>
<td style="text-align: right;">0.50</td>
</tr>
<tr class="even">
<td style="text-align: left;">Ci et al.</td>
<td style="text-align: right;">0.23</td>
<td style="text-align: right;">0.62</td>
<td style="text-align: right;">0.43</td>
</tr>
<tr class="odd">
<td style="text-align: left;">PaintsTorch</td>
<td style="text-align: right;"><strong>0.18</strong></td>
<td style="text-align: right;">0.59</td>
<td style="text-align: right;">0.39</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Subjective Evaluation:</strong> As stated in the methodology
chapter (see Sec <strong>¿sec:methodology?</strong>) a subjective
evaluation is required due to the nature of the task we are trying to
solve. We thus perform a MOS using the population of study described
earlier (see Sec <strong>¿sec:methodology?</strong>). The results (see
<a href="#tbl:core_pt_mos">Tbl 3</a>) show that our model PaintsTorch
produces colored images with better perceptual qualities.</p>
<div id="tbl:core_pt_mos">
<table>
<caption>Table 3: Mean Opinion Score (MOS) benchmark comparing our work
PaintsTorch against previous from Zhang et al. <span class="citation"
data-cites="zhang_2018">[<a href="#ref-zhang_2018"
role="doc-biblioref">88</a>]</span>, PaintsChainer <span
class="citation" data-cites="paintschainer_2018">[<a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>]</span>, and
Ci et al. <span class="citation" data-cites="ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>]</span>. The <span
class="math inline">\(t\)</span>-test <span
class="math inline">\(p\)</span>-values for the mean is provided for
every comparison.</caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 24%" />
<col style="width: 24%" />
<col style="width: 32%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: right;">MOS <span
class="math inline">\(\uparrow\)</span></th>
<th style="text-align: right;">STD <span
class="math inline">\(\uparrow\)</span></th>
<th style="text-align: right;">p-value <span
class="math inline">\(\downarrow\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Zhang et al.</td>
<td style="text-align: right;">1.79</td>
<td style="text-align: right;">0.51</td>
<td style="text-align: right;">6.04<span
class="math inline">\(e^{-23}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">PaintsChainer</td>
<td style="text-align: right;">2.18</td>
<td style="text-align: right;">0.56</td>
<td style="text-align: right;">7.72<span
class="math inline">\(e^{-18}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Ci et al.</td>
<td style="text-align: right;">2.83</td>
<td style="text-align: right;">0.67</td>
<td style="text-align: right;">9.84<span
class="math inline">\(e^{-08}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">PaintsTorch</td>
<td style="text-align: right;"><strong>3.05</strong></td>
<td style="text-align: right;"><strong>0.42</strong></td>
<td style="text-align: right;"><strong>9.15<span
class="math inline">\(e^{-09}\)</span></strong></td>
</tr>
</tbody>
</table>
</div>
<h4 id="sec:IV.2.3.2">Visual Qualities</h4>
<figure id="fig:core_pt_comparison">
<img src="./figures/core_pt_comparison.png"
alt="Comparison of PaintsChainer [58] on top and our contribution PaintsTorch bottom. The comparison shows that our method is more robust to variable colored strokes leading to less bleeding, and an overall better look in the generatoed output." />
<figcaption>Figure 56: Comparison of PaintsChainer <span
class="citation" data-cites="paintschainer_2018">[<a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>]</span> on
top and our contribution PaintsTorch bottom. The comparison shows that
our method is more robust to variable colored strokes leading to less
bleeding, and an overall better look in the generatoed
output.</figcaption>
</figure>
<p>The differences in our approach PaintsTorch results in visible
improvements in comparison to previous work. Our work is more robust to
messy inputs. The examples shown in <a
href="#fig:core_pt_comparison">Fig 56</a> shows the benefits of the
introduction of the use of synthetic color scribbles during training in
opposition to the previously used random pixel activations. Thanks to
this change, PaintsTorch is less prone to color bleeding, is able to
fill color better and produces clean gradients as shown in <a
href="#fig:core_pt_bleeding">Fig 57</a>.</p>
<figure id="fig:core_pt_bleeding">
<img src="./figures/core_pt_bleeding.png"
alt="Comparison of previous work from Ci et al. [14] on top and our contribution PaintsTorch bottom using a toy lineart coloring example. Our model is more robust to messy colored strokes, is more consistent with color filling and gradients." />
<figcaption>Figure 57: Comparison of previous work from Ci et al. <span
class="citation" data-cites="ci_2018">[<a href="#ref-ci_2018"
role="doc-biblioref">14</a>]</span> on top and our contribution
PaintsTorch bottom using a toy lineart coloring example. Our model is
more robust to messy colored strokes, is more consistent with color
filling and gradients.</figcaption>
</figure>
<p>Samples generated using our method are shown in <a
href="#fig:core_pt_samples">Fig 58</a>. PaintsTorch can generate
qualitative colored illustration from a single lineart, and a hint map
filled with colored strokes representing the intent of its user. The
hints are localized and mostly reflected in the final output.</p>
<figure id="fig:core_pt_samples">
<img src="./figures/core_pt_samples.jpeg"
alt="Samples of colored illustration generated with PaintsTorch. For each illustration, lineart is one the right, the hint map in the middle, and the generated illustration on the right. The hint maps’ strokes have been produced by hand." />
<figcaption>Figure 58: Samples of colored illustration generated with
PaintsTorch. For each illustration, lineart is one the right, the hint
map in the middle, and the generated illustration on the right. The hint
maps’ strokes have been produced by hand.</figcaption>
</figure>
<h4 id="sec:IV.2.3.3">Application</h4>
<p>A custom web application has been built to explore the use of
PaintsTorch as a tool inspired by modern digital drawing worflows. A
screenshot of the application is shown <a
href="#fig:core_pt_app">Fig 59</a>. The model first trained using the
PyTorch framework is then exported to TensorFlowJS using the
intermediate and universal ONNX format. This allows us to serve the web
application as a standalone static web page and make use of the GPU from
a web browser to allow for real-time interaction with our model
PaintsTorch. The application propose to interact with the model by first
providing a lineart using the file tool bar available on the right side,
populate the hint map using a brush and tool options made available on
the left side tool bar as well as seeing the resulting generated colored
illustration in realtime and save it using the file icons.</p>
<p>Our model PaintsTorch integrates naturally into the modern digital
illustration production workflow as it uses similar tools and can be
futher inhanced using the user favorite pipeline. An example of such
workflow is shown in <a href="#fig:core_pt_workflow">Fig 60</a>.</p>
<figure id="fig:core_pt_app">
<img src="./figures/core_pt_app.jpeg"
alt="The figure is a screenshot of our web application. The model, PaintsTorch, is exported using the ONNX framework for TesnorflowJS and deployed in the browser as a standalone static page. The web app is built to follow the digital drawing tools and aesthetic enabling the use of different tools on the left-side bar such as a brush with various sizes, a color picker and color wheel, a drawing canvas and a result canvas. The generated output can be saved using the file menu on the right-side bar." />
<figcaption>Figure 59: The figure is a screenshot of our web
application. The model, PaintsTorch, is exported using the ONNX
framework for TesnorflowJS and deployed in the browser as a standalone
static page. The web app is built to follow the digital drawing tools
and aesthetic enabling the use of different tools on the left-side bar
such as a brush with various sizes, a color picker and color wheel, a
drawing canvas and a result canvas. The generated output can be saved
using the file menu on the right-side bar.</figcaption>
</figure>
<figure id="fig:core_pt_workflow">
<img src="./figures/core_pt_workflow.png"
alt="Illustration of a natural workflow using PaintsTorch as a bootstrapping tool for producing high quality illustration. Starting left, the first image is the lineart, next the hint map, then the generated illustration, then corrections and adjustement added by the artist, and further refined in the final image." />
<figcaption>Figure 60: Illustration of a natural workflow using
PaintsTorch as a bootstrapping tool for producing high quality
illustration. Starting left, the first image is the lineart, next the
hint map, then the generated illustration, then corrections and
adjustement added by the artist, and further refined in the final
image.</figcaption>
</figure>
<h4 id="sec:IV.2.3.4">Limitations</h4>
<p>While PaintsTorch allow the creation of qualitative colored
illustration with minimal work, our approache presents limitations. Our
model is still subject to the generation of visual artificats that needs
to be cleaned by the end-user for proper use. They tend to appear more
when the hint map is dense and contains highly saturated colors as shown
in <a href="#fig:core_pt_artifacts">Fig 61</a>. The model is also
limited by the dataset used for training. A cleaner and more varied
dataset would certainly result in even better outputs as our minimal
quality filtering already shows in comparison to the used of unfiltered
data from previous contributions.</p>
<figure id="fig:core_pt_artifacts">
<img src="./figures/core_pt_artifacts.png"
alt="The illustration shows the appearance of artifacts when our model PaintsTorch is used with highly saturated and dense colored strokes." />
<figcaption>Figure 61: The illustration shows the appearance of
artifacts when our model PaintsTorch is used with highly saturated and
dense colored strokes.</figcaption>
</figure>
<h3 id="sec:IV.2.4">Summary</h3>
<p>User-guided lineart colorization is a challenging task for CV.
Previous work introduced the use of the GAN architecture and shows that
DL methods yield better results than previous methods. In this work,
PaintsTorch <span class="citation" data-cites="hati_2019">[<a
href="#ref-hati_2019" role="doc-biblioref">29</a>]</span>, we porpose
three changes to the setup proposed by Ci et al. <span class="citation"
data-cites="ci_2018">[<a href="#ref-ci_2018"
role="doc-biblioref">14</a>]</span> and demonstrate their impact through
an objective and subjective evaluation. We first introduce the use of
simulated strokes as a replacement for the usual random pixel activation
scheme porposed by previous work. This change positively impact the
interaction with the model by making it robust to messy input strokes
and improve the overall quality of the produced illustrations. Our
second contribution is the use of a custom high resolution dataset
filtered with a qualitative objective. In our paper PaintsTorch <span
class="citation" data-cites="hati_2019">[<a href="#ref-hati_2019"
role="doc-biblioref">29</a>]</span>, we demonstrate the impact of the
data quality on the objective metrics. Our third contribution is the
exploration of the use of a second generator producing synthetic lineart
from the generated illustration from the first generation to enforce a
cycle consistency signal.</p>
<p>PaintsTorch is able to generate high quality colored illustrations
given a lineart and a coarse hint map made of user-defined color
strokes. However, our model is still subject to visual artificats,
especially when provided with highly saturated and dense colored
strokes. Is also struggles at representing the user intent when hinting
for small details in the lineart.</p>
<h2 id="sec:IV.3">StencitTorch: Human-Machine Colaboration Summary</h2>
<figure id="fig:core_st_teaser">
<img src="./figures/core_st_teaser.png"
alt="The figure is a screenshot of our model StencilTorch [30] deployed in a digital drawing web applicaton on top, and a photo of a user interacting with the application bottom. The left canavs is used to provide a mask, the middle canavas the colored strokes hint map, and the right canvas displayes the result illustration generated by the model." />
<figcaption>Figure 62: The figure is a screenshot of our model
StencilTorch <span class="citation" data-cites="hati_2023">[<a
href="#ref-hati_2023" role="doc-biblioref">30</a>]</span> deployed in a
digital drawing web applicaton on top, and a photo of a user interacting
with the application bottom. The left canavs is used to provide a mask,
the middle canavas the colored strokes hint map, and the right canvas
displayes the result illustration generated by the model.</figcaption>
</figure>
<h3 id="sec:IV.3.1">Introduction</h3>
<p>This chapter discusses our work on StencilTorch <span
class="citation" data-cites="hati_2023">[<a href="#ref-hati_2023"
role="doc-biblioref">30</a>]</span>, an iterative and user-guided
framework for anime lineart colorization (see <a
href="#fig:core_st_teaser">Fig 62</a>). Previous work <span
class="citation"
data-cites="zhang_2018 paintschainer_2018 petalicapaint_2023 ci_2018 hati_2019">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>, <a
href="#ref-hati_2019" role="doc-biblioref">29</a>, <a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>, <a
href="#ref-petalicapaint_2023" role="doc-biblioref">62</a>, <a
href="#ref-zhang_2018" role="doc-biblioref">88</a>]</span> introduced
the use of cWGAN-GP and have demonstrated unprecendant lineart
colorization capabilities guided by user strokes and conditionned on a
local feature extractor called Illustration2Vec <span class="citation"
data-cites="saito_2015">[<a href="#ref-saito_2015"
role="doc-biblioref">70</a>]</span> to compensate for the lack of
semantic information provided by a black and white lineart with no
lighting nor texture.</p>
<p>Previous methods consist of a one-step process where the user is
involved one time only, at the beginning of the process when asked to
provide a hint map populated with colored bursh strokes. This type of
pipeline is limited and is not ideal in the context of creation where th
artist needs to iterate and explore its design space. In this work, we
propose to give the power back to the user by formulating the task of
automatic lineart colorization as a human-in-the-loop process where the
end-user collaborates with the machine to produce the final colored
illustration.</p>
<figure id="fig:core_st_workflow">
<img src="./figures/core_st_workflow.png"
alt="The schematic illustrate StencilTorch workflows enabled by the in-painting forimation of the automatic linear colorization task. The artist is able to explores a potential colorization for its lineart on the left, and explore an alternative colorization on the right after introducing additional shading and lighting manually." />
<figcaption>Figure 63: The schematic illustrate StencilTorch workflows
enabled by the in-painting forimation of the automatic linear
colorization task. The artist is able to explores a potential
colorization for its lineart on the left, and explore an alternative
colorization on the right after introducing additional shading and
lighting manually.</figcaption>
</figure>
<p>Our framework, StencilTorch <span class="citation"
data-cites="hati_2023">[<a href="#ref-hati_2023"
role="doc-biblioref">30</a>]</span>, is motivated by human workflows and
is a follow-up to our previous contribution PaintsTorch <span
class="citation" data-cites="hati_2019">[<a href="#ref-hati_2019"
role="doc-biblioref">29</a>]</span>. We train a cWGAN-GP to generate
colored illustrations for a given lineart by conditionning the model on
local features, synthetic color hints produced by simulated strokes, and
an additional mask describing the area of the image to color. We thus
reformulate our task as an in-painting problem. This shift allows the
emergence of iterative and collaborative workflows between the user and
the machine where the output of a first step can become the input of a
second. An example of interaction with the system is shown in <a
href="#fig:core_st_workflow">Fig 63</a>.</p>
<p>In this work, StencilTorch <span class="citation"
data-cites="hati_2023">[<a href="#ref-hati_2023"
role="doc-biblioref">30</a>]</span>, our contributions are the
following:</p>
<ul>
<li>We present a new synthetic pipeline for generating colored strokes
from prepeprocessed illustrations from which the texture, and lighting
information are removed. This processed uses semantic color segmentation
and aims at producing better hints for training.</li>
<li>StencilTorch, a cWGAN-GP trained to produce illustrations from a
given lineart, a local semantic feature vector, a colored hint map, and
a mask enabling emergent collaborative workflows. We additionally guide
the training with a guide network to help recovering flat colorization
in the early layers of the network.</li>
<li>We evaluate our method against previous work using both objective
and subjective evaluation metrics.</li>
<li>An interactive web application allowing users to naturally interact
with our model by providing a linear, drawing scribble colored lines and
a mask.</li>
</ul>
<h3 id="sec:IV.3.2">Method</h3>
<p>This section discusses the data generation pipeline used in
StencilTorch <span class="citation" data-cites="hati_2023">[<a
href="#ref-hati_2023" role="doc-biblioref">30</a>]</span> (see
Sec <strong>¿sec:st_synthetic?</strong>), the Conditional Wasserstein
Generative Adversarial Network with Gradient Penalty (cWGAN-GP) (see
Sec <strong>¿sec:st_arch?</strong>) model architecture and its guide
network, the objective functions we aim to optimize (see
Sec <strong>¿sec:st_losses?</strong>); and the training regime (see
Sec <strong>¿sec:st_train?</strong>).</p>
<h4 id="sec:IV.3.2.1">Synthetic Inputs</h4>
<p>One of the challenges for the task of automatic lineart colorization
is the lack of qualitative public datasets. In our previous work on
PaintsTorch <span class="citation" data-cites="hati_2019">[<a
href="#ref-hati_2019" role="doc-biblioref">29</a>]</span> we demonstrate
the importance of the quality of the dataset and its impact on the
overall perceptual quality of the images produced by the generative
models. We thus resused our custom curated filtered dataset and employed
a similar data transformation pipeline with additional improvements
explained in <a href="#fig:core_st_pipeline">Fig 64</a>.</p>
<figure id="fig:core_st_pipeline">
<img src="./figures/core_st_pipeline.png"
alt="StencilTorch synthetic input generation process diagram. The illustration sampled form our custom dataset is first transformeed into a synthetic lineart using the xDoG method. The same illustration is simplified to partially remove texture and shadow information using a semantic color segmentation network, color quantization, and k-means to further reduce th number of colors present in the image. The selection mask and the color strokes are then generated from this simplified segmented illustration. Finally, a composite input is aggregated for inpainting where the black region of the mask contains the original illustration, and the white region, the synthetic lineart to be colorized." />
<figcaption>Figure 64: StencilTorch synthetic input generation process
diagram. The illustration sampled form our custom dataset is first
transformeed into a synthetic lineart using the xDoG method. The same
illustration is simplified to partially remove texture and shadow
information using a semantic color segmentation network, color
quantization, and <span class="math inline">\(k\)</span>-means to
further reduce th number of colors present in the image. The selection
mask and the color strokes are then generated from this simplified
segmented illustration. Finally, a composite input is aggregated for
inpainting where the black region of the mask contains the original
illustration, and the white region, the synthetic lineart to be
colorized.</figcaption>
</figure>
<p>Randomly sampling color strokes from the input illustration means
that the similated random strokes capture the variation of color present
in the image containing shadow, lighting, texture and more. However,
based on our previous observations from interactons with our previous
work PaintsTorch <span class="citation" data-cites="hati_2019">[<a
href="#ref-hati_2019" role="doc-biblioref">29</a>]</span>, the end-user
tend to specify mid-tone colors whithout thinking about lighting as they
expect the model to handle that aspect. In that regarde, we developped a
new preprocessing pipeline for the illustration enabling the extraction
and generation of color scribbles with limited shadow and texture
information.</p>
<p>To this end, we trained a ResNet <span class="citation"
data-cites="he_2016">[<a href="#ref-he_2016"
role="doc-biblioref">31</a>]</span> mode to regress displacement maps
from colored illustrations on the DabooRegion dataset <span
class="citation" data-cites="danboo_region_2020">[<a
href="#ref-danboo_region_2020" role="doc-biblioref">87</a>]</span>. The
displacement map is robust to noise and allows us to extract unique
color sections. We further assign each region to its median color,
further reduce the number of colors using color quantization and further
refine them down to <span class="math inline">\(25\)</span> colors using
<span class="math inline">\(k\)</span>-means clustering. This number has
empirically be selected to avoid sacrificing too much details from the
original illustration. The mask and color hints inputs are then
generated from this simplified version of the input illustration.</p>
<h4 id="sec:IV.3.2.2">Model Architecture</h4>
<figure id="fig:core_st_architecture">
<img src="./figures/core_st_architecture.svg"
alt="StencilTorch Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) architecture schematic." />
<figcaption>Figure 65: StencilTorch Wasserstein Generative Adversarial
Network with Gradient Penalty (WGAN-GP) architecture
schematic.</figcaption>
</figure>
<p>The model architecture of StencilTorch is similar to PaintsTorch
<span class="citation" data-cites="hati_2019">[<a href="#ref-hati_2019"
role="doc-biblioref">29</a>]</span> at the exception that the cycle
consistency lineart generator is removed and replaced by a guide network
(see <a href="#fig:core_st_architecture">Fig 65</a>). As we reformulate
the task of automatic colorization as in-painting, we found the this
second generator is not required and does not bring additional value to
StencilTorch.</p>
<p>We however introduce a secondary network which is a copy of the
WGAN-GP decoder section of the generator inspired by previous work from
Zhang et al. <span class="citation" data-cites="zhang_ji_2017">[<a
href="#ref-zhang_ji_2017" role="doc-biblioref">86</a>]</span>. This
secondary network is activated during training only and is trained to
reconstruct the simplified illustration using the latent code provided
by the encoder section of the generator. By introducing this network to
StencilTorch we aim at helping the rest of the generator to produce
disantangled inner representations and thus improved the final output
colored illustration.</p>
<h4 id="sec:IV.3.2.3">Objective Functions</h4>
<p>Similarly to PaintsTorch <span class="citation"
data-cites="hati_2019">[<a href="#ref-hati_2019"
role="doc-biblioref">29</a>]</span>, StencilTorch is trained end-to-end
following the Wasserstein Generative Adversarial Network with Gradient
Penalty (WGAN-GP) objectives described in
Sec <strong>¿sec:pt_losses?</strong> with minimal changes, the
reconstruction loss is replaced by a guidance signal:</p>
<p><span id="eq:core_st_objectives"><span class="math display">\[
\begin{aligned}
L_G = \lambda_{adv} \cdot L_{adv} + L_{cont} + L_{guided} \\
L_C = L_{w} + L_{gp}
\end{aligned}
\qquad{(36)}\]</span></span></p>
<p>where:</p>
<p><span id="eq:core_st_guide"><span class="math display">\[
L_{guided} = \frac{1}{chw} ||\hat{s} - s||_2^2
\qquad{(37)}\]</span></span></p>
<p>with <span class="math inline">\(\hat{s}\)</span> being the
reconstructed simplified illustration described in the data generation
pipeline, and <span class="math inline">\(s\)</span> beign the
simplified illustration. We later found that this loss should be
weighted down as being easier to optimize in comparison to the
adversarial and content losses resulting in the lack of lighting and
texture generate by StencilTorch when the network is asked to color an
entire lineart in one pass.</p>
<h4 id="sec:IV.3.2.4">Training</h4>
<p>The model is trained end-to-end using the Adam optimizer with a
learning rate <span class="math inline">\(\epsilon = 1e^{-4}\)</span>
and beta parameters <span class="math inline">\(\beta_1 = 0.5\)</span>
and <span class="math inline">\(\beta_2 = 0.9\)</span>. They are trained
for <span class="math inline">\(40\)</span> epochs using a batch size of
<span class="math inline">\(32\)</span> on each of the four GPUs during
<span class="math inline">\(24\)</span> hours straight.</p>
<h3 id="sec:IV.3.3">Results</h3>
<p>In this section we present the objective and subjective evaluations
(see Sec <strong>¿sec:sec:core_st_eval?</strong>), perform a qualitative
visual analysis of the strengh and limitations of our contribution (see
Sec <strong>¿sec:sec_core_st_limit?</strong>).</p>
<h4 id="sec:IV.3.3.1">Evaluation</h4>
<p>Our model StencilTorch is evaluated using subjective metrics, and
objective metrics against state-of-the-art methods <span
class="citation"
data-cites="zhang_2018 paintschainer_2018 ci_2018 hati_2019">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>, <a
href="#ref-hati_2019" role="doc-biblioref">29</a>, <a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>, <a
href="#ref-zhang_2018" role="doc-biblioref">88</a>]</span> on our <span
class="math inline">\(3,545\)</span> curated test set centerd cropped to
<span class="math inline">\(512 \times 512\)</span> images.</p>
<p><strong>Baseline:</strong> We evaluate StencilTorch against previous
work: PaintsChainer Zhang et al. <span class="citation"
data-cites="zhang_2018">[<a href="#ref-zhang_2018"
role="doc-biblioref">88</a>]</span>, PaintsChainer <span
class="citation" data-cites="paintschainer_2018">[<a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>]</span>, Ci
et al. <span class="citation" data-cites="ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>]</span> who has made
their code available and thus helped in the reproduction of their work,
and our previous contribution PaintsTorch <span class="citation"
data-cites="hati_2019">[<a href="#ref-hati_2019"
role="doc-biblioref">29</a>]</span>.</p>
<p><strong>Objective Evaluation:</strong> We evaulate the Fréchet
Inception Distance (FID) of our generated illustration against the
targets. The FID measures the intra-class dropping, diversity, and
quality. A small value means that the two distributions compared are
similar. The results of the FID evaluation are shown in <a
href="#tbl:core_st_fid">Tbl 4</a>. Our model StencilTorch outperform
previous work with various amount of hint conditioning.</p>
<div id="tbl:core_st_fid">
<table style="width:100%;">
<caption>Table 4: Fréchet Inception Distance (FID) benchmark comparing
our work StencilTorch against previous from Zhang et al. <span
class="citation" data-cites="zhang_2018">[<a href="#ref-zhang_2018"
role="doc-biblioref">88</a>]</span>, PaintsChainer <span
class="citation" data-cites="paintschainer_2018">[<a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>]</span>, Ci
et al. <span class="citation" data-cites="ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>]</span> and our previous
work PaintsTorch <span class="citation" data-cites="hati_2019">[<a
href="#ref-hati_2019" role="doc-biblioref">29</a>]</span>. Three
configurations are used, no hints, hints, and full hint where the
simplified illustration is used as hints to evaluate the models in all
conditions. The “+G” mention is our model StencilTorch with the
additional guide network.</caption>
<colgroup>
<col style="width: 26%" />
<col style="width: 18%" />
<col style="width: 16%" />
<col style="width: 19%" />
<col style="width: 19%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: right;">No Hints</th>
<th style="text-align: right;">Hints</th>
<th style="text-align: right;">Full Hints</th>
<th style="text-align: right;">Mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><em>FID</em> <span
class="math inline">\(\downarrow\)</span></td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;">Zhang et al.</td>
<td style="text-align: right;">134.06</td>
<td style="text-align: right;">274.87</td>
<td style="text-align: right;">242.58</td>
<td style="text-align: right;">245.33</td>
</tr>
<tr class="odd">
<td style="text-align: left;">PaintsChainer</td>
<td style="text-align: right;">54.97</td>
<td style="text-align: right;">99.63</td>
<td style="text-align: right;">112.16</td>
<td style="text-align: right;">93.02</td>
</tr>
<tr class="even">
<td style="text-align: left;">Ci et al.</td>
<td style="text-align: right;">52.48</td>
<td style="text-align: right;">96.22</td>
<td style="text-align: right;">106.73</td>
<td style="text-align: right;">85.14</td>
</tr>
<tr class="odd">
<td style="text-align: left;">PaintsTorch</td>
<td style="text-align: right;">51.54</td>
<td style="text-align: right;">95.71</td>
<td style="text-align: right;">98.37</td>
<td style="text-align: right;">81.87</td>
</tr>
<tr class="even">
<td style="text-align: left;">StencilTorch</td>
<td style="text-align: right;"><strong>51.16</strong></td>
<td style="text-align: right;">94.40</td>
<td style="text-align: right;">106.05</td>
<td style="text-align: right;"><strong>81.63</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;">StencilTorch + G</td>
<td style="text-align: right;">85.00</td>
<td style="text-align: right;"><strong>91.60</strong></td>
<td style="text-align: right;"><strong>93.80</strong></td>
<td style="text-align: right;">89.98</td>
</tr>
</tbody>
</table>
</div>
<p>We additionally evaluate our work on a second feature-based metric,
LPIPS measuring the similarity of two images in feature space using
different winodw sizes and the features of an ImageNet <span
class="citation" data-cites="deng_2009">[<a href="#ref-deng_2009"
role="doc-biblioref">16</a>]</span> pretrained network such as VGG <span
class="citation" data-cites="simonyan_2014">[<a
href="#ref-simonyan_2014" role="doc-biblioref">74</a>]</span>. The
results shown in <a href="#tbl:core_st_lpips">Tbl 5</a> shows that our
model is able to compete with previous work and produces relatively
better results with no hints.</p>
<div id="tbl:core_st_lpips">
<table>
<caption>Table 5: Learned Perceptual Image Patch Similarity (LPIPS)
benchmark comparing our work StencilTorch against previous from Zhang et
al. <span class="citation" data-cites="zhang_2018">[<a
href="#ref-zhang_2018" role="doc-biblioref">88</a>]</span>,
PaintsChainer <span class="citation" data-cites="paintschainer_2018">[<a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>]</span>, Ci
et al. <span class="citation" data-cites="ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>]</span> and our previous
work PaintsTorch <span class="citation" data-cites="hati_2019">[<a
href="#ref-hati_2019" role="doc-biblioref">29</a>]</span>. Three
configurations are used, no hints, hints, and full hint where the
simplified illustration is used as hints to evaluate the models in all
conditions. The “+G” mention is our model StencilTorch with the
additional guide network.</caption>
<colgroup>
<col style="width: 30%" />
<col style="width: 17%" />
<col style="width: 15%" />
<col style="width: 18%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: right;">No Hints</th>
<th style="text-align: right;">Hints</th>
<th style="text-align: right;">Full Hints</th>
<th style="text-align: right;">Mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><em>LPIPS</em> <span
class="math inline">\(\downarrow\)</span></td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">-</td>
</tr>
<tr class="even">
<td style="text-align: left;">Zhang et al.</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;"><strong>0.46</strong></td>
<td style="text-align: right;"><strong>0.26</strong></td>
<td style="text-align: right;"><strong>0.37</strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;">PaintsChainer</td>
<td style="text-align: right;">0.28</td>
<td style="text-align: right;">0.71</td>
<td style="text-align: right;">0.60</td>
<td style="text-align: right;">0.54</td>
</tr>
<tr class="even">
<td style="text-align: left;">Ci et al.</td>
<td style="text-align: right;">0.23</td>
<td style="text-align: right;">0.62</td>
<td style="text-align: right;">0.59</td>
<td style="text-align: right;">0.48</td>
</tr>
<tr class="odd">
<td style="text-align: left;">PaintsTorch</td>
<td style="text-align: right;">0.18</td>
<td style="text-align: right;">0.59</td>
<td style="text-align: right;">0.56</td>
<td style="text-align: right;">0.44</td>
</tr>
<tr class="even">
<td style="text-align: left;">StencilTorch</td>
<td style="text-align: right;"><strong>0.16</strong></td>
<td style="text-align: right;">0.51</td>
<td style="text-align: right;">0.58</td>
<td style="text-align: right;">0.40</td>
</tr>
<tr class="odd">
<td style="text-align: left;">StencilTorch + G</td>
<td style="text-align: right;">0.31</td>
<td style="text-align: right;">0.50</td>
<td style="text-align: right;">0.58</td>
<td style="text-align: right;">0.46</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Subjective Evaluation:</strong> As stated in the methodology
chapter (see Sec <strong>¿sec:methodology?</strong>) a subjective
evaluation is required due to the nature of the task we are trying to
solve. We thus perform a MOS using the population of study described
earlier (see Sec <strong>¿sec:methodology?</strong>). The results (see
<a href="#tbl:core_st_mos">Tbl 6</a>) show that our model StencilTorch
produces colored images with better perceptual qualities in most
cases.</p>
<div id="tbl:core_st_mos">
<table>
<caption>Table 6: Mean Opinion Score (MOS) benchmark comparing our work
StencilTorch against previous from Zhang et al. <span class="citation"
data-cites="zhang_2018">[<a href="#ref-zhang_2018"
role="doc-biblioref">88</a>]</span>, PaintsChainer <span
class="citation" data-cites="paintschainer_2018">[<a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>]</span>, Ci
et al. <span class="citation" data-cites="ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">14</a>]</span> and our previous
work PaintsTorch <span class="citation" data-cites="hati_2019">[<a
href="#ref-hati_2019" role="doc-biblioref">29</a>]</span>. The <span
class="math inline">\(t\)</span>-test <span
class="math inline">\(p\)</span>-values for the mean is provided for
every comparison.</caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 24%" />
<col style="width: 24%" />
<col style="width: 32%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Model</th>
<th style="text-align: right;">MOS <span
class="math inline">\(\uparrow\)</span></th>
<th style="text-align: right;">STD <span
class="math inline">\(\uparrow\)</span></th>
<th style="text-align: right;">p-value <span
class="math inline">\(\downarrow\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Zhang et al.</td>
<td style="text-align: right;">1.79</td>
<td style="text-align: right;">0.51</td>
<td style="text-align: right;">6.04<span
class="math inline">\(e^{-23}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">PaintsChainer</td>
<td style="text-align: right;">2.18</td>
<td style="text-align: right;">0.56</td>
<td style="text-align: right;">7.72<span
class="math inline">\(e^{-18}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Ci et al.</td>
<td style="text-align: right;">2.83</td>
<td style="text-align: right;">0.67</td>
<td style="text-align: right;">9.84<span
class="math inline">\(e^{-08}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">PaintsTorch</td>
<td style="text-align: right;">3.05</td>
<td style="text-align: right;">0.42</td>
<td style="text-align: right;"><strong>9.15<span
class="math inline">\(e^{-09}\)</span></strong></td>
</tr>
<tr class="odd">
<td style="text-align: left;">StencilTorch</td>
<td style="text-align: right;"><strong>3.71</strong></td>
<td style="text-align: right;"><strong>0.28</strong></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
</div>
<h4 id="sec:IV.3.3.2">Visual Qualities</h4>
<figure id="fig:core_st_samples">
<img src="./figures/core_st_samples.png"
alt="Samples mosaic of input and output pairs generated using one pass with StencilTorch. In every cell of the mosaic, the lineart, the mask, and the hint map are shown in the bottom, and the generated colored illustration on top." />
<figcaption>Figure 66: Samples mosaic of input and output pairs
generated using one pass with StencilTorch. In every cell of the mosaic,
the lineart, the mask, and the hint map are shown in the bottom, and the
generated colored illustration on top.</figcaption>
</figure>
<p>Our model StencilTorch is able to generate useful and consistent
colored illustrations from linearts, color hints, and masks. Samples
generated by our methods are shown in <a
href="#fig:core_st_samples">Fig 66</a>. The use of masks allows the user
to specify the region of the image he wants to color and ensure there is
no bleeding outside of the delimited zone.</p>
<h4 id="sec:IV.3.3.3">Emerging Workflow</h4>
<p>As shown in the <a href="#fig:core_st_workflow">Fig 63</a> diagram,
natural iterative workflows emerge from the interaction of users with
our framework. The end-user can use StencilTorch to initiate a first
colored illustration and then reuse the generated image as input for a
second pass with another mask and different hints. The user also has the
possibility to refine the produced illustration using its own digital
art workflow and style and in-paint missing parts using StencilTorch.
The model can use the information already present and given by the user
outside of the mask to complete the rest of the illustration while
following the lighting and style provided by the user. These operations
can be repeated indefinitely and allow to interactively explore the
colorization design space by collaborating with the machine.</p>
<h4 id="sec:IV.3.3.4">Application</h4>
<p>Similarly to PaintsTorch <span class="citation"
data-cites="hati_2019">[<a href="#ref-hati_2019"
role="doc-biblioref">29</a>]</span>, we export our model from PyTorch to
TensorFlowJS using the ONNX universal framework and deploy the model in
a custom static and standalone web application. The web application
allows the user to import a lineart, create a mask, hints maps with
scribble colored lines, and visualize the resulting generated
illustration in realtime making use of the GPU from the web browser. A
screenshot of the application is shown in <a
href="#fig:core_st_teaser">Fig 62</a>.</p>
<h4 id="sec:IV.3.3.5">Limitations</h4>
<figure id="fig:core_st_comparison">
<img src="./figures/core_st_comparison.png"
alt="Comparison of StencilTorch, PaintsTorch [29], and PaintsChainer [58] from left to right given the same lineart and hint map. This one pass generation shows the limitation of our work. When not used with in-painting, StencilTorch tends to generate flat colorization although no artifacts is visible in comparison with the others." />
<figcaption>Figure 67: Comparison of StencilTorch, PaintsTorch <span
class="citation" data-cites="hati_2019">[<a href="#ref-hati_2019"
role="doc-biblioref">29</a>]</span>, and PaintsChainer <span
class="citation" data-cites="paintschainer_2018">[<a
href="#ref-paintschainer_2018" role="doc-biblioref">58</a>]</span> from
left to right given the same lineart and hint map. This one pass
generation shows the limitation of our work. When not used with
in-painting, StencilTorch tends to generate flat colorization although
no artifacts is visible in comparison with the others.</figcaption>
</figure>
<p>While our model StencilTorch produces qualitative and useful colored
illustrations from linearts and conditioning inputs, our model tends to
generate flat illustrations without depth and is lacking shadow and
texture especially when used in a one-pass fashion. Our model has been
designed to work iteratively with in-painting and works best in this
situation. A comparison of a one-pass coloring process is shown in <a
href="#fig:core_st_comparison">Fig 67</a>.</p>
<figure id="fig:core_st_minhints">
<img src="./figures/core_st_minhints.png"
alt="The figure shows the implication of the stroke density in the ability of our model StencilTorch to produce qualitative illustrations from a lineart, a mask and the hint map. The lineart is shown top left and the mask bottom left. The stroke density is increased from left to right with the hint map bottom and the generated illustration on top. The final top right image has been improved by an artist base on the last generated image." />
<figcaption>Figure 68: The figure shows the implication of the stroke
density in the ability of our model StencilTorch to produce qualitative
illustrations from a lineart, a mask and the hint map. The lineart is
shown top left and the mask bottom left. The stroke density is increased
from left to right with the hint map bottom and the generated
illustration on top. The final top right image has been improved by an
artist base on the last generated image.</figcaption>
</figure>
<p>The model is also sensible to the density of stroke used. A minimal
amount of user inputs needs to be provided for the model to generate
useful outputs as shown in <a
href="#fig:core_st_minhints">Fig 68</a>.</p>
<h3 id="sec:IV.3.4">Summary</h3>
<p>Our contribution StencilTorch <span class="citation"
data-cites="hati_2023">[<a href="#ref-hati_2023"
role="doc-biblioref">30</a>]</span> addresses the need for AI-driven
tools that naturally integrate into the artist workflow allowing fast
prototyping and iterative collaboration with the machine. While current
approaches have focused on improving the generation quality of
user-guided GAN architectures, we explored the use of in-painting to
enable natural emerging iterative workflows. The output of our model can
be used as the input of a second pass. We demonstrate that in particular
settings where the in-painting capabilities of our model shine, our
approach is able to generate qualitative colored illustrations for the
task of automatic lineart colorization.</p>
<p>While our approach finds its use in collaborative colorization, its
still suffers from a lack of depth in the output during a first pass
where the model is asked to color the entire lineart. This is certainly
due to the introduction of the guide network. Recovering flat
colorization is easier than coloring the lineart with shadow and
texture. The model would certainly perform better in this context if the
guidance component of the generator loss is pushed down in comparison to
the other terms.</p>
<!-- TODO: Here -->
<h2 id="sec:IV.4">StablePaint: Conditional Denoising Diffusion</h2>
<h3 id="sec:IV.4.1">Introduction</h3>
<h3 id="sec:IV.4.2">Method</h3>
<h4 id="sec:IV.4.2.1">Synthetic Inputs</h4>
<h4 id="sec:IV.4.2.2">Model Architecture</h4>
<h4 id="sec:IV.4.2.3">Objective Functions</h4>
<h4 id="sec:IV.4.2.4">Training</h4>
<h3 id="sec:IV.4.3">Intermediate Results</h3>
<figure id="fig:stablepaint_illustration_autoencoder">
<img src="./figures/stablepaint_illustration_autoencoder.png"
alt="The figure shows sample illustrations from our training set on the top row and their reconstruction using our illustration autoencoder. The illustration autoencoder is trained until saturation and is able to encode and decode the original signal with almost no perceptible difference." />
<figcaption>Figure 69: The figure shows sample illustrations from our
training set on the top row and their reconstruction using our
illustration autoencoder. The illustration autoencoder is trained until
saturation and is able to encode and decode the original signal with
almost no perceptible difference.</figcaption>
</figure>
<figure id="fig:stablepaint_lineart_autoencoder">
<img src="./figures/stablepaint_lineart_autoencoder.png"
alt="The figure shows sample illustrations from our training set on the top row and their reconstruction using our context autoencoder. The decoder is shared with our illustration autoencoder and is pretrained. The context encoder is trained until saturation and tries to project the given lineart and corresponding hints to a latent code that matches the illustration one when encoded with the illustration encoder." />
<figcaption>Figure 70: The figure shows sample illustrations from our
training set on the top row and their reconstruction using our context
autoencoder. The decoder is shared with our illustration autoencoder and
is pretrained. The context encoder is trained until saturation and tries
to project the given lineart and corresponding hints to a latent code
that matches the illustration one when encoded with the illustration
encoder.</figcaption>
</figure>
<figure id="fig:stablepaint_latent_noise_samples">
<img src="./figures/stablepaint_latent_noise_samples.png"
alt="The figure shows samples from our model StablePaint. The samples are generated starting from corrupted colored images from our training set. The corruption level is at 75% for the first two rows, 50% for the second row, 25% for the third row and 12.5% for the last one (top to bottom). The noise model can recover the altered signal to a certain extent by being conditioned on a lineart and color hints." />
<figcaption>Figure 71: The figure shows samples from our model
StablePaint. The samples are generated starting from corrupted colored
images from our training set. The corruption level is at 75% for the
first two rows, 50% for the second row, 25% for the third row and 12.5%
for the last one (top to bottom). The noise model can recover the
altered signal to a certain extent by being conditioned on a lineart and
color hints.</figcaption>
</figure>
<figure id="fig:stablepaint_demo_corruption">
<img src="./figures/stablepaint_demo_corruption.png" style="width:75.0%"
alt="The figure illustrate an example of input and output pair and the generation of colored illustrations using our model StablePaint. The noise model of Stable Paint is conditioning on the inputs and the initial noise is initialized from corrupted versions of latent representations of the inputs obtained using our pretrained context autoencoder. Various levels of corruption are applied to serve as the initial noise used by our diffusion process. The top row shows the result without any corruption, then 25%, 50%, 75%, and 100% corruption. Different runs with different noise corruption seeds are shown in the 4 columns. The input lineart and hints are shown at the very top.”" />
<figcaption>Figure 72: The figure illustrate an example of input and
output pair and the generation of colored illustrations using our model
StablePaint. The noise model of Stable Paint is conditioning on the
inputs and the initial noise is initialized from corrupted versions of
latent representations of the inputs obtained using our pretrained
context autoencoder. Various levels of corruption are applied to serve
as the initial noise used by our diffusion process. The top row shows
the result without any corruption, then 25%, 50%, 75%, and 100%
corruption. Different runs with different noise corruption seeds are
shown in the 4 columns. The input lineart and hints are shown at the
very top.”</figcaption>
</figure>
<h3 id="sec:IV.4.4">Summary</h3>
<!-- ===================== [END] PART CONTRIBUTIONS ===================== -->
<!-- ===================== [START] PART CONCLUSION ===================== -->
<h1 id="sec:V">Conclusion</h1>
<h2 id="sec:ethical-and-societal-impact">Ethical and Societal
Impact</h2>
<p>The use of Artificial Intelligence (AI) is becoming pervasive in our
environment and is impacting every domain. The creative industry is not
an exception. Recent advances in applied Deep Learning (DL) for art,
resumed as generative AI, are transforming the art sector. Thanks to
generative models such as Chat-GPT, DALL-E <span class="citation"
data-cites="openai_2023">[<a href="#ref-openai_2023"
role="doc-biblioref">11</a>]</span>, and open source initiatives such as
StableDiffusion <span class="citation" data-cites="rombach_2021">[<a
href="#ref-rombach_2021" role="doc-biblioref">66</a>]</span>, the
art-making process is more accessible than ever. By giving the user
means of control over the model conditioning, everybody can now generate
text, images, audio, and videos from natural language prompts.</p>
<figure id="fig:ref_artstation_cancel_ai">
<img src="./figures/ref_artstation_cancel_ai.png"
alt="Screenshot of the website Artstation used by artists to professionally expose their work. A quick exploration shows the appearance of anti AI pictures expressing the concerns of artists from AI companies exploiting their artwork for training their models for profit." />
<figcaption>Figure 73: Screenshot of the website Artstation used by
artists to professionally expose their work. A quick exploration shows
the appearance of anti AI pictures expressing the concerns of artists
from AI companies exploiting their artwork for training their models for
profit.</figcaption>
</figure>
<p>While such ability can present benefits in ways that are discussed
throughout this chapter, it also comes at a cost. Generative AI in the
art domain presents limitations in its usage but also presents
challenges questioning ethics and its impact on our society. Its use is
seen as controversial by most of the artist communities. While some may
see generative AI as a tool that enables increase creativity potential
and productivity others consider it as a Damocles sword, a threat for
their job (see <a href="#fig:ref_artstation_cancel_ai">Fig 73</a>).</p>
<p>This chapter discusses the benefits of generative AI models, their
ethical concerns and their impact on our society but also their
limitations and the need for regulations and guidelines.</p>
<h3 id="benefits">Benefits</h3>
<p>Generative AI brings a lot of benefits to the art industry. It can be
used to automate redundant tasks such as in-between frame colorization
in the digital animation domain and increase the productivity of the
artist by enabling faster exploration of variation art productions. The
artist can for example bootstrap an idea using natural prompts such as
text or image collages, and further refine it to explore a specific
design.</p>
<figure id="fig:ref_midjourney">
<img src="./figures/ref_midjourney.png"
alt="Sample from Midjourney’s generative AI using the prompt “the year 1203, the crowned Byzantine emperor watches as Venetian ships sail into the harbor of Constantinople, view over his shoulder, dramatic, photorealistic, details –ar 2:1”." />
<figcaption>Figure 74: Sample from Midjourney’s generative AI using the
prompt “the year 1203, the crowned Byzantine emperor watches as Venetian
ships sail into the harbor of Constantinople, view over his shoulder,
dramatic, photorealistic, details –ar 2:1”.</figcaption>
</figure>
<p>One of the most important benefits of generative AI tools comes from
their democratization. They enable individuals with no background in
creative arts, to express their creativity and generate artistic pieces
with natural interactions without requiring specific technical knowledge
(see Figs <a href="#fig:ref_midjourney">74</a>, <a
href="#fig:ref_hp">75</a>). It lowers down the barrier for newcomers and
lowers the cost of production. This aspect is especially important for
independent and small studios that do not have the means of big
production companies.</p>
<p>Here is a sampled list of use cases for image generative AI:</p>
<ul>
<li>Art directors can generate images to better explain their vision to
the production team using rough storyboards as a guideline for the
generative model.</li>
<li>Indie game studios can generate character turn tables using
descriptions from the character’s background and appearance.</li>
<li>Art museums can revive the art of classic artists such as Picasso,
Leonardo Da Vinci, or Van Gogh, and transfer their style to videos or
photographs.</li>
<li>Artists can generate thousands of variations from their artwork to
better match their vision and perform iterative exploration.</li>
<li>Individuals can use inpainting to remove or change part of their
photos.</li>
<li>Card games can be generated on the fly using procedural
artworks.</li>
<li>A UX designer can generate a set of website pages that correspond to
their vision before spending time to produce the actual mockup and
follow their standard workflow.</li>
<li>Indie movie studios could generate entire animated movies from video
footage at a low cost without requiring the help of professional 2D
animators.</li>
</ul>
<figure id="fig:ref_hp">
<img src="./figures/ref_hp.png"
alt="Portrait generated using various image generative AI models with a simplified description of the character Harry Potter from J.K. Rowling: “young boy portrait, lighting bolt scar on the forehead, thin face, black hair, green eyes, round black glasses held, black robe cape, magic castle in the background, trending in art station, 4k”. The models are having trouble with details such as the scar." />
<figcaption>Figure 75: Portrait generated using various image generative
AI models with a simplified description of the character Harry Potter
from J.K. Rowling: “young boy portrait, lighting bolt scar on the
forehead, thin face, black hair, green eyes, round black glasses held,
black robe cape, magic castle in the background, trending in art
station, 4k”. The models are having trouble with details such as the
scar.</figcaption>
</figure>
<h3 id="ethical-concerns">Ethical Concerns</h3>
<p>While such technologies can benefit the creative industry in many
ways, ethical problems such as the notion of ownership and authenticity
arise. As demonstrated throughout this thesis dissertation the most
common regimes used to train generative models are based on huge
quantities of scrapped data. Scrapping does not respect the ownership
and copyrighting policies of the content downloaded. While this practice
may be acceptable in the realm of education and research it is not the
case for commercial use or impersonation. This is even more problematic
as some training pictures can be leaked from inferred images which in
some cases resemble the training data too much so that it can be
affiliated as a fake copy of the original artwork. This has been
demonstrated by Carlini et al. <span class="citation"
data-cites="carlini_2023">[<a href="#ref-carlini_2023"
role="doc-biblioref">9</a>]</span> in their work on extracting training
data from diffusion models. Users can also replicate the style of
artists without their consent nor credit. This is extremely problematic
as it constitutes an art identity steal (see <a
href="#fig:ref_steal">Fig 76</a>).</p>
<figure id="fig:ref_steal">
<img src="./figures/ref_steal.jpg"
alt="Artwork by Hollie Mengert (left) vs. images generated with Stable Diffusion DreamBooth in her style (right). Credit: Andy BAIO from waxy.org." />
<figcaption>Figure 76: Artwork by Hollie Mengert (left) vs. images
generated with Stable Diffusion DreamBooth in her style (right). Credit:
Andy BAIO from waxy.org.</figcaption>
</figure>
<p>Because of their training process, most generative AI models are
subject to miss representation and bias if not part of their objective
or without extensive data cleaning and exploration. Some populations or
types of content may be miss represented by being undersampled during
training. Generative AI can also be used to produce harmful content. As
with every new piece of technology, miss uses can happen and the
population needs to be warned.</p>
<h3 id="societal-impact">Societal Impact</h3>
<p>With the democratization of the art-making process enabled by image
generative AI, human artists’ labor may be devaluated. Producing art can
be made faster, easier, and without the need for technically trained
artists at an unprecedented level of fidelity, quality, and quantity.
This emerging process may threaten the incomes of professional artists
that may struggle to keep up with such as rate.</p>
<p>This could also lead to the homogenization of artistic styles making
it hard for new artists to establish themselves. This is especially true
in the case of the combination of such tools with social media. One
could easily train a model to generate images that the majority of
people like and pollute the network with such produced content.</p>
<h3 id="limitations">Limitations</h3>
<p>While generative AI can produce qualitative art, users have to keep
in mind that the models they use are subject to limitations. They rely
on huge image datasets affected by bias, misrepresentation, and sensible
content. For example, some models still struggle at generating hands,
feet, or coherent images. The produced output may require multiple
trials, and modifications, and often cannot be used in their raw
versions (see <a href="#fig:ref_feet_hands">Fig 77</a>).</p>
<figure id="fig:ref_feet_hands">
<img src="./figures/ref_feet_hands.png"
alt="Feet and hands generated by StableDiffusion [66]." />
<figcaption>Figure 77: Feet and hands generated by StableDiffusion <span
class="citation" data-cites="rombach_2021">[<a href="#ref-rombach_2021"
role="doc-biblioref">66</a>]</span>.</figcaption>
</figure>
<p>One other limitation is that such models are hard and expensive to
train. Only huge corporations with access to modern hardware can produce
high-quality general-purpose models, called foundation models. This may
however change as the domain is improving constantly and thanks to
open-source initiatives such as those of Stability AI <span
class="citation" data-cites="rombach_2021">[<a href="#ref-rombach_2021"
role="doc-biblioref">66</a>]</span>.</p>
<h3 id="regulations-and-guidelines">Regulations and Guidelines</h3>
<p>Because of the current generative AI ethical concerns, societal
impact, and limitations, its usage needs to be regulated or at least
follow some guidelines which are yet to be find. We can however
speculate and propose some of them. Here is a small list of proposed
guidelines or regulations for the fair use of generative AI for
producing art content:</p>
<ul>
<li>The dataset used for training huge foundation generative models
should be opt-in and not opt-out. The artists should have to decide if
they want to enroll in such programs or not.</li>
<li>Artists could be given incomes and credited when the generated art
resembles theirs. This could be done by reverse prompting or by the
introduction of a similarity metric to automatically assign a generated
image to the closest set of artists.</li>
<li>Every piece of art generated using AI should be classified as so for
transparency purposes. This would also help in classifying real art from
“fake”.</li>
</ul>
<h3 id="summary">Summary</h3>
<p>The use of generative AI in the creative industry is transforming the
art sector and presenting benefits such as automating redundant tasks,
increasing productivity, democratizing art production, and enabling
individuals with no background in creative arts to express their
creativity. However, it also presents ethical concerns such as
ownership, copyright, and authenticity issues. The training process of
generative AI models, which is often based on scrapped data, can violate
ownership and copyright policies and lead to art identity theft.</p>
<p>Regulations and guidelines are thus required to lead the field toward
fair use in collaboration with the artist community. Too much regulation
may hinder the rate of improvement of the field. The Balance is yet to
be found. Similarly to when digital art was introduced and faced the
criticism of classical analog mediums, artists need to learn how to
harness such tools to their advantage and avoid being put on the side.
Current models still require human intervention and creativity to
produce professional content.</p>
<h2 id="sec:conclusion">Conclusion</h2>
<!-- ===================== [END] PART Conclusion ===================== -->
<h2 class="unnumbered" id="references">References</h2>
<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-tensorflow" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div
class="csl-right-inline">Abadi, M., Barham, P., Chen, J., Chen, Z.,
Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et
al. 2016. Tensorflow: A system for large-scale machine learning.
<em>Osdi</em> (2016), 265–283.</div>
</div>
<div id="ref-danbooru_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div
class="csl-right-inline">Anonymous, community, D. and Branwen, G. 2021.
<a href="https://gwern.net/Danbooru2020">Danbooru2020: A large-scale
crowdsourced and tagged anime illustration dataset</a>. <a
href="https://gwern.net/Danbooru2020"
class="uri">https://gwern.net/Danbooru2020</a>.</div>
</div>
<div id="ref-arjovsky_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div
class="csl-right-inline">Arjovsky, M., Chintala, S. and Bottou, L. 2017.
<a
href="https://proceedings.mlr.press/v70/arjovsky17a.html"><span>W</span>asserstein
generative adversarial networks</a>. <em>Proceedings of the 34th
international conference on machine learning</em> (2017), 214–223.</div>
</div>
<div id="ref-ba_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">Ba,
J.L., Kiros, J.R. and Hinton, G.E. 2016. Layer normalization. <em>arXiv
preprint arXiv:1607.06450</em>. (2016).</div>
</div>
<div id="ref-bahdanau_2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div
class="csl-right-inline">Bahdanau, D., Cho, K. and Bengio, Y. 2014.
Neural machine translation by jointly learning to align and translate.
<em>arXiv preprint arXiv:1409.0473</em>. (2014).</div>
</div>
<div id="ref-bai_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div
class="csl-right-inline">Bai, J., Lu, F., Zhang, K., et al. 2019. ONNX:
Open neural network exchange. <em>GitHub repository</em>. <a
href="https://github.com/onnx/onnx"
class="uri">https://github.com/onnx/onnx</a>; GitHub.</div>
</div>
<div id="ref-foundation_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div
class="csl-right-inline">Bommasani, R., Hudson, D.A., Adeli, E., Altman,
R., Arora, S., Arx, S. von, Bernstein, M.S., Bohg, J., Bosselut, A.,
Brunskill, E., et al. 2021. On the opportunities and risks of foundation
models. <em>arXiv preprint arXiv:2108.07258</em>. (2021).</div>
</div>
<div id="ref-brown_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div
class="csl-right-inline">Brown, T., Mann, B., Ryder, N., Subbiah, M.,
Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. 2020. Language models are few-shot learners.
<em>Advances in neural information processing systems</em>. 33, (2020),
1877–1901.</div>
</div>
<div id="ref-carlini_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div
class="csl-right-inline">Carlini, N., Hayes, J., Nasr, M., Jagielski,
M., Sehwag, V., Tramèr, F., Balle, B., Ippolito, D. and Wallace, E.
2023. Extracting training data from diffusion models. <em>arXiv preprint
arXiv:2301.13188</em>. (2023).</div>
</div>
<div id="ref-caron_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div
class="csl-right-inline">Caron, M., Touvron, H., Misra, I., Jégou, H.,
Mairal, J., Bojanowski, P. and Joulin, A. 2021. Emerging properties in
self-supervised vision transformers. <em>Proceedings of the IEEE/CVF
international conference on computer vision</em> (2021),
9650–9660.</div>
</div>
<div id="ref-openai_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div
class="csl-right-inline">CHATGPT: Optimizing language models for
dialogue: 2023. <a
href="https://openai.com/blog/chatgpt/"><em>https://openai.com/blog/chatgpt/</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-chen_2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div
class="csl-right-inline">Chen, J., Shen, Y., Gao, J., Liu, J. and Liu,
X. 2018. Language-based image editing with recurrent attentive models.
<em>Proceedings of the IEEE conference on computer vision and pattern
recognition</em> (2018), 8721–8729.</div>
</div>
<div id="ref-chollet_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div
class="csl-right-inline">Chollet, F. 2017. Xception: Deep learning with
depthwise separable convolutions. <em>Proceedings of the IEEE conference
on computer vision and pattern recognition</em> (2017), 1251–1258.</div>
</div>
<div id="ref-ci_2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div
class="csl-right-inline">Ci, Y., Ma, X., Wang, Z., Li, H. and Luo, Z.
2018. <a href="https://doi.org/10.1145/3240508.3240661">User-guided deep
anime line art colorization with conditional adversarial networks</a>.
<em>Proceedings of the 26th ACM international conference on
multimedia</em> (New York, NY, USA, 2018), 1536–1544.</div>
</div>
<div id="ref-clipstudiopaint" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div
class="csl-right-inline">Clip studio PAINT: <a
href="https://www.clipstudio.net/"><em>https://www.clipstudio.net/</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-deng_2009" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div
class="csl-right-inline">Deng, J., Dong, W., Socher, R., Li, L.-J., Li,
K. and Fei-Fei, L. 2009. <a
href="https://doi.org/10.1109/CVPR.2009.5206848">ImageNet: A large-scale
hierarchical image database</a>. <em>2009 IEEE conference on computer
vision and pattern recognition</em> (2009), 248–255.</div>
</div>
<div id="ref-devlin_2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div
class="csl-right-inline">Devlin, J., Chang, M.-W., Lee, K. and
Toutanova, K. 2018. Bert: Pre-training of deep bidirectional
transformers for language understanding. <em>arXiv preprint
arXiv:1810.04805</em>. (2018).</div>
</div>
<div id="ref-dosovitskiy_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div
class="csl-right-inline">Dosovitskiy, A., Beyer, L., Kolesnikov, A.,
Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. 2020. An image is worth 16x16 words:
Transformers for image recognition at scale. <em>arXiv preprint
arXiv:2010.11929</em>. (2020).</div>
</div>
<div id="ref-duchi_2011" class="csl-entry" role="listitem">
<div class="csl-left-margin">[19] </div><div
class="csl-right-inline">Duchi, J., Hazan, E. and Singer, Y. 2011. <a
href="http://jmlr.org/papers/v12/duchi11a.html">Adaptive subgradient
methods for online learning and stochastic optimization</a>. <em>Journal
of Machine Learning Research</em>. 12, 61 (2011), 2121–2159.</div>
</div>
<div id="ref-beck_2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[20] </div><div
class="csl-right-inline">Fourey, S., Tschumperlé, D. and Revoy, D. 2018.
<a href="https://doi.org/10.2312/vmv.20181247">A fast and efficient
semi-guided algorithm for flat coloring line-arts</a>. <em>Vision,
modeling and visualization</em> (2018).</div>
</div>
<div id="ref-frans_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[21] </div><div
class="csl-right-inline">Frans, K. 2017. <a
href="http://arxiv.org/abs/1704.08834">Outline colorization through
tandem adversarial networks</a>. <em>CoRR</em>. abs/1704.08834,
(2017).</div>
</div>
<div id="ref-furusawa_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[22] </div><div
class="csl-right-inline">Furusawa, C., Hiroshiba, K., Ogaki, K. and
Odagiri, Y. 2017. <a
href="https://doi.org/10.1145/3145749.3149430">Comicolorization:
Semi-automatic manga colorization</a>. <em>SIGGRAPH asia 2017 technical
briefs</em> (New York, NY, USA, 2017).</div>
</div>
<div id="ref-furusawa_2O17" class="csl-entry" role="listitem">
<div class="csl-left-margin">[23] </div><div
class="csl-right-inline">Furusawa, C., Hiroshiba, K., Ogaki, K. and
Odagiri, Y. 2017. <a
href="https://doi.org/10.1145/3145749.3149430">Comicolorization:
Semi-automatic manga colorization</a>. <em>SIGGRAPH asia 2017 technical
briefs</em> (New York, NY, USA, 2017).</div>
</div>
<div id="ref-gangnet_1994" class="csl-entry" role="listitem">
<div class="csl-left-margin">[24] </div><div
class="csl-right-inline">Gangnet, M., Thong, J.-M.V. and Fekete, J.-D.
1994. Automatic gap closing for freehand drawing. (1994).</div>
</div>
<div id="ref-goodfellow_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[25] </div><div
class="csl-right-inline">Goodfellow, I.J., Bengio, Y. and Courville, A.
2016. <em>Deep learning</em>. MIT Press.</div>
</div>
<div id="ref-goodfellow_2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[26] </div><div
class="csl-right-inline">Goodfellow, I., Pouget-Abadie, J., Mirza, M.,
Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y. 2014.
<a
href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">Generative
adversarial nets</a>. <em>Advances in neural information processing
systems</em> (2014).</div>
</div>
<div id="ref-gulrajani_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[27] </div><div
class="csl-right-inline">Gulrajani, I., Ahmed, F., Arjovsky, M.,
Dumoulin, V. and Courville, A.C. 2017. <a
href="https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf">Improved
training of wasserstein GANs</a>. <em>Advances in neural information
processing systems</em> (2017).</div>
</div>
<div id="ref-ishaan_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[28] </div><div
class="csl-right-inline">Gulrajani, I., Ahmed, F., Arjovsky, M.,
Dumoulin, V. and Courville, A.C. 2017. <a
href="https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf">Improved
training of wasserstein GANs</a>. <em>Advances in neural information
processing systems</em> (2017).</div>
</div>
<div id="ref-hati_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[29] </div><div
class="csl-right-inline">HATI, Y., JOUET, G., ROUSSEAUX, F. and DUHART,
C. 2019. <a href="https://doi.org/10.1145/3359998.3369401">PaintsTorch:
A user-guided anime line art colorization tool with double generator
conditional adversarial network</a>. <em>Proceedings of the 16th ACM
SIGGRAPH european conference on visual media production</em> (New York,
NY, USA, 2019).</div>
</div>
<div id="ref-hati_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[30] </div><div
class="csl-right-inline">Hati, Y., Thevenin, V., Nolot, F., Rousseaux,
F. and Duhart, C. 2023. StencilTorch: An iterative and user-guided
framework for anime lineart colorization. <em>Image and vision
computing</em> (Cham, 2023), 1–17.</div>
</div>
<div id="ref-he_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[31] </div><div
class="csl-right-inline">He, K., Zhang, X., Ren, S. and Sun, J. 2016.
Deep residual learning for image recognition. <em>Proceedings of the
IEEE conference on computer vision and pattern recognition</em> (2016),
770–778.</div>
</div>
<div id="ref-hensman_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[32] </div><div
class="csl-right-inline">Hensman, P. and Aizawa, K. 2017. cGAN-based
manga colorization using a single training image. <em>2017 14th IAPR
international conference on document analysis and recognition
(ICDAR)</em> (2017), 72–77.</div>
</div>
<div id="ref-heusel_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[33] </div><div
class="csl-right-inline">Heusel, M., Ramsauer, H., Unterthiner, T.,
Nessler, B. and Hochreiter, S. 2017. Gans trained by a two time-scale
update rule converge to a local nash equilibrium. <em>Advances in neural
information processing systems</em>. 30, (2017).</div>
</div>
<div id="ref-hinton_lecture6a" class="csl-entry" role="listitem">
<div class="csl-left-margin">[34] </div><div
class="csl-right-inline">Hinton, G., Srivastava, N. and Swersky, K.
Neural networks for machine learning: Overview of mini-batch gradient
descent.</div>
</div>
<div id="ref-ho_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[35] </div><div
class="csl-right-inline">Ho, J., Jain, A. and Abbeel, P. 2020. Denoising
diffusion probabilistic models. <em>Advances in Neural Information
Processing Systems</em>. 33, (2020), 6840–6851.</div>
</div>
<div id="ref-hornik_1989" class="csl-entry" role="listitem">
<div class="csl-left-margin">[36] </div><div
class="csl-right-inline">Hornik, K., Stinchcombe, M. and White, H. 1989.
Multilayer feedforward networks are universal approximators. <em>Neural
Networks</em>. 2, 5 (1989), 359–366. DOI:https://doi.org/<a
href="https://doi.org/10.1016/0893-6080(89)90020-8">10.1016/0893-6080(89)90020-8</a>.</div>
</div>
<div id="ref-iizuka_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[37] </div><div
class="csl-right-inline">Iizuka, S., Simo-Serra, E. and Ishikawa, H.
2016. Let there be color!: Joint end-to-end learning of global and local
image priors for automatic image colorization with simultaneous
classification. <em>ACM Transactions on Graphics</em>. 35, 4 (Jul.
2016). DOI:https://doi.org/<a
href="https://doi.org/10.1145/2897824.2925974">10.1145/2897824.2925974</a>.</div>
</div>
<div id="ref-ioffe_2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[38] </div><div
class="csl-right-inline">Ioffe, S. and Szegedy, C. 2015. <a
href="https://proceedings.mlr.press/v37/ioffe15.html">Batch
normalization: Accelerating deep network training by reducing internal
covariate shift</a>. <em>Proceedings of the 32nd international
conference on machine learning</em> (Lille, France, 2015),
448–456.</div>
</div>
<div id="ref-kandinsky_1977" class="csl-entry" role="listitem">
<div class="csl-left-margin">[39] </div><div
class="csl-right-inline">Kandinsky, W. and Sadleir, M. 1977.
<em>Concerning the spiritual in art</em>. Dover Publications.</div>
</div>
<div id="ref-karras_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[40] </div><div
class="csl-right-inline">Karras, T., Aila, T., Laine, S. and Lehtinen,
J. 2017. Progressive growing of gans for improved quality, stability,
and variation. <em>arXiv preprint arXiv:1710.10196</em>. (2017).</div>
</div>
<div id="ref-kim_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[41] </div><div
class="csl-right-inline">Kim, H., Jhoo, H.Y., Park, E. and Yoo, S. 2019.
Tag2pix: Line art colorization using text tag with secat and changing
loss. <em>Proceedings of the IEEE/CVF international conference on
computer vision</em> (2019), 9056–9065.</div>
</div>
<div id="ref-kingma_2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[42] </div><div
class="csl-right-inline">Kingma, D.P. and Ba, J. 2014. Adam: A method
for stochastic optimization. <em>arXiv preprint arXiv:1412.6980</em>.
(2014).</div>
</div>
<div id="ref-kingma_2013" class="csl-entry" role="listitem">
<div class="csl-left-margin">[43] </div><div
class="csl-right-inline">Kingma, D.P. and Welling, M. 2013.
Auto-encoding variational bayes. <em>arXiv preprint
arXiv:1312.6114</em>. (2013).</div>
</div>
<div id="ref-lecun_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[44] </div><div class="csl-right-inline">Le
Cun, Y. 2019. <em><a
href="https://www.odilejacob.fr/catalogue/sciences-humaines/questions-de-societe/quand-la-machine-apprend\_9782738149312.php">Quand
la machine apprend: La r<span>é</span>volution des neurones artificiels
et de l’apprentissage profond</a></em>. Odile Jacob.</div>
</div>
<div id="ref-ledig_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[45] </div><div
class="csl-right-inline">Ledig, C., Theis, L., Huszár, F., Caballero,
J., Cunningham, A., Acosta, A., Aitken, A., Tejani, A., Totz, J., Wang,
Z., et al. 2017. Photo-realistic single image super-resolution using a
generative adversarial network. <em>Proceedings of the IEEE conference
on computer vision and pattern recognition</em> (2017), 4681–4690.</div>
</div>
<div id="ref-lim_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[46] </div><div
class="csl-right-inline">Lim, J.H. and Ye, J.C. 2017. Geometric gan.
<em>arXiv preprint arXiv:1705.02894</em>. (2017).</div>
</div>
<div id="ref-liu_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[47] </div><div
class="csl-right-inline">Liu, F., Deng, X., Lai, Y.-K., Liu, Y.-J., Ma,
C. and Wang, H. 2019. SketchGAN: Joint sketch completion and recognition
with generative adversarial network. <em>Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition (CVPR)</em>
(2019).</div>
</div>
<div id="ref-liu_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[48] </div><div
class="csl-right-inline">Liu, Y., Qin, Z., Wan, T. and Luo, Z. 2018.
Auto-painter: Cartoon image generation from sketch by using conditional
wasserstein generative adversarial networks. <em>Neurocomputing</em>.
311, (2018), 78–87. DOI:https://doi.org/<a
href="https://doi.org/10.1016/j.neucom.2018.05.045">10.1016/j.neucom.2018.05.045</a>.</div>
</div>
<div id="ref-loshchilov_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[49] </div><div
class="csl-right-inline">Loshchilov, I. and Hutter, F. 2017. Decoupled
weight decay regularization. <em>arXiv preprint arXiv:1711.05101</em>.
(2017).</div>
</div>
<div id="ref-kautz_2007" class="csl-entry" role="listitem">
<div class="csl-left-margin">[50] </div><div
class="csl-right-inline">Luan, Q., Wen, F., Cohen-Or, D., Liang, L., Xu,
Y.-Q. and Shum, H.-Y. 2007. <a
href="https://doi.org/10.2312/EGWR/EGSR07/309-320">Natural image
colorization</a>. <em>Rendering techniques</em> (2007).</div>
</div>
<div id="ref-luong_2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[51] </div><div
class="csl-right-inline">Luong, M.-T., Pham, H. and Manning, C.D. 2015.
Effective approaches to attention-based neural machine translation.
<em>arXiv preprint arXiv:1508.04025</em>. (2015).</div>
</div>
<div id="ref-minsky_1969" class="csl-entry" role="listitem">
<div class="csl-left-margin">[52] </div><div
class="csl-right-inline">Minsky, M. and Papert, S. 1969.
<em>Perceptrons: An introduction to computational geometry</em>. MIT
Press.</div>
</div>
<div id="ref-miyato_2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[53] </div><div
class="csl-right-inline">Miyato, T., Kataoka, T., Koyama, M. and
Yoshida, Y. 2018. Spectral normalization for generative adversarial
networks. <em>arXiv preprint arXiv:1802.05957</em>. (2018).</div>
</div>
<div id="ref-mumford_2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">[54] </div><div
class="csl-right-inline">Mumford, M., Medeiros, K. and Partlow, P. 2012.
Creative thinking: Processes, strategies, and knowledge. <em>The Journal
of Creative Behavior</em>. 46, (Mar. 2012). DOI:https://doi.org/<a
href="https://doi.org/10.1002/jocb.003">10.1002/jocb.003</a>.</div>
</div>
<div id="ref-newell_1959" class="csl-entry" role="listitem">
<div class="csl-left-margin">[55] </div><div
class="csl-right-inline">Newell, A., Shaw, J.C. and Simon, H.A. 1959.
<em><a href="https://doi.org/10.1037/13117-003">The processes of
creative thinking</a></em>. RAND Corporation.</div>
</div>
<div id="ref-nichol_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[56] </div><div
class="csl-right-inline">Nichol, A.Q. and Dhariwal, P. 2021. Improved
denoising diffusion probabilistic models. <em>International conference
on machine learning</em> (2021), 8162–8171.</div>
</div>
<div id="ref-paintman" class="csl-entry" role="listitem">
<div class="csl-left-margin">[57] </div><div
class="csl-right-inline">Paintman: <a
href="http://www.retasstudio.net/products/paintman/"><em>http://www.retasstudio.net/products/paintman/</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-paintschainer_2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[58] </div><div
class="csl-right-inline">Paints chainer: 2018. <a
href="https://github.com/pfnet/PaintsChainer"><em>https://github.com/pfnet/PaintsChainer</em></a>.
Accessed: 2023-02-25.</div>
</div>
<div id="ref-wilkie_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[59] </div><div
class="csl-right-inline">Parakkat, A.D., Madipally, P., Gowtham, H.H.
and Cani, M.-P. 2020. <a
href="https://doi.org/10.2312/egs.20201024">Interactive flat coloring of
minimalist neat sketches</a>. <em>Eurographics 2020 - short papers</em>
(2020).</div>
</div>
<div id="ref-parakkat_2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[60] </div><div
class="csl-right-inline">Parakkat, A.D., Memari, P. and Cani, M.-P.
2022. Delaunay painting: Perceptual image colouring from raster contours
with gaps. <em>Computer Graphics Forum</em>. 41, 6 (2022), 166–181.
DOI:https://doi.org/<a
href="https://doi.org/10.1111/cgf.14517">https://doi.org/10.1111/cgf.14517</a>.</div>
</div>
<div id="ref-pytorch" class="csl-entry" role="listitem">
<div class="csl-left-margin">[61] </div><div
class="csl-right-inline">Paszke, A. et al. 2019. PyTorch: An imperative
style, high-performance deep learning library. <em>Proceedings of the
33rd international conference on neural information processing
systems</em>. Curran Associates Inc.</div>
</div>
<div id="ref-petalicapaint_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[62] </div><div
class="csl-right-inline">Petalica paint: AI-powered automatic
colorization: 2023. <a
href="https://petalica.com/index\_en.html"><em>https://petalica.com/index\_en.html</em></a>.
Accessed: 2023-02-25.</div>
</div>
<div id="ref-photoshop" class="csl-entry" role="listitem">
<div class="csl-left-margin">[63] </div><div
class="csl-right-inline">Photoshop: <a
href="https://www.adobe.com/products/photoshop.html"><em>https://www.adobe.com/products/photoshop.html</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-qian_1999" class="csl-entry" role="listitem">
<div class="csl-left-margin">[64] </div><div
class="csl-right-inline">Qian, N. 1999. On the momentum term in gradient
descent learning algorithms. <em>Neural Networks</em>. 12, 1 (1999),
145–151. DOI:https://doi.org/<a
href="https://doi.org/10.1016/S0893-6080(98)00116-6">10.1016/S0893-6080(98)00116-6</a>.</div>
</div>
<div id="ref-ramesh_2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[65] </div><div
class="csl-right-inline">Ramesh, A., Dhariwal, P., Nichol, A., Chu, C.
and Chen, M. 2022. Hierarchical text-conditional image generation with
clip latents. <em>arXiv preprint arXiv:2204.06125</em>. (2022).</div>
</div>
<div id="ref-rombach_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[66] </div><div
class="csl-right-inline">Rombach, R., Blattmann, A., Lorenz, D., Esser,
P. and Ommer, B. 2021. <a
href="https://arxiv.org/abs/2112.10752">High-resolution image synthesis
with latent diffusion models</a>.</div>
</div>
<div id="ref-ronneberger_2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[67] </div><div
class="csl-right-inline">Ronneberger, O., Fischer, P. and Brox, T. 2015.
U-net: Convolutional networks for biomedical image segmentation.
<em>Medical image computing and computer-assisted intervention–MICCAI
2015: 18th international conference, munich, germany, october 5-9, 2015,
proceedings, part III 18</em> (2015), 234–241.</div>
</div>
<div id="ref-rosenblatt_1958" class="csl-entry" role="listitem">
<div class="csl-left-margin">[68] </div><div
class="csl-right-inline">Rosenblatt, F. 1958. The perceptron: A
probabilistic model for information storage and organization in the
brain. <em>Psychological Review</em>. 65, 6 (1958), 386–408.
DOI:https://doi.org/<a
href="https://doi.org/10.1037/h0042519">10.1037/h0042519</a>.</div>
</div>
<div id="ref-rumelhart_1986" class="csl-entry" role="listitem">
<div class="csl-left-margin">[69] </div><div
class="csl-right-inline">Rumelhart, D.E., Hinton, G.E. and Williams,
R.J. 1986. Learning representations by back-propagating errors.
<em>Nature</em>. 323, 6088 (Oct. 1986), 533–536. DOI:https://doi.org/<a
href="https://doi.org/10.1038/323533a0">10.1038/323533a0</a>.</div>
</div>
<div id="ref-saito_2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[70] </div><div
class="csl-right-inline">Saito, M. and Matsui, Y. 2015. <a
href="https://doi.org/10.1145/2820903.2820907">Illustration2Vec: A
semantic vector representation of illustrations</a>. <em>SIGGRAPH asia
2015 technical briefs</em> (New York, NY, USA, 2015), 5:1–5:4.</div>
</div>
<div id="ref-sangkloy_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[71] </div><div
class="csl-right-inline">Sangkloy, P., Lu, J., Fang, C., Yu, F. and
Hays, J. 2017. <a
href="https://doi.org/10.1109/CVPR.2017.723">Scribbler: Controlling deep
image synthesis with sketch and color</a>. <em>2017 <span>IEEE</span>
conference on computer vision and pattern recognition, <span>CVPR</span>
2017, honolulu, HI, USA, july 21-26, 2017</em> (2017), 6836–6845.</div>
</div>
<div id="ref-sasaki_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[72] </div><div
class="csl-right-inline">Sasaki, K., Iizuka, S., Simo-Serra, E. and
Ishikawa, H. 2017. <a href="https://doi.org/10.1109/CVPR.2017.611">Joint
gap detection and inpainting of line drawings</a>. <em>2017 IEEE
conference on computer vision and pattern recognition (CVPR)</em>
(2017), 5768–5776.</div>
</div>
<div id="ref-shi_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[73] </div><div
class="csl-right-inline">Shi, M., Zhang, J.-Q., Chen, S.-Y., Gao, L.,
Lai, Y.-K. and Zhang, F.-L. 2020. Deep line art video colorization with
a few references. <em>arXiv preprint arXiv:2003.10685</em>.
(2020).</div>
</div>
<div id="ref-simonyan_2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[74] </div><div
class="csl-right-inline">Simonyan, K. and Zisserman, A. 2014. Very deep
convolutional networks for large-scale image recognition. <em>arXiv
preprint arXiv:1409.1556</em>. (2014).</div>
</div>
<div id="ref-smilkov_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[75] </div><div
class="csl-right-inline">Smilkov, D., Thorat, N., Assogba, Y.,
Nicholson, C., Kreeger, N., Yu, P., Cai, S., Nielsen, E., Soegel, D.,
Bileschi, S., et al. 2019. Tensorflow. Js: Machine learning for the web
and beyond. <em>Proceedings of Machine Learning and Systems</em>. 1,
(2019), 309–321.</div>
</div>
<div id="ref-su_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[76] </div><div
class="csl-right-inline">Su, J.-W., Chu, H.-K. and Huang, J.-B. 2020.
Instance-aware image colorization. <em>Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</em> (2020),
7968–7977.</div>
</div>
<div id="ref-sykora_2009" class="csl-entry" role="listitem">
<div class="csl-left-margin">[77] </div><div
class="csl-right-inline">Sykora, D., Dingliana, J. and Collins, S. 2009.
LazyBrush: Flexible painting tool for hand-drawn cartoons. <em>Computer
Graphics Forum</em>. 28, 2 (2009), 599–608. DOI:https://doi.org/<a
href="https://doi.org/10.1111/j.1467-8659.2009.01400.x">https://doi.org/10.1111/j.1467-8659.2009.01400.x</a>.</div>
</div>
<div id="ref-szegedy_2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[78] </div><div
class="csl-right-inline">Szegedy, C., Liu, W., Jia, Y., Sermanet, P.,
Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V. and Rabinovich, A.
2015. Going deeper with convolutions. <em>Proceedings of the IEEE
conference on computer vision and pattern recognition</em> (2015),
1–9.</div>
</div>
<div id="ref-szegedy_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[79] </div><div
class="csl-right-inline">Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens,
J. and Wojna, Z. 2016. Rethinking the inception architecture for
computer vision. <em>Proceedings of the IEEE conference on computer
vision and pattern recognition</em> (2016), 2818–2826.</div>
</div>
<div id="ref-mnist" class="csl-entry" role="listitem">
<div class="csl-left-margin">[80] </div><div
class="csl-right-inline">The MNIST database: <a
href="http://yann.lecun.com/exdb/mnist/"><em>http://yann.lecun.com/exdb/mnist/</em></a>.
Accessed: 2023-02-07.</div>
</div>
<div id="ref-vaswani_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[81] </div><div
class="csl-right-inline">Vaswani, A., Shazeer, N., Parmar, N.,
Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I.
2017. Attention is all you need. <em>Advances in neural information
processing systems</em>. 30, (2017).</div>
</div>
<div id="ref-winnermoller_2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">[82] </div><div
class="csl-right-inline">Winnemöller, H., Kyprianidis, J.E. and Olsen,
S.C. 2012. XDoG: An eXtended difference-of-gaussians compendium
including advanced image stylization. <em>Computers &amp; Graphics</em>.
36, 6 (2012), 740–753. DOI:https://doi.org/<a
href="https://doi.org/10.1016/j.cag.2012.03.004">https://doi.org/10.1016/j.cag.2012.03.004</a>.</div>
</div>
<div id="ref-wolf_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[83] </div><div
class="csl-right-inline">Wolf, M.J., Miller, K. and Grodzinsky, F.S.
2017. Why we should have seen that coming: Comments on microsoft’s tay
"experiment," and wider implications. <em>SIGCAS Comput. Soc.</em> 47, 3
(Sep. 2017), 54–64. DOI:https://doi.org/<a
href="https://doi.org/10.1145/3144592.3144598">10.1145/3144592.3144598</a>.</div>
</div>
<div id="ref-yoo_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[84] </div><div
class="csl-right-inline">Yoo, S., Bahng, H., Chung, S., Lee, J., Chang,
J. and Choo, J. 2019. Coloring with limited data: Few-shot colorization
via memory augmented networks. <em>Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition (CVPR)</em>
(2019).</div>
</div>
<div id="ref-aston_zhang_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[85] </div><div
class="csl-right-inline">Zhang, A., Lipton, Z.C., Li, M. and Smola, A.J.
2021. Dive into deep learning. <em>arXiv preprint arXiv:2106.11342</em>.
(2021).</div>
</div>
<div id="ref-zhang_ji_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[86] </div><div
class="csl-right-inline">Zhang, L., Ji, Y., Lin, X. and Liu, C. 2017. <a
href="https://doi.org/10.1109/ACPR.2017.61">Style transfer for anime
sketches with enhanced residual u-net and auxiliary classifier GAN</a>.
<em>2017 4th IAPR asian conference on pattern recognition (ACPR)</em>
(2017), 506–511.</div>
</div>
<div id="ref-danboo_region_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[87] </div><div
class="csl-right-inline">Zhang, L., JI, Y. and Liu, C. 2020.
DanbooRegion: An illustration region dataset. <em>European conference on
computer vision (ECCV)</em> (2020).</div>
</div>
<div id="ref-zhang_2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[88] </div><div
class="csl-right-inline">Zhang, L., Li, C., Wong, T.-T., Ji, Y. and Liu,
C. 2018. Two-stage sketch colorization. <em>ACM Trans. Graph.</em> 37, 6
(Dec. 2018). DOI:https://doi.org/<a
href="https://doi.org/10.1145/3272127.3275090">10.1145/3272127.3275090</a>.</div>
</div>
<div id="ref-zhang_richard_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[89] </div><div
class="csl-right-inline">Zhang, R., Zhu, J.-Y., Isola, P., Geng, X.,
Lin, A.S., Yu, T. and Efros, A.A. 2017. Real-time user-guided image
colorization with learned deep priors. <em>ACM Trans. Graph.</em> 36, 4
(Jul. 2017). DOI:https://doi.org/<a
href="https://doi.org/10.1145/3072959.3073703">10.1145/3072959.3073703</a>.</div>
</div>
<div id="ref-zhu_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[90] </div><div
class="csl-right-inline">Zhu, J.-Y., Park, T., Isola, P. and Efros, A.A.
2017. Unpaired image-to-image translation using cycle-consistent
adversarial networks. <em>Proceedings of the IEEE international
conference on computer vision</em> (2017), 2223–2232.</div>
</div>
<div id="ref-zou_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[91] </div><div
class="csl-right-inline">Zou, C., Mo, H., Gao, C., Du, R. and Fu, H.
2019. Language-based colorization of scene sketches. <em>ACM Trans.
Graph.</em> 38, 6 (Nov. 2019). DOI:https://doi.org/<a
href="https://doi.org/10.1145/3355089.3356561">10.1145/3355089.3356561</a>.</div>
</div>
</div>
</body>
</html>
