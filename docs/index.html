<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Yliess Hati" />
  <meta name="keywords" content="keyword" />
  <title>AI-Assisted Creative Expression: a Case for Automatic Lineart Colorization</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title"><p>AI-Assisted Creative Expression: a Case for
Automatic Lineart Colorization</p></h1>
<p class="author">Yliess Hati</p>
</header>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Contents</h2>
<ul>
<li><a href="#list-of-abbreviations" id="toc-list-of-abbreviations">List
of Abbreviations</a></li>
<li><a href="#acronym-list" id="toc-acronym-list">Acronyms</a></li>
<li><a href="#abstract" id="toc-abstract">Abstract</a></li>
<li><a href="#aknowledgements"
id="toc-aknowledgements">Aknowledgements</a></li>
<li><a href="#context" id="toc-context">Context</a>
<ul>
<li><a href="#ch:introduction" id="toc-ch:introduction">Introduction</a>
<ul>
<li><a href="#motivations" id="toc-motivations">Motivations</a></li>
<li><a href="#problem-statement" id="toc-problem-statement">Problem
Statement</a></li>
<li><a href="#contributions"
id="toc-contributions">Contributions</a></li>
<li><a href="#concerns" id="toc-concerns">Concerns</a></li>
<li><a href="#outline" id="toc-outline">Outline</a></li>
</ul></li>
<li><a href="#ch:background" id="toc-ch:background">Background</a>
<ul>
<li><a href="#sec:history" id="toc-sec:history">A Brief History of
Artificial Intelligence</a></li>
<li><a href="#sec:core" id="toc-sec:core">Core Principles</a></li>
<li><a href="#sec:generative" id="toc-sec:generative">Generative
Architectures</a></li>
<li><a href="#sec:attention" id="toc-sec:attention">Attention is all you
Need</a></li>
</ul></li>
<li><a href="#ch:methodology" id="toc-ch:methodology">Methodology</a>
<ul>
<li><a href="#implementation"
id="toc-implementation">Implementation</a></li>
<li><a href="#objective-evaluation"
id="toc-objective-evaluation">Objective Evaluation</a></li>
<li><a href="#subjective-evaluation"
id="toc-subjective-evaluation">Subjective Evaluation</a></li>
<li><a href="#reproducibility"
id="toc-reproducibility">Reproducibility</a></li>
</ul></li>
</ul></li>
<li><a href="#core" id="toc-core">Core</a>
<ul>
<li><a href="#ch:contrib-1" id="toc-ch:contrib-1">Contrib I (Find Catchy
Explicit Name)</a>
<ul>
<li><a href="#state-of-the-art" id="toc-state-of-the-art">State of the
Art</a></li>
<li><a href="#method" id="toc-method">Method</a></li>
<li><a href="#setup" id="toc-setup">Setup</a></li>
<li><a href="#results" id="toc-results">Results</a></li>
<li><a href="#summary" id="toc-summary">Summary</a></li>
</ul></li>
<li><a href="#ch:contrib-2" id="toc-ch:contrib-2">Contrib II (Find
Catchy Explicit Name)</a>
<ul>
<li><a href="#state-of-the-art-1" id="toc-state-of-the-art-1">State of
the Art</a></li>
<li><a href="#method-1" id="toc-method-1">Method</a></li>
<li><a href="#setup-1" id="toc-setup-1">Setup</a></li>
<li><a href="#results-1" id="toc-results-1">Results</a></li>
<li><a href="#summary-1" id="toc-summary-1">Summary</a></li>
</ul></li>
<li><a href="#ch:contrib-3" id="toc-ch:contrib-3">Contrib III (Find
Catchy Explicit Name)</a>
<ul>
<li><a href="#state-of-the-art-2" id="toc-state-of-the-art-2">State of
the Art</a></li>
<li><a href="#method-2" id="toc-method-2">Method</a></li>
<li><a href="#setup-2" id="toc-setup-2">Setup</a></li>
<li><a href="#results-2" id="toc-results-2">Results</a></li>
<li><a href="#summary-2" id="toc-summary-2">Summary</a></li>
</ul></li>
<li><a href="#ch:contrib-4" id="toc-ch:contrib-4">Contrib IV (Find
Catchy Explicit Name)</a>
<ul>
<li><a href="#state-of-the-art-3" id="toc-state-of-the-art-3">State of
the Art</a></li>
<li><a href="#method-3" id="toc-method-3">Method</a></li>
<li><a href="#setup-3" id="toc-setup-3">Setup</a></li>
<li><a href="#results-3" id="toc-results-3">Results</a></li>
<li><a href="#summary-3" id="toc-summary-3">Summary</a></li>
</ul></li>
</ul></li>
<li><a href="#reflection" id="toc-reflection">Reflection</a>
<ul>
<li><a href="#ch:ethical-and-societal-impact"
id="toc-ch:ethical-and-societal-impact">Ethical and Societal
Impact</a></li>
<li><a href="#ch:conclusion" id="toc-ch:conclusion">Conclusion</a></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul></li>
</ul>
</nav>
<h2 class="unnumbered" id="list-of-abbreviations">List of
Abbreviations</h2>
<h1 id="acronym-list">Acronyms</h1>
<ul>
<li><strong>ACT-R</strong>: Adaptive Control of Thought—Rational</li>
<li><strong>AI</strong>: Artificial Intelligence</li>
<li><strong>ANN</strong>: Artificial Neural Network</li>
<li><strong>CNN</strong>: Convolutional Neural Network</li>
<li><strong>CV</strong>: Computer Vision</li>
<li><strong>DDM</strong>: Denoising Diffusion Model</li>
<li><strong>DL</strong>: Deep Learning</li>
<li><strong>GAN</strong>: Generative Adversarial Network</li>
<li><strong>GD</strong>: Gradient Descent</li>
<li><strong>GPU</strong>: Graphical Processing Unit</li>
<li><strong>LLM</strong>: Large Language Model</li>
<li><strong>LSTM</strong>: Long Short-Term Memory</li>
<li><strong>MLP</strong>: Multi-Layer Perceptron</li>
<li><strong>NN</strong>: Neural Network</li>
<li><strong>NPU</strong>: Neural Processing Unit</li>
<li><strong>RLHF</strong>: Reinforcement Learning from Human
Feedback</li>
<li><strong>RNN</strong>: Recurrent Neural Network</li>
<li><strong>SVM</strong>: Support Vector Machine</li>
<li><strong>TPU</strong>: Tensor Processing Unit</li>
<li><strong>VAE</strong>: Variational Autoencoder</li>
</ul>
<h2 class="unnumbered" id="abstract">Abstract</h2>

<h2 class="unnumbered" id="aknowledgements">Aknowledgements</h2>

<h1 id="context">Context</h1>
<h2 id="ch:introduction">Introduction</h2>
<p>Humans possess the ability to perceive and understand the world
allowing us to accomplish a wide range of complex tasks through the
combination of visual recognition, scene understanding, and
communication. The ability to quickly and accurately extract information
from a single image is a testament to the complexity and sophistication
of the human brain and is often taken for granted. One of the Artificial
Intelligence (AI) field’s ultimate goals is to empower computers with
such human-like abilities, one of them being creativity, being able to
produce something original and worthwhile <span class="citation"
data-cites="mumford_2012">[<a href="#ref-mumford_2012"
role="doc-biblioref">33</a>]</span>.</p>
<p>Computational creativity is the field at the intersection of AI,
cognitive psychology, philosophy, and art, which aims at understanding,
simulating, replicating, or in some cases enhancing human creativity.
One definition of computational creativity <span class="citation"
data-cites="newell_1959">[<a href="#ref-newell_1959"
role="doc-biblioref">34</a>]</span> is the ability to produce something
that is novel and useful, demands that we reject common beliefs, results
from intense motivation and persistence, or comes from clarifying a
vague problem. Top-down approaches to this definition use a mix of
explicit formulations of recipes and randomness such as procedural
generation. On the opposite, bottom-up approaches use Artificial Neural
Networks (ANN) to learn patterns and heuristics from large datasets to
enable non-linear generation.</p>
<p>We, as a species, are currently witnessing the beginning of a new era
where the gap between machines and humans is starting to blur. Current
breakthroughs in the field of AI, more specifically in Deep Learning
(DL), are giving computers the ability to perceive and understand our
world, but also to interact with our environment using natural
interactions such as speech and natural language. ANNs, once mocked by
the AI community <span class="citation" data-cites="lecun_2019">[<a
href="#ref-lecun_2019" role="doc-biblioref">25</a>]</span>, are now
trainable using Gradient Descent (GD) <span class="citation"
data-cites="rumelhart_1986">[<a href="#ref-rumelhart_1986"
role="doc-biblioref">40</a>]</span> thanks to the massive availability
of data and the processing power of modern hardware accelerators such as
Graphical Processing Units (GPU), Tensor Processing Units (TPU), and
Neural Processing Units (NPU).</p>
<p>Neural Networks (NN), those trainable general function approximators,
gave rise to the field of generative NNs. Specialized DL architectures
such as Variational Autoencoders (VAE) <span class="citation"
data-cites="kingma_2013">[<a href="#ref-kingma_2013"
role="doc-biblioref">22</a>]</span>, Generative Adversarial Networks
(GAN) <span class="citation" data-cites="goodfellow_2014">[<a
href="#ref-goodfellow_2014" role="doc-biblioref">11</a>]</span>,
Denoising Diffusion Models (DDM) <span class="citation"
data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">16</a>]</span>, and Large Language Models (LLM)
<span class="citation" data-cites="vaswani_2017 brown_2020">[<a
href="#ref-brown_2020" role="doc-biblioref">2</a>, <a
href="#ref-vaswani_2017" role="doc-biblioref">46</a>]</span> are used to
generate artifacts such as text, audio, images, and videos of
unprecedented quality and complexity.</p>
<p>This dissertation aims at exploring how one could train and use
generative NN to create AI-powered tools capable of enhancing human
creative expression. The task of automatic lineart colorization act as
the example case used to illustrate this process throughout the entire
thesis.</p>
<h3 id="motivations">Motivations</h3>
<p>Lineart colorization is an essential aspect of the work of artists,
illustrators, and animators. The task of manually coloring lineart can
be time-consuming, repetitive, and exhausting, particularly in the
animation industry, where every frame of an animated product must be
colored and shaded. This process is typically done using image editing
software such as Photoshop <span class="citation"
data-cites="photoshop">[<a href="#ref-photoshop"
role="doc-biblioref">37</a>]</span>, Clip Studio PAINT <span
class="citation" data-cites="clipstudiopaint">[<a
href="#ref-clipstudiopaint" role="doc-biblioref">5</a>]</span>, and
PaintMan <span class="citation" data-cites="paintman">[<a
href="#ref-paintman" role="doc-biblioref">35</a>]</span>. Automating the
colorization process can greatly improve the workflow of these creative
professionals and has the potential to lower the barrier for newcomers
and amateurs. Such a system was integrated into Clip Studio PAINT <span
class="citation" data-cites="clipstudiopaint">[<a
href="#ref-clipstudiopaint" role="doc-biblioref">5</a>]</span>,
demonstrating the growing significance of automatic colorization in the
field.</p>
<p>The most common digital illustration process can be broken down into
four distinct stages: sketching, inking, coloring, and post-processing.
As demonstrated by the work of Kandinsky <span class="citation"
data-cites="kandinsky_1977">[<a href="#ref-kandinsky_1977"
role="doc-biblioref">20</a>]</span>, the colorization process can
greatly impact the overall meaning of a piece of art through the
introduction of various color schemes, shading, and textures. These
elements of the coloring process present significant challenges for the
Computer Vision (CV) task of automatic lineart colorization,
particularly in comparison to its grayscale counterpart <span
class="citation"
data-cites="furusawa_2O17 hensman_2017 zhang_richard_2017">[<a
href="#ref-furusawa_2O17" role="doc-biblioref">10</a>, <a
href="#ref-hensman_2017" role="doc-biblioref">15</a>, <a
href="#ref-zhang_richard_2017" role="doc-biblioref">50</a>]</span>.
Without the added semantic information provided by textures and shadows,
inferring materials and 3D shapes from black and white linearts is
difficult. They can only be deduced from silhouettes.</p>
<h3 id="problem-statement">Problem Statement</h3>
<p>One major challenge of automatic lineart colorization is the
availability of qualitative public datasets. Illustrations do not always
come with their corresponding lineart. The few datasets available for
the task are lacking consistency in the quality of the illustrations,
gathering images from different types, mediums and styles. For those
reasons, online scrapping and synthetic lineart extraction is the method
of choice for many of the contributions in the field <span
class="citation" data-cites="ci_2018 zhang_richard_2017">[<a
href="#ref-ci_2018" role="doc-biblioref">4</a>, <a
href="#ref-zhang_richard_2017" role="doc-biblioref">50</a>]</span>.</p>
<p>Previous works in automatic lineart colorization are based on the GAN
<span class="citation" data-cites="goodfellow_2014">[<a
href="#ref-goodfellow_2014" role="doc-biblioref">11</a>]</span>
architecture. They can generate unperfect but high-quality illustrations
in a quasi realtime setting. They achieve user control and guidance via
different means, color hints <span class="citation"
data-cites="frans_2017 liu_2017 sangkloy_2016 paintschainer_2017 ci_2018">[<a
href="#ref-ci_2018" role="doc-biblioref">4</a>, <a
href="#ref-frans_2017" role="doc-biblioref">8</a>, <a
href="#ref-liu_2017" role="doc-biblioref">29</a>, <a
href="#ref-paintschainer_2017" role="doc-biblioref">36</a>, <a
href="#ref-sangkloy_2016" role="doc-biblioref">43</a>]</span>, style
transfer <span class="citation" data-cites="zhang_ji_2017">[<a
href="#ref-zhang_ji_2017" role="doc-biblioref">49</a>]</span>, tagging
<span class="citation" data-cites="kim_2019">[<a href="#ref-kim_2019"
role="doc-biblioref">21</a>]</span>, and more recently natural language
<span class="citation" data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">16</a>]</span>. One common pattern in these methods
is the use of a feature extractor such as Illustration2Vec <span
class="citation" data-cites="saito_2015">[<a href="#ref-saito_2015"
role="doc-biblioref">42</a>]</span> allowing to compensate for the lack
of semantic descriptors by injecting its feature vector into the
models.</p>
<h3 id="contributions">Contributions</h3>
<p>This work focuses on the use of color hints in the form of user
strokes as it fits the natural digital artist workflow and does not
involve learning and mastering a new skill. While previous works offers
improving quality compared to classical CV techniques, they are still
subject to noisy training data, artifacts, a lack of variety, and a lack
of fidelity in the user intent. In this dissertation we explore the
importance of a clean, qualitative and consistent dataset. We
investigate how to better capture the user intent via natural artistic
controls and how to reflect them into the generated model artifact while
preserving or improving its quality. We also look at how the creative
process can be transformed into a dynamic iterative workflow where the
user collaborates with the machine to refine and carry out variations of
his artwork.</p>
<p>Here is a brief enumeration of this thesis’s contributions:</p>
<ul>
<li>We present a recipe for curating datasets for the task of automatic
lineart colorization <span class="citation"
data-cites="hati_2019 hati_2022">[<a href="#ref-hati_2019"
role="doc-biblioref">12</a>, <a href="#ref-hati_2022"
role="doc-biblioref">13</a>]</span></li>
<li>We introduce three generative models:
<ul>
<li>PaintsTorch <span class="citation" data-cites="hati_2019">[<a
href="#ref-hati_2019" role="doc-biblioref">12</a>]</span>, a double GAN
generator that improved generation quality compared to previous work
while allowing realtime interaction with the user.</li>
<li>StencilTorch <span class="citation" data-cites="hati_2022">[<a
href="#ref-hati_2022" role="doc-biblioref">13</a>]</span>, an upgrade
upon PaintsTorch, shifting the colorization problem to in-painting
allowing for human collaboration to emerge as a natural workflow where
the input of a first pass becomes the potential input for a second.</li>
<li>StablePaint, an exploration of DDM for bringing more variety into
the generated outputs allowing for variation exploration and conserving
the iterative workflow introduced by StencilTorch for the cost of
inference speed.</li>
</ul></li>
<li>We offer an advised reflection on current generative AI ethical and
societal impact.</li>
</ul>
<h3 id="concerns">Concerns</h3>
<p>Recent advances in generative AI for text, image, audio, and video
synthesis are raising important ethical and societal concerns,
especially because of its availability and ease of use. Models such as
Stable Diffusion <span class="citation" data-cites="rombach_2021">[<a
href="#ref-rombach_2021" role="doc-biblioref">38</a>]</span> and more
recently Chat-GPT <span class="citation" data-cites="openai_2023">[<a
href="#ref-openai_2023" role="doc-biblioref">3</a>]</span> are
disturbing our common beliefs and relation with copyright, creativity,
the distribution of fake information and so on.</p>
<p>One of the main issues with generative AI is the potential for model
fabulation. Generative models can create entirely new, synthetic data
that is indistinguishable from real data. This can lead to the
dissemination of false information and the manipulation of public
opinion. Additionally, there are ambiguities surrounding the ownership
and copyright of the generated content, as it is unclear who holds the
rights to the generated images and videos. Training data is often
obtained via online scrapping and thus copyright ownership is not
propagated. This is especially true for commercial applications.</p>
<p>Another important concern is the potential for biases and
discrimination. These models are trained on large amounts of data, and
if the data is not diverse or representative enough, the model may
perpetuate or even amplify existing biases. The Microsoft Tay Twitter
bot <span class="citation" data-cites="wolf_2017">[<a
href="#ref-wolf_2017" role="doc-biblioref">48</a>]</span> scandal is an
outcome of such a phenomenon. This initially innocent chatbot has been
easily turned into a racist bot perpetuating hate speech. The task was
made easier because of the inherently biased dataset it was trained
on.</p>
<p>In this work, we are committed to addressing and raising awareness
for these concerns. The illustrations used for training our models and
for our experiments are only used for educational and research purposes.
We only provide recipes for reproducibility and do not distribute the
dataset nor the weights resulting from model training, only the code. We
hope this will not ensure that our work is used ethically and
responsibly but limit its potential misuse.</p>
<h3 id="outline">Outline</h3>
<p>The first part of this thesis (chapters <a
href="#ch:introduction">1</a>-<a href="#ch:methodology">3</a>) provides
context to the recent advances in generative AI and introduces the CV
task of user-guided automatic lineart colorization, its challenges, and
our contributions to the field. It then provides additional background,
from DL first principles to current architectures used in modern
generative NN, and introduces the methodology used throughout the entire
document. This part should be accessible to the majority, experts and
non-experts, and serve as an introduction to the field.</p>
<p>The second part (chapters <a href="#ch:contrib-1">4</a>-<a
href="#ch:contrib-4">7</a>) presents our contributions, some of which
have previously been presented in <span class="citation"
data-cites="hati_2019 hati_2022">[<a href="#ref-hati_2019"
role="doc-biblioref">12</a>, <a href="#ref-hati_2022"
role="doc-biblioref">13</a>]</span>. It introduces into detail our
recipe for sourcing and curating consistent and qualitative datasets for
automatic lineart colorization, PaintsTorch <span class="citation"
data-cites="hati_2019">[<a href="#ref-hati_2019"
role="doc-biblioref">12</a>]</span> our first double generator GAN
conditioned on user strokes, StencilTorch <span class="citation"
data-cites="hati_2022">[<a href="#ref-hati_2022"
role="doc-biblioref">13</a>]</span> our in-painting reformulation
introducing the use of masks to allow the emergence of iterative
workflow and collaboration with the machine, and finally StablePaint, an
exploration of the use of DDM models for variations qualitative
exploration.</p>
<p>The third and final part (chapters <a
href="#ch:ethdical-and-societal-impact">7</a>-<a
href="#ch:conclusion">8</a>) offers a detailed reflection on this
thesis’s contributions and more generally about the field of generative
AI ethical and societal impact, identifies the remaining challenges and
discusses future work.</p>
<p>The code base for the experiments and contributions is publicly
available on GitHub at <a
href="https://github.com/yliess86">https://github.com/yliess86</a>.</p>
<h2 id="ch:background">Background</h2>
<p>This chapter introduces the reader to the field of Deep Learning (DL)
from first principles to the current architectures used in modern
generative AI. The first section (section <a href="#sec:history">1</a>)
presents a brief history of AI to ground this technical dissertation
into its historical context. The following sections (sections <a
href="#sec:core">2</a>-<a href="#sec:attention">4</a>) are discussing
the first principles of modern DL from the early Perceptron to more
modern frameworks such as Large Language Models (LLM).</p>
<p>Additional snippets of code are included to make this chapter more
insightful and valuable for newcomers.</p>
<h3 id="sec:history">A Brief History of Artificial Intelligence</h3>
<figure id="fig:dartmouth">
<img src="./figures/boai_dartmouth.png"
alt="Photography of seven of the Dartmouth workshop participants. From left to right: John McCarthy, Marvin Lee Minsky, Nathaniel Rochester, Claude Elwood Shannon, Ray Solomonoff, Trenchard More, and Oliver Gordon Selfridge. Credit: Margaret Minksy" />
<figcaption>Figure 1: Photography of seven of the Dartmouth workshop
participants. From left to right: John McCarthy, Marvin Lee Minsky,
Nathaniel Rochester, Claude Elwood Shannon, Ray Solomonoff, Trenchard
More, and Oliver Gordon Selfridge. Credit: Margaret Minksy</figcaption>
</figure>
<p>The history of the field of AI is not a simple linear and
straightforward story. The field had its success and failures. The term
Artificial Intelligence (AI) has first been introduced in 1956 by John
Mc Carthy and Marvin Lee Minsky at a workshop sponsored by Dartmouth
College <span class="citation" data-cites="dartmouth_2006">[<a
href="#ref-dartmouth_2006" role="doc-biblioref">31</a>]</span>,
gathering about twenty researchers and intellectuals such as the
renowned Claude Shannon (see <a href="#fig:dartmouth">Fig 1</a>). The
field’s main questions were supposed to be solved in a short period.</p>
<p>However, the reality has been far less rosy. Over the years, AI has
gone through several “winters”, periods of inactivity and disillusion
where funding was cut and research interest dropped. But with the advent
of Big Data and the rise of Deep Learning (DL), AI is once again in the
spotlight. The following sections provide a brief overview of the
history of AI, from its early days to the current state of the
field.</p>
<h4 id="the-early-years">The Early Years</h4>
<p>The term Artificial Intelligence (AI) was first used at the 1956
Dartmouth Workshop <span class="citation"
data-cites="dartmouth_2006">[<a href="#ref-dartmouth_2006"
role="doc-biblioref">31</a>]</span>, where John McCarthy proposed the
idea of creating a machine that could learn from its mistakes and
improve its performance over time. The twenty researchers and
intellectuals present worked on topics such as the automatic computer,
the use of natural language by machines, neuron nets (Neural Network
(NN)), randomness and creativity, and many more. This was a
revolutionary idea at the time, and the work done at Dartmouth attracted
a great deal of attention and funding.</p>
<p>Much of the early research focused on symbolic AI, which uses symbols
and logical operations to represent and manipulate data. Logic
programming, production rules, semantic nets and frames, knowledge-based
systems, symbolic mathematics, automatons, automated provers, ontologies
and other paradigms were at the core of symbolic AI <span
class="citation" data-cites="russell_2016">[<a href="#ref-russell_2016"
role="doc-biblioref">41</a>]</span>. This approach was based on the
early work of Alan Turing and the development of functional languages
such as the LISP by McCarthy and al. at MIT <span class="citation"
data-cites="mccarthy_1978">[<a href="#ref-mccarthy_1978"
role="doc-biblioref">30</a>]</span>.</p>
<p>One significant contribution of this period was the Perceptron by
Frank Rosenblatt <span class="citation" data-cites="rosenblatt_1958">[<a
href="#ref-rosenblatt_1958" role="doc-biblioref">39</a>]</span>, a
simplified biomimical model of a single neuron. This artificial neuron
fires when the weighted sum of its input is above a predefined
threshold. The weights, scalars attributed to the connection edges of
the neuron’s inputs, are tuned iteratively and manually given supervised
data, inputs with corresponding labels, until good enough classification
accuracy is met.</p>
<h4 id="the-first-ai-winter">The First AI Winter</h4>
<p>The Perceptron was an early example of a connectionist approach,
which uses a network of artificial neurons to process data. The
Perceptron was met with much enthusiasm but was eventually criticized by
Marvin L. Minsky and Seymour Papert <span class="citation"
data-cites="minsky_1969">[<a href="#ref-minsky_1969"
role="doc-biblioref">32</a>]</span>, who argued that it could not solve
a simple XOR problem. The criticisms, as well as other issues, led to a
period of disillusion in the field of AI, known as the “First AI
Winter”. It was a time when AI research lost its momentum and funding
was not abundant anymore.</p>
<h4 id="expert-systems-and-symbolic-ai">Expert Systems and Symbolic
AI</h4>
<p>The eighties saw a resurgence of interest in AI. Expert systems <span
class="citation" data-cites="jackson_1998">[<a href="#ref-jackson_1998"
role="doc-biblioref">18</a>]</span> were the new hot AI topic. They are
made of hierarchical and specialized ensembles of symbolic reasoning
models and are used to solve complex problems. Symbolic AI continued to
prosper as the dominant approach until the mid-nineties.</p>
<p>During this period, AI was developed as logic-based systems,
search-based systems using depth-first-search, and genetic algorithms,
requiring complex engineering and domain-specific knowledge from experts
to work. It was also the time of the first cognitive architectures <span
class="citation" data-cites="lieto_2021">[<a href="#ref-lieto_2021"
role="doc-biblioref">28</a>]</span> inspired by advances in the field of
neuroscience such as SOAR <span class="citation"
data-cites="larid_2019">[<a href="#ref-larid_2019"
role="doc-biblioref">24</a>]</span> and Adaptive Control of
Thought—Rational (ACT-R) <span class="citation"
data-cites="john_1992">[<a href="#ref-john_1992"
role="doc-biblioref">1</a>]</span> attempting at simulating the human
cognitive process for solving and task automation.</p>
<p>Although the connectionist approaches were not well received by the
community at the time, some individuals are known for significant
contributions that later would form the basis for modern NN
architectures. It was the case for Kunihiko Fukushima and his
NeoCognitron <span class="citation" data-cites="fukushima_1980">[<a
href="#ref-fukushima_1980" role="doc-biblioref">9</a>]</span>, or David
E. Rumelhart et al. who introduced the most used learning procedure for
training Multi-Layer Perceptrons (MLP), the backpropagation <span
class="citation" data-cites="rumelhart_1986">[<a
href="#ref-rumelhart_1986" role="doc-biblioref">40</a>]</span>.</p>
<h4 id="the-second-ai-winter">The Second AI Winter</h4>
<p>Unfortunately, this period was also marked by a lack of progress
because of the resource limitations of the time. Those algorithms
required too much power, data, and investments to work. They were not
sufficient to make AI truly successful. The lack of progress in the
eighties led to the “Second AI Winter”. AI research was largely
abandoned during this period. Funding and enthusiasm dwindled.</p>
<h5 id="the-indomitable-researchers">The Indomitable Researchers</h5>
<p>The second AI winter limited research for NN. However, some
indomitable individuals continued their work. During this period,
Vladimir Vapnik et al. developed the Support Vector Machine (SVM) <span
class="citation" data-cites="cortes_1995">[<a href="#ref-cortes_1995"
role="doc-biblioref">6</a>]</span>, a robust non-probabilistic binary
linear classifier. The method has the advantage to generalize well even
with small datasets. Sepp Hochreiter et al. introduced the Long
Short-Term Memory (LSTM) for Recurrent Neural Networks (RNN) <span
class="citation" data-cites="hochreiter_1997">[<a
href="#ref-hochreiter_1997" role="doc-biblioref">17</a>]</span>, a
complex recurrent cell using gates to route the information flow and
simulate long and short-term memory buffers. In 1989, Yann LeCun
provided the first practical and industrial demonstration of
backpropagation at Bell Labs with a Convolutional Neural Network (CNN)
to read handwritten digits <span class="citation"
data-cites="lecun_1989 lecun_1998">[<a href="#ref-lecun_1989"
role="doc-biblioref">26</a>, <a href="#ref-lecun_1998"
role="doc-biblioref">27</a>]</span> later used by the American postal
services to sort letters.</p>
<h4 id="the-modern-deep-learning-success">The Modern Deep Learning
Success</h4>
<p>The next significant evolutionary step for Deep Learning (DL), those
deep hierarchical NN, descendants of the connectionist movement,
occurred in the early twenty-first century. Computers were now faster
and GPUs were developed for high compute parallelization. Data was
starting to be abundant thanks to the internet and the rapid rise of
search engines and social networks. It is the era of Big Data. NN were
competing with SVM. In 2009 Fei-Fei Li and her group launched ImageNet
<span class="citation" data-cites="deng_2009">[<a href="#ref-deng_2009"
role="doc-biblioref">7</a>]</span>, a dataset assembling billions of
labeled images.</p>
<p>By 2011, the speed of GPUs had increased significantly, making it
possible to train CNNs without layer-by-layer pre-training. The rest of
the story includes a succession of deep NN architectures including,
AlexNet <span class="citation" data-cites="krizhevsky_2012">[<a
href="#ref-krizhevsky_2012" role="doc-biblioref">23</a>]</span>, one of
the first award-winning deep CNN, ResNet <span class="citation"
data-cites="he_2016">[<a href="#ref-he_2016"
role="doc-biblioref">14</a>]</span>, introducing residual connections,
the Generative Adversarial Networks (GAN) <span class="citation"
data-cites="goodfellow_2014">[<a href="#ref-goodfellow_2014"
role="doc-biblioref">11</a>]</span>, a high fidelity and high-resolution
generative framework, attention mechanisms with the rise of the
Transformer “Attention is all you Need” architecture <span
class="citation" data-cites="vaswani_2017">[<a href="#ref-vaswani_2017"
role="doc-biblioref">46</a>]</span> present in almost all modern DL
contributions, and more recently the Denoising Diffusion Model (DDM)
<span class="citation" data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">16</a>]</span>, the spiritual autoregressive
successor of the GAN.</p>
<h4 id="deep-learning-milestones">Deep Learning Milestones</h4>
<p>DL is responsible for many AI milestones in the past decade. These
milestones have been essential in advancing the field and enabling its
applications within various sectors. One of the first notable milestones
was AlphaGo from DeepMind in 2016 <span class="citation"
data-cites="silver_2016">[<a href="#ref-silver_2016"
role="doc-biblioref">45</a>]</span>, where an AI system was able to beat
the Korean world champion Lee Se Dol in the game of Go. AlphaGo is an
illustration of the compression and pattern recognition capabilities of
deep NN in combination with efficient search algorithms.</p>
<p>In 2019, AlphStar <span class="citation"
data-cites="vinyals_2019">[<a href="#ref-vinyals_2019"
role="doc-biblioref">47</a>]</span> from DeepMind also was able to
compete and defeat grandmasters in StarCraft the real-time strategy game
of Blizzard. This demonstrated the capability of Deep Learning
algorithms to achieve beyond human-level performance in real-time and
long-term plannification. In 2020, AlphaFold <span class="citation"
data-cites="senior_2020">[<a href="#ref-senior_2020"
role="doc-biblioref">44</a>]</span> improved the Protein Folding
competition by quite a margin, showing that DL could be used to help
solve complex problems that have implications for medical research and
drug discovery. In 2021 a follow-up model, AlphaFold 2 <span
class="citation" data-cites="jumper_2021">[<a href="#ref-jumper_2021"
role="doc-biblioref">19</a>]</span>, was presented as an impressive
successor of AlphaFold, showcasing further advances in this field.</p>
<p>In 2021, Stable Diffusion <span class="citation"
data-cites="rombach_2021">[<a href="#ref-rombach_2021"
role="doc-biblioref">38</a>]</span> from Stability AI was released. This
Latent DDM conditioned on text prompts allows to generate images of
unprecedented quality and met unprecedented public reach. Finally,
Chat-GPT <span class="citation" data-cites="openai_2023">[<a
href="#ref-openai_2023" role="doc-biblioref">3</a>]</span> was released
in 2023 as a chatbot based on GPT3 <span class="citation"
data-cites="brown_2020">[<a href="#ref-brown_2020"
role="doc-biblioref">2</a>]</span> and fine-tuned using Reinforcement
Learning from Human Feedback (RLHF) for natural question-answering
interaction publicly available as a web demo. However, these last two
milestones are also responsible for ethical and societal concerns about
copyright, creativity, and more. This highlights both the potential of
Deep Learning algorithms but also the need for further research around
their implications.</p>
<h3 id="sec:core">Core Principles</h3>
<h4 id="perceptron">Perceptron</h4>
<h4 id="multi-layer-perceptron">Multi-Layer Perceptron</h4>
<h4 id="convolutional-neural-network">Convolutional Neural Network</h4>
<h3 id="sec:generative">Generative Architectures</h3>
<h4 id="autoencoders">Autoencoders</h4>
<h4 id="variational-autoencoders">Variational Autoencoders</h4>
<h4 id="generative-adversarial-networks">Generative Adversarial
Networks</h4>
<h4 id="denoising-diffusion-models">Denoising Diffusion Models</h4>
<h3 id="sec:attention">Attention is all you Need</h3>
<h4 id="multihead-self-attention">Multihead Self-Attention</h4>
<h4 id="large-language-models">Large Language Models</h4>

<h2 id="ch:methodology">Methodology</h2>
<h3 id="implementation">Implementation</h3>
<h3 id="objective-evaluation">Objective Evaluation</h3>
<h3 id="subjective-evaluation">Subjective Evaluation</h3>
<h3 id="reproducibility">Reproducibility</h3>

<h1 id="core">Core</h1>
<h2 id="ch:contrib-1">Contrib I (Find Catchy Explicit Name)</h2>
<h3 id="state-of-the-art">State of the Art</h3>
<h3 id="method">Method</h3>
<h3 id="setup">Setup</h3>
<h3 id="results">Results</h3>
<h3 id="summary">Summary</h3>

<h2 id="ch:contrib-2">Contrib II (Find Catchy Explicit Name)</h2>
<h3 id="state-of-the-art-1">State of the Art</h3>
<h3 id="method-1">Method</h3>
<h3 id="setup-1">Setup</h3>
<h3 id="results-1">Results</h3>
<h3 id="summary-1">Summary</h3>

<h2 id="ch:contrib-3">Contrib III (Find Catchy Explicit Name)</h2>
<h3 id="state-of-the-art-2">State of the Art</h3>
<h3 id="method-2">Method</h3>
<h3 id="setup-2">Setup</h3>
<h3 id="results-2">Results</h3>
<h3 id="summary-2">Summary</h3>

<h2 id="ch:contrib-4">Contrib IV (Find Catchy Explicit Name)</h2>
<h3 id="state-of-the-art-3">State of the Art</h3>
<h3 id="method-3">Method</h3>
<h3 id="setup-3">Setup</h3>
<h3 id="results-3">Results</h3>
<h3 id="summary-3">Summary</h3>

<h1 id="reflection">Reflection</h1>
<h2 id="ch:ethical-and-societal-impact">Ethical and Societal Impact</h2>

<h2 id="ch:conclusion">Conclusion</h2>

<h2 class="unnumbered" id="references">References</h2>
<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-john_1992" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div
class="csl-right-inline">Anderson, J.R. 1992. Automaticity and the ACT
theory. <em>The American Journal of Psychology</em>. 105, 2 (1992),
165–180. DOI:https://doi.org/<a
href="https://doi.org/10.2307/1423026">10.2307/1423026</a>.</div>
</div>
<div id="ref-brown_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div
class="csl-right-inline">Brown, T., Mann, B., Ryder, N., Subbiah, M.,
Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. 2020. Language models are few-shot learners.
<em>Advances in neural information processing systems</em>. 33, (2020),
1877–1901.</div>
</div>
<div id="ref-openai_2023" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div
class="csl-right-inline">CHATGPT: Optimizing language models for
dialogue: 2023. <a
href="https://openai.com/blog/chatgpt/"><em>https://openai.com/blog/chatgpt/</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-ci_2018" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">Ci,
Y., Ma, X., Wang, Z., Li, H. and Luo, Z. 2018. <a
href="https://doi.org/10.1145/3240508.3240661">User-guided deep anime
line art colorization with conditional adversarial networks</a>.
<em>Proceedings of the 26th ACM international conference on
multimedia</em> (New York, NY, USA, 2018), 1536–1544.</div>
</div>
<div id="ref-clipstudiopaint" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div
class="csl-right-inline">Clip studio PAINT: <a
href="https://www.clipstudio.net/"><em>https://www.clipstudio.net/</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-cortes_1995" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div
class="csl-right-inline">Cortes, C. and Vapnik, V. 1995. Support-vector
networks. <em>Machine Learning</em>. 20, 3 (Sep. 1995), 273–297.
DOI:https://doi.org/<a
href="https://doi.org/10.1007/BF00994018">10.1007/BF00994018</a>.</div>
</div>
<div id="ref-deng_2009" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div
class="csl-right-inline">Deng, J., Dong, W., Socher, R., Li, L.-J., Li,
K. and Fei-Fei, L. 2009. <a
href="https://doi.org/10.1109/CVPR.2009.5206848">ImageNet: A large-scale
hierarchical image database</a>. <em>2009 IEEE conference on computer
vision and pattern recognition</em> (2009), 248–255.</div>
</div>
<div id="ref-frans_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div
class="csl-right-inline">Frans, K. 2017. <a
href="http://arxiv.org/abs/1704.08834">Outline colorization through
tandem adversarial networks</a>. <em>CoRR</em>. abs/1704.08834,
(2017).</div>
</div>
<div id="ref-fukushima_1980" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div
class="csl-right-inline">Fukushima, K. 1980. Neocognitron: A
self-organizing neural network model for a mechanism of pattern
recognition unaffected by shift in position. <em>Biological
Cybernetics</em>. 36, 4 (Apr. 1980), 193–202. DOI:https://doi.org/<a
href="https://doi.org/10.1007/BF00344251">10.1007/BF00344251</a>.</div>
</div>
<div id="ref-furusawa_2O17" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div
class="csl-right-inline">Furusawa, C., Hiroshiba, K., Ogaki, K. and
Odagiri, Y. 2017. <a
href="https://doi.org/10.1145/3145749.3149430">Comicolorization:
Semi-automatic manga colorization</a>. <em>SIGGRAPH asia 2017 technical
briefs</em> (New York, NY, USA, 2017).</div>
</div>
<div id="ref-goodfellow_2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div
class="csl-right-inline">Goodfellow, I., Pouget-Abadie, J., Mirza, M.,
Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y. 2014.
<a
href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">Generative
adversarial nets</a>. <em>Advances in neural information processing
systems</em> (2014).</div>
</div>
<div id="ref-hati_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div
class="csl-right-inline">Hati, Y., Jouet, G., Rousseaux, F. and Duhart,
C. 2019. <a href="https://doi.org/10.1145/3359998.3369401">PaintsTorch:
A user-guided anime line art colorization tool with double generator
conditional adversarial network</a>. <em>European conference on visual
media production</em> (New York, NY, USA, 2019).</div>
</div>
<div id="ref-hati_2022" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div
class="csl-right-inline">Hati, Y., Thevenin, V., Nolot, F., Rousseaux,
F. and Duhart, C. 2022. StencilTorch: An iterative and user-guided
framework for anime lineart colorization. (2022).</div>
</div>
<div id="ref-he_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div
class="csl-right-inline">He, K., Zhang, X., Ren, S. and Sun, J. 2016.
Deep residual learning for image recognition. <em>Proceedings of the
IEEE conference on computer vision and pattern recognition</em> (2016),
770–778.</div>
</div>
<div id="ref-hensman_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div
class="csl-right-inline">Hensman, P. and Aizawa, K. 2017. <a
href="https://doi.org/10.1109/ICDAR.2017.295">cGAN-based manga
colorization using a single training image</a>. <em>2017 14th IAPR
international conference on document analysis and recognition
(ICDAR)</em> (Los Alamitos, CA, USA, Nov. 2017), 72–77.</div>
</div>
<div id="ref-ho_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div
class="csl-right-inline">Ho, J., Jain, A. and Abbeel, P. 2020. Denoising
diffusion probabilistic models. <em>Advances in Neural Information
Processing Systems</em>. 33, (2020), 6840–6851.</div>
</div>
<div id="ref-hochreiter_1997" class="csl-entry" role="listitem">
<div class="csl-left-margin">[17] </div><div
class="csl-right-inline">Hochreiter, S. and Schmidhuber, J. 1997.
<span>Long Short-Term Memory</span>. <em>Neural Computation</em>. 9, 8
(Nov. 1997), 1735–1780. DOI:https://doi.org/<a
href="https://doi.org/10.1162/neco.1997.9.8.1735">10.1162/neco.1997.9.8.1735</a>.</div>
</div>
<div id="ref-jackson_1998" class="csl-entry" role="listitem">
<div class="csl-left-margin">[18] </div><div
class="csl-right-inline">Jackson, P. 1998. <em>Introduction to expert
systems</em>. Addison-Wesley Longman Publishing Co., Inc.</div>
</div>
<div id="ref-jumper_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[19] </div><div
class="csl-right-inline">Jumper, J. et al. 2021. Highly accurate protein
structure prediction with AlphaFold. <em>Nature</em>. 596, 7873 (Aug.
2021), 583–589. DOI:https://doi.org/<a
href="https://doi.org/10.1038/s41586-021-03819-2">10.1038/s41586-021-03819-2</a>.</div>
</div>
<div id="ref-kandinsky_1977" class="csl-entry" role="listitem">
<div class="csl-left-margin">[20] </div><div
class="csl-right-inline">Kandinsky, W. and Sadleir, M. 1977.
<em>Concerning the spiritual in art</em>. Dover Publications.</div>
</div>
<div id="ref-kim_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[21] </div><div
class="csl-right-inline">Kim, H., Jhoo, H.Y., Park, E. and Yoo, S. 2019.
<a href="https://doi.org/10.1109/ICCV.2019.00915">Tag2Pix: Line art
colorization using text tag with SECat and changing loss</a>. <em>2019
IEEE/CVF international conference on computer vision (ICCV)</em> (2019),
9055–9064.</div>
</div>
<div id="ref-kingma_2013" class="csl-entry" role="listitem">
<div class="csl-left-margin">[22] </div><div
class="csl-right-inline">Kingma, D.P. and Welling, M. 2013.
Auto-encoding variational bayes. <em>arXiv preprint
arXiv:1312.6114</em>. (2013).</div>
</div>
<div id="ref-krizhevsky_2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">[23] </div><div
class="csl-right-inline">Krizhevsky, A., Sutskever, I. and Hinton, G.E.
2012. <a
href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">ImageNet
classification with deep convolutional neural networks</a>. <em>Advances
in neural information processing systems</em> (2012).</div>
</div>
<div id="ref-larid_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[24] </div><div
class="csl-right-inline">Laird, J.E. 2019. <em>The soar cognitive
architecture</em>. The MIT Press.</div>
</div>
<div id="ref-lecun_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[25] </div><div class="csl-right-inline">Le
Cun, Y. 2019. <em><a
href="https://www.odilejacob.fr/catalogue/sciences-humaines/questions-de-societe/quand-la-machine-apprend_9782738149312.php">Quand
la machine apprend: La r<span>é</span>volution des neurones artificiels
et de l’apprentissage profond</a></em>. Odile Jacob.</div>
</div>
<div id="ref-lecun_1989" class="csl-entry" role="listitem">
<div class="csl-left-margin">[26] </div><div
class="csl-right-inline">LeCun, Y., Boser, B., Denker, J.S., Henderson,
D., Howard, R.E., Hubbard, W. and Jackel, L.D. 1989. <span
class="nocase">Backpropagation Applied to Handwritten Zip Code
Recognition</span>. <em>Neural Computation</em>. 1, 4 (Dec. 1989),
541–551. DOI:https://doi.org/<a
href="https://doi.org/10.1162/neco.1989.1.4.541">10.1162/neco.1989.1.4.541</a>.</div>
</div>
<div id="ref-lecun_1998" class="csl-entry" role="listitem">
<div class="csl-left-margin">[27] </div><div
class="csl-right-inline">Lecun, Y., Bottou, L., Bengio, Y. and Haffner,
P. 1998. Gradient-based learning applied to document recognition.
<em>Proceedings of the IEEE</em>. 86, 11 (1998), 2278–2324.
DOI:https://doi.org/<a
href="https://doi.org/10.1109/5.726791">10.1109/5.726791</a>.</div>
</div>
<div id="ref-lieto_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[28] </div><div
class="csl-right-inline">Lieto, A. 2021. <em><a
href="https://doi.org/10.4324/9781315460536">Cognitive design for
artificial minds (1st ed.)</a></em>. Routledge.</div>
</div>
<div id="ref-liu_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[29] </div><div
class="csl-right-inline">Liu, Y., Qin, Z., Wan, T. and Luo, Z. 2018.
Auto-painter: Cartoon image generation from sketch by using conditional
wasserstein generative adversarial networks. <em>Neurocomputing</em>.
311, (2018), 78–87. DOI:https://doi.org/<a
href="https://doi.org/10.1016/j.neucom.2018.05.045">10.1016/j.neucom.2018.05.045</a>.</div>
</div>
<div id="ref-mccarthy_1978" class="csl-entry" role="listitem">
<div class="csl-left-margin">[30] </div><div
class="csl-right-inline">McCarthy, J. 1978. <a
href="https://doi.org/10.1145/800025.1198360">History of LISP</a>.
<em>History of programming languages</em>. Association for Computing
Machinery. 173–185.</div>
</div>
<div id="ref-dartmouth_2006" class="csl-entry" role="listitem">
<div class="csl-left-margin">[31] </div><div
class="csl-right-inline">McCarthy, J., Minsky, M.L., Rochester, N. and
Shannon, C.E. 2006. A proposal for the dartmouth summer research project
on artificial intelligence, august 31, 1955. <em>AI Magazine</em>. 27, 4
(2006), 12. DOI:https://doi.org/<a
href="https://doi.org/10.1609/aimag.v27i4.1904">10.1609/aimag.v27i4.1904</a>.</div>
</div>
<div id="ref-minsky_1969" class="csl-entry" role="listitem">
<div class="csl-left-margin">[32] </div><div
class="csl-right-inline">Minsky, M. and Papert, S. 1969.
<em>Perceptrons: An introduction to computational geometry</em>. MIT
Press.</div>
</div>
<div id="ref-mumford_2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">[33] </div><div
class="csl-right-inline">Mumford, M., Medeiros, K. and Partlow, P. 2012.
Creative thinking: Processes, strategies, and knowledge. <em>The Journal
of Creative Behavior</em>. 46, (Mar. 2012). DOI:https://doi.org/<a
href="https://doi.org/10.1002/jocb.003">10.1002/jocb.003</a>.</div>
</div>
<div id="ref-newell_1959" class="csl-entry" role="listitem">
<div class="csl-left-margin">[34] </div><div
class="csl-right-inline">Newell, A., Shaw, J.C. and Simon, H.A. 1959.
<em><a href="https://doi.org/10.1037/13117-003">The processes of
creative thinking</a></em>. RAND Corporation.</div>
</div>
<div id="ref-paintman" class="csl-entry" role="listitem">
<div class="csl-left-margin">[35] </div><div
class="csl-right-inline">Paintman: <a
href="http://www.retasstudio.net/products/paintman/"><em>http://www.retasstudio.net/products/paintman/</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-paintschainer_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[36] </div><div
class="csl-right-inline">Pelica paint: 2017. <a
href="https://petalica-paint.pixiv.dev/index_en.html"><em>https://petalica-paint.pixiv.dev/index_en.html</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-photoshop" class="csl-entry" role="listitem">
<div class="csl-left-margin">[37] </div><div
class="csl-right-inline">Photoshop: <a
href="https://www.adobe.com/products/photoshop.html"><em>https://www.adobe.com/products/photoshop.html</em></a>.
Accessed: 2023-01-26.</div>
</div>
<div id="ref-rombach_2021" class="csl-entry" role="listitem">
<div class="csl-left-margin">[38] </div><div
class="csl-right-inline">Rombach, R., Blattmann, A., Lorenz, D., Esser,
P. and Ommer, B. 2021. <a
href="https://arxiv.org/abs/2112.10752">High-resolution image synthesis
with latent diffusion models</a>.</div>
</div>
<div id="ref-rosenblatt_1958" class="csl-entry" role="listitem">
<div class="csl-left-margin">[39] </div><div
class="csl-right-inline">Rosenblatt, F. 1958. <span class="nocase">The
perceptron: A probabilistic model for information storage and
organization in the brain.</span> <em>Psychological Review</em>. 65, 6
(1958), 386–408. DOI:https://doi.org/<a
href="https://doi.org/10.1037/h0042519">10.1037/h0042519</a>.</div>
</div>
<div id="ref-rumelhart_1986" class="csl-entry" role="listitem">
<div class="csl-left-margin">[40] </div><div
class="csl-right-inline">Rumelhart, D.E., Hinton, G.E. and Williams,
R.J. 1986. Learning representations by back-propagating errors.
<em>Nature</em>. 323, 6088 (Oct. 1986), 533–536. DOI:https://doi.org/<a
href="https://doi.org/10.1038/323533a0">10.1038/323533a0</a>.</div>
</div>
<div id="ref-russell_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[41] </div><div
class="csl-right-inline">Russell, S.J. and Norvig, P. 2009.
<em>Artificial intelligence: A modern approach</em>. Pearson.</div>
</div>
<div id="ref-saito_2015" class="csl-entry" role="listitem">
<div class="csl-left-margin">[42] </div><div
class="csl-right-inline">Saito, M. and Matsui, Y. 2015. <a
href="https://doi.org/10.1145/2820903.2820907">Illustration2Vec: A
semantic vector representation of illustrations</a>. <em>SIGGRAPH asia
2015 technical briefs</em> (New York, NY, USA, 2015), 5:1–5:4.</div>
</div>
<div id="ref-sangkloy_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[43] </div><div
class="csl-right-inline">Sangkloy, P., Lu, J., Fang, C., Yu, F. and
Hays, J. 2017. <a
href="https://doi.org/10.1109/CVPR.2017.723">Scribbler: Controlling deep
image synthesis with sketch and color</a>. <em>2017 <span>IEEE</span>
conference on computer vision and pattern recognition, <span>CVPR</span>
2017, honolulu, HI, USA, july 21-26, 2017</em> (2017), 6836–6845.</div>
</div>
<div id="ref-senior_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[44] </div><div
class="csl-right-inline">Senior, A.W. et al. 2020. Improved protein
structure prediction using potentials from deep learning.
<em>Nature</em>. 577, 7792 (Jan. 2020), 706–710. DOI:https://doi.org/<a
href="https://doi.org/10.1038/s41586-019-1923-7">10.1038/s41586-019-1923-7</a>.</div>
</div>
<div id="ref-silver_2016" class="csl-entry" role="listitem">
<div class="csl-left-margin">[45] </div><div
class="csl-right-inline">Silver, D. et al. 2016. Mastering the game of
go with deep neural networks and tree search. <em>Nature</em>. 529, 7587
(Jan. 2016), 484–489. DOI:https://doi.org/<a
href="https://doi.org/10.1038/nature16961">10.1038/nature16961</a>.</div>
</div>
<div id="ref-vaswani_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[46] </div><div
class="csl-right-inline">Vaswani, A., Shazeer, N., Parmar, N.,
Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I.
2017. Attention is all you need. <em>Advances in neural information
processing systems</em>. 30, (2017).</div>
</div>
<div id="ref-vinyals_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[47] </div><div
class="csl-right-inline">Vinyals, O. et al. 2019. Grandmaster level in
StarCraft II using multi-agent reinforcement learning. <em>Nature</em>.
575, 7782 (Nov. 2019), 350–354. DOI:https://doi.org/<a
href="https://doi.org/10.1038/s41586-019-1724-z">10.1038/s41586-019-1724-z</a>.</div>
</div>
<div id="ref-wolf_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[48] </div><div
class="csl-right-inline">Wolf, M.J., Miller, K. and Grodzinsky, F.S.
2017. Why we should have seen that coming: Comments on microsoft’s tay
"experiment," and wider implications. <em>SIGCAS Comput. Soc.</em> 47, 3
(Sep. 2017), 54–64. DOI:https://doi.org/<a
href="https://doi.org/10.1145/3144592.3144598">10.1145/3144592.3144598</a>.</div>
</div>
<div id="ref-zhang_ji_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[49] </div><div
class="csl-right-inline">Zhang, L., Ji, Y., Lin, X. and Liu, C. 2017. <a
href="https://doi.org/10.1109/ACPR.2017.61">Style transfer for anime
sketches with enhanced residual u-net and auxiliary classifier GAN</a>.
<em>2017 4th IAPR asian conference on pattern recognition (ACPR)</em>
(2017), 506–511.</div>
</div>
<div id="ref-zhang_richard_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[50] </div><div
class="csl-right-inline">Zhang, R., Zhu, J.-Y., Isola, P., Geng, X.,
Lin, A.S., Yu, T. and Efros, A.A. 2017. Real-time user-guided image
colorization with learned deep priors. <em>ACM Trans. Graph.</em> 36, 4
(Jul. 2017). DOI:https://doi.org/<a
href="https://doi.org/10.1145/3072959.3073703">10.1145/3072959.3073703</a>.</div>
</div>
</div>
</body>
</html>
