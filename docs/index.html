<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Yliess Hati" />
  <meta name="keywords" content="keyword" />
  <title>AI-Assisted Creative Expression: a Case for Automatic Lineart Colorization</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title"><p>AI-Assisted Creative Expression: a Case for
Automatic Lineart Colorization</p></h1>
<p class="author">Yliess Hati</p>
</header>
<nav id="TOC" role="doc-toc">
<h2 id="toc-title">Contents</h2>
<ul>
<li><a href="#abstract" id="toc-abstract">Abstract</a></li>
<li><a href="#introduction" id="toc-introduction">Introduction</a>
<ul>
<li><a href="#motivations" id="toc-motivations">Motivations</a></li>
<li><a href="#problem-statement" id="toc-problem-statement">Problem
Statement</a></li>
<li><a href="#contributions"
id="toc-contributions">Contributions</a></li>
<li><a href="#concerns" id="toc-concerns">Concerns</a></li>
<li><a href="#outline" id="toc-outline">Outline</a></li>
</ul></li>
<li><a href="#background" id="toc-background">Background</a>
<ul>
<li><a href="#history-of-artificial-intelligence"
id="toc-history-of-artificial-intelligence">History of Artificial
Intelligence</a></li>
<li><a href="#neural-networks" id="toc-neural-networks">Neural
Networks</a></li>
<li><a href="#autoencoders" id="toc-autoencoders">Autoencoders</a></li>
<li><a href="#variational-autoencoders"
id="toc-variational-autoencoders">Variational Autoencoders</a></li>
<li><a href="#generative-adversarial-networks"
id="toc-generative-adversarial-networks">Generative Adversarial
Networks</a></li>
<li><a href="#denoising-diffusion-models"
id="toc-denoising-diffusion-models">Denoising Diffusion Models</a></li>
</ul></li>
<li><a href="#methodology" id="toc-methodology">Methodology</a>
<ul>
<li><a href="#implementation"
id="toc-implementation">Implementation</a></li>
<li><a href="#objective-evaluation"
id="toc-objective-evaluation">Objective Evaluation</a></li>
<li><a href="#subjective-evaluation"
id="toc-subjective-evaluation">Subjective Evaluation</a></li>
<li><a href="#reproducibility"
id="toc-reproducibility">Reproducibility</a></li>
</ul></li>
<li><a href="#contrib-i-find-catchy-explicit-name"
id="toc-contrib-i-find-catchy-explicit-name">Contrib I (Find Catchy
Explicit Name)</a>
<ul>
<li><a href="#state-of-the-art" id="toc-state-of-the-art">State of the
Art</a></li>
<li><a href="#method" id="toc-method">Method</a></li>
<li><a href="#setup" id="toc-setup">Setup</a></li>
<li><a href="#results" id="toc-results">Results</a></li>
<li><a href="#summary" id="toc-summary">Summary</a></li>
</ul></li>
<li><a href="#contrib-ii-find-catchy-explicit-name"
id="toc-contrib-ii-find-catchy-explicit-name">Contrib II (Find Catchy
Explicit Name)</a>
<ul>
<li><a href="#state-of-the-art-1" id="toc-state-of-the-art-1">State of
the Art</a></li>
<li><a href="#method-1" id="toc-method-1">Method</a></li>
<li><a href="#setup-1" id="toc-setup-1">Setup</a></li>
<li><a href="#results-1" id="toc-results-1">Results</a></li>
<li><a href="#summary-1" id="toc-summary-1">Summary</a></li>
</ul></li>
<li><a href="#contrib-iii-find-catchy-explicit-name"
id="toc-contrib-iii-find-catchy-explicit-name">Contrib III (Find Catchy
Explicit Name)</a>
<ul>
<li><a href="#state-of-the-art-2" id="toc-state-of-the-art-2">State of
the Art</a></li>
<li><a href="#method-2" id="toc-method-2">Method</a></li>
<li><a href="#setup-2" id="toc-setup-2">Setup</a></li>
<li><a href="#results-2" id="toc-results-2">Results</a></li>
<li><a href="#summary-2" id="toc-summary-2">Summary</a></li>
</ul></li>
<li><a href="#contrib-iv-find-catchy-explicit-name"
id="toc-contrib-iv-find-catchy-explicit-name">Contrib IV (Find Catchy
Explicit Name)</a>
<ul>
<li><a href="#state-of-the-art-3" id="toc-state-of-the-art-3">State of
the Art</a></li>
<li><a href="#method-3" id="toc-method-3">Method</a></li>
<li><a href="#setup-3" id="toc-setup-3">Setup</a></li>
<li><a href="#results-3" id="toc-results-3">Results</a></li>
<li><a href="#summary-3" id="toc-summary-3">Summary</a></li>
</ul></li>
<li><a href="#ethical-and-societal-impact"
id="toc-ethical-and-societal-impact">Ethical and Societal
Impact</a></li>
<li><a href="#conclusion" id="toc-conclusion">Conclusion</a></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</nav>
<h2 id="abstract">Abstract</h2>

<h2 id="introduction">Introduction</h2>
<p>Humans possess the ability to perceive and understand the world
allowing us to accomplish a wide range of complex tasks through the
combination of visual recognition, scene understanding, and
communication. The ability to quickly and accurately extract information
from a single image is a testament to the complexity and sophistication
of the human brain and is often taken for granted. One of the Artificial
Intelligence (AI) field’s ultimate goals is to empower computers with
such human-like abilities, one of them being creativity, being able to
produce something original and worthwhile <span class="citation"
data-cites="mumford_2012">[<a href="#ref-mumford_2012"
role="doc-biblioref">9</a>]</span>.</p>
<p>Computational creativity is the field at the intersection of AI,
cognitive psychology, philosophy, and art, which aims at understanding,
simulating, replicating, or in some cases enhancing human creativity.
One definition of computational creativity <span class="citation"
data-cites="newell_1959">[<a href="#ref-newell_1959"
role="doc-biblioref">10</a>]</span> is the ability to produce something
that is novel and useful, demands that we reject common beliefs, results
from intense motivation and persistence, or comes from clarifying a
vague problem. Top-down approaches to this definition use a mix of
explicit formulations of recipes and randomness such as procedural
generation. On the opposite, bottom-up approaches use Artificial Neural
Networks (ANN) to learn patterns and heuristics from large datasets to
enable non-linear generation.</p>
<p>We, as a species, are currently witnessing the beginning of a new era
where the gap between machines and humans is starting to blur. Current
breakthroughs in the field of AI, more specifically in Deep Learning
(DL), are giving computers the ability to perceive and understand our
world, but also to interact with our environment using natural
interactions such as speech and natural language. ANNs, once mocked by
the AI community <span class="citation" data-cites="lecun_2019">[<a
href="#ref-lecun_2019" role="doc-biblioref">8</a>]</span>, are now
trainable using Gradient Descent (GD) <span class="citation"
data-cites="rumelhart_1986">[<a href="#ref-rumelhart_1986"
role="doc-biblioref">11</a>]</span> thanks to the massive availability
of data and the processing power of modern hardware accelerators such as
Graphical Processing Units (GPU), Tensor Processing Units (TPU), and
Neural Processing Units (NPU).</p>
<p>Neural Networks (NN), those trainable general function approximators,
gave rise to the field of generative NN. Specialized DL architectures
such as Variational Autoencoders (VAE) <span class="citation"
data-cites="kingma_2013">[<a href="#ref-kingma_2013"
role="doc-biblioref">7</a>]</span>, Generative Adversarial Networks
(GAN) <span class="citation" data-cites="goodfellow_2014">[<a
href="#ref-goodfellow_2014" role="doc-biblioref">3</a>]</span>,
Denoising Diffusion Models (DDM) <span class="citation"
data-cites="ho_2020">[<a href="#ref-ho_2020"
role="doc-biblioref">5</a>]</span>, and Large Language Models (LLM)
<span class="citation" data-cites="vaswani_2017 brown_2020">[<a
href="#ref-brown_2020" role="doc-biblioref">1</a>, <a
href="#ref-vaswani_2017" role="doc-biblioref">12</a>]</span> are used to
generate artifacts such as text, audio, images, and videos of
unprecedented quality and complexity.</p>
<p>This thesis aims at exploring how one could train and use generative
NN to create AI-powered tools capable of enhancing human creative
expression. The task of automatic lineart colorization act as the
example case used to illustrate this process throughout the entire
thesis.</p>
<h3 id="motivations">Motivations</h3>
<p>The most common digital illustration process can be broken down into
four distinct stages: sketching, inking, coloring, and post-processing.
As demonstrated by the work of Kandinsky <span class="citation"
data-cites="kandinsky_1977">[<a href="#ref-kandinsky_1977"
role="doc-biblioref">6</a>]</span>, the colorization process can greatly
impact the overall meaning of a piece of art through the introduction of
various color schemes, shading, and textures. These elements of the
coloring process present significant challenges for the Computer Vision
(CV) task of automatic lineart colorization, particularly in comparison
to its grayscale counterpart <span class="citation"
data-cites="furusawa_2O17 hensman_2017 zhang_richard_2017">[<a
href="#ref-furusawa_2O17" role="doc-biblioref">2</a>, <a
href="#ref-hensman_2017" role="doc-biblioref">4</a>, <a
href="#ref-zhang_richard_2017" role="doc-biblioref">13</a>]</span>.
Without the added semantic information provided by textures and shadows,
inferring materials and 3D shapes from black and white linearts only is
difficult. They can only be deduced from silhouettes.</p>
<h3 id="problem-statement">Problem Statement</h3>
<blockquote>
<ul>
<li>Black &amp; White Lineart VS Gray Scale</li>
<li>Incomplete Information Challenge fo Computer Vision</li>
<li>Natural Artisitic Control Back to the User</li>
</ul>
</blockquote>
<h3 id="contributions">Contributions</h3>
<blockquote>
<ul>
<li>Recipe for curating datasets for the task of automatic
colorization</li>
<li>3 Models exploring different aspect of the topic:
<ul>
<li>PaintsTorch: High Quality, User-Guided, Fast Realtime Feedback</li>
<li>StencilTorch: Human-Machine Collaboration, Human-in-the-Loop</li>
<li>StableTorch: Variance and Iterative Exploration</li>
</ul></li>
<li>A reflexion on Current Generative AI Ethical and Societal Impact in
our Society</li>
</ul>
</blockquote>
<h3 id="concerns">Concerns</h3>
<blockquote>
<ul>
<li>Raise awareness about
<ul>
<li>Deepfakes</li>
<li>Model Fabulations</li>
<li>Ownership &amp; Copyright Ambiguities</li>
<li>Biases &amp; Discrimination</li>
</ul></li>
<li>About this work
<ul>
<li>Images used only for Educational and Research Purposes</li>
<li>Only describe recipes for reproducibility</li>
<li>Dataset and Weights are not Distributed (Only Code)</li>
</ul></li>
</ul>
</blockquote>
<h3 id="outline">Outline</h3>
<blockquote>
<ul>
<li>Plain Language Expanded TOC</li>
</ul>
</blockquote>
<h2 id="background">Background</h2>
<h3 id="history-of-artificial-intelligence">History of Artificial
Intelligence</h3>
<h3 id="neural-networks">Neural Networks</h3>
<h3 id="autoencoders">Autoencoders</h3>
<h3 id="variational-autoencoders">Variational Autoencoders</h3>
<h3 id="generative-adversarial-networks">Generative Adversarial
Networks</h3>
<h3 id="denoising-diffusion-models">Denoising Diffusion Models</h3>

<h2 id="methodology">Methodology</h2>
<h3 id="implementation">Implementation</h3>
<h3 id="objective-evaluation">Objective Evaluation</h3>
<h3 id="subjective-evaluation">Subjective Evaluation</h3>
<h3 id="reproducibility">Reproducibility</h3>

<h2 id="contrib-i-find-catchy-explicit-name">Contrib I (Find Catchy
Explicit Name)</h2>
<h3 id="state-of-the-art">State of the Art</h3>
<h3 id="method">Method</h3>
<h3 id="setup">Setup</h3>
<h3 id="results">Results</h3>
<h3 id="summary">Summary</h3>

<h2 id="contrib-ii-find-catchy-explicit-name">Contrib II (Find Catchy
Explicit Name)</h2>
<h3 id="state-of-the-art-1">State of the Art</h3>
<h3 id="method-1">Method</h3>
<h3 id="setup-1">Setup</h3>
<h3 id="results-1">Results</h3>
<h3 id="summary-1">Summary</h3>

<h2 id="contrib-iii-find-catchy-explicit-name">Contrib III (Find Catchy
Explicit Name)</h2>
<h3 id="state-of-the-art-2">State of the Art</h3>
<h3 id="method-2">Method</h3>
<h3 id="setup-2">Setup</h3>
<h3 id="results-2">Results</h3>
<h3 id="summary-2">Summary</h3>

<h2 id="contrib-iv-find-catchy-explicit-name">Contrib IV (Find Catchy
Explicit Name)</h2>
<h3 id="state-of-the-art-3">State of the Art</h3>
<h3 id="method-3">Method</h3>
<h3 id="setup-3">Setup</h3>
<h3 id="results-3">Results</h3>
<h3 id="summary-3">Summary</h3>

<h2 id="ethical-and-societal-impact">Ethical and Societal Impact</h2>

<h2 id="conclusion">Conclusion</h2>

<h2 class="unnumbered" id="references">References</h2>
<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-brown_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div
class="csl-right-inline">Brown, T., Mann, B., Ryder, N., Subbiah, M.,
Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. 2020. Language models are few-shot learners.
<em>Advances in neural information processing systems</em>. 33, (2020),
1877–1901.</div>
</div>
<div id="ref-furusawa_2O17" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div
class="csl-right-inline">Furusawa, C., Hiroshiba, K., Ogaki, K. and
Odagiri, Y. 2017. <a
href="https://doi.org/10.1145/3145749.3149430">Comicolorization:
Semi-automatic manga colorization</a>. <em>SIGGRAPH asia 2017 technical
briefs</em> (New York, NY, USA, 2017).</div>
</div>
<div id="ref-goodfellow_2014" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div
class="csl-right-inline">Goodfellow, I., Pouget-Abadie, J., Mirza, M.,
Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y. 2014.
<a
href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">Generative
adversarial nets</a>. <em>Advances in neural information processing
systems</em> (2014).</div>
</div>
<div id="ref-hensman_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div
class="csl-right-inline">Hensman, P. and Aizawa, K. 2017. <a
href="https://doi.org/10.1109/ICDAR.2017.295">cGAN-based manga
colorization using a single training image</a>. <em>2017 14th IAPR
international conference on document analysis and recognition
(ICDAR)</em> (Los Alamitos, CA, USA, Nov. 2017), 72–77.</div>
</div>
<div id="ref-ho_2020" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">Ho,
J., Jain, A. and Abbeel, P. 2020. Denoising diffusion probabilistic
models. <em>Advances in Neural Information Processing Systems</em>. 33,
(2020), 6840–6851.</div>
</div>
<div id="ref-kandinsky_1977" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div
class="csl-right-inline">Kandinsky, W. and Sadleir, M. 1977.
<em>Concerning the spiritual in art</em>. Dover Publications.</div>
</div>
<div id="ref-kingma_2013" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div
class="csl-right-inline">Kingma, D.P. and Welling, M. 2013.
Auto-encoding variational bayes. <em>arXiv preprint
arXiv:1312.6114</em>. (2013).</div>
</div>
<div id="ref-lecun_2019" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">Le
Cun, Y. 2019. <em><a
href="https://www.odilejacob.fr/catalogue/sciences-humaines/questions-de-societe/quand-la-machine-apprend_9782738149312.php">Quand
la machine apprend: La r<span>é</span>volution des neurones artificiels
et de l’apprentissage profond</a></em>. Odile Jacob.</div>
</div>
<div id="ref-mumford_2012" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div
class="csl-right-inline">Mumford, M., Medeiros, K. and Partlow, P. 2012.
Creative thinking: Processes, strategies, and knowledge. <em>The Journal
of Creative Behavior</em>. 46, (Mar. 2012). DOI:https://doi.org/<a
href="https://doi.org/10.1002/jocb.003">10.1002/jocb.003</a>.</div>
</div>
<div id="ref-newell_1959" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div
class="csl-right-inline">Newell, A., Shaw, J.C. and Simon, H.A. 1959.
<em><a href="https://doi.org/10.1037/13117-003">The processes of
creative thinking</a></em>. RAND Corporation.</div>
</div>
<div id="ref-rumelhart_1986" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div
class="csl-right-inline">Rumelhart, D.E., Hinton, G.E. and Williams,
R.J. 1986. Learning representations by back-propagating errors.
<em>Nature</em>. 323, 6088 (Oct. 1986), 533–536. DOI:https://doi.org/<a
href="https://doi.org/10.1038/323533a0">10.1038/323533a0</a>.</div>
</div>
<div id="ref-vaswani_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div
class="csl-right-inline">Vaswani, A., Shazeer, N., Parmar, N.,
Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł. and Polosukhin, I.
2017. Attention is all you need. <em>Advances in neural information
processing systems</em>. 30, (2017).</div>
</div>
<div id="ref-zhang_richard_2017" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div
class="csl-right-inline">Zhang, R., Zhu, J.-Y., Isola, P., Geng, X.,
Lin, A.S., Yu, T. and Efros, A.A. 2017. Real-time user-guided image
colorization with learned deep priors. <em>ACM Trans. Graph.</em> 36, 4
(Jul. 2017). DOI:https://doi.org/<a
href="https://doi.org/10.1145/3072959.3073703">10.1145/3072959.3073703</a>.</div>
</div>
</div>
</body>
</html>
