@article{mumford_2012,
    author = {Mumford, Michael and Medeiros, Kelsey and Partlow, Paul},
    year = {2012},
    month = {03},
    pages = {},
    title = {Creative Thinking: Processes, Strategies, and Knowledge},
    volume = {46},
    journal = {The Journal of Creative Behavior},
    doi = {10.1002/jocb.003}
}

@book{newell_1959,
    author={Newell, Allen and J. C. Shaw and Herbert Alexander Simon},
    title={The Processes of Creative Thinking},
    address={Santa Monica, CA},
    year={1959},
    doi={10.1037/13117-003},
    publisher={RAND Corporation}
}

@article{rumelhart_1986,
    author={Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
    title={Learning representations by back-propagating errors},
    journal={Nature},
    year={1986},
    month={Oct},
    day={01},
    volume={323},
    number={6088},
    pages={533-536},
    abstract={We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
    issn={1476-4687},
    doi={10.1038/323533a0},
}

@book{lecun_2019,
    title={Quand la machine apprend: La r{\'e}volution des neurones artificiels et de l'apprentissage profond},
    author={Le Cun, Y.},
    isbn={9782738149329},
    url={https://www.odilejacob.fr/catalogue/sciences-humaines/questions-de-societe/quand-la-machine-apprend_9782738149312.php},
    year={2019},
    publisher={Odile Jacob}
}

@article{kingma_2013,
    title={Auto-encoding variational bayes},
    author={Kingma, Diederik P and Welling, Max},
    journal={arXiv preprint arXiv:1312.6114},
    year={2013}
}

@inproceedings{goodfellow_2014,
    author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
    pages={},
    publisher={Curran Associates, Inc.},
    title={Generative Adversarial Nets},
    url={https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
    volume={27},
    year={2014}
}

@article{ho_2020,
    title={Denoising diffusion probabilistic models},
    author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
    journal={Advances in Neural Information Processing Systems},
    volume={33},
    pages={6840--6851},
    year={2020}
}

@article{vaswani_2017,
    title={Attention is all you need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    journal={Advances in neural information processing systems},
    volume={30},
    year={2017}
}

@article{brown_2020,
    title={Language models are few-shot learners},
    author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
    journal={Advances in neural information processing systems},
    volume={33},
    pages={1877--1901},
    year={2020}
}

@online{photoshop,
    author = {Adobe},
    title = {Photoshop},
    url = {https://www.adobe.com/products/photoshop.html},
    urldate = {2023-01-26}
}

@online{clipstudiopaint,
    author = {Celsys, Inc.},
    title = {Clip Studio PAINT},
    url = {https://www.clipstudio.net/},
    urldate = {2023-01-26}
}

@online{paintman,
    author = {RETAS STUDIO},
    title = {Paintman},
    url = {http://www.retasstudio.net/products/paintman/},
    urldate = {2023-01-26}
}

@book{kandinsky_1977,
    address={New York},
    title={Concerning the spiritual in art},
    isbn={978-0-486-23411-3},
    abstract={A pioneering work in the movement to free art from its traditional bonds to material reality, this book is one of the most important documents in the history of modern art. Written by the famous nonobjective painter Wassily Kandinsky (1866-1944), it explains Kandinsky's own theory of painting and crystallizes the ideas that were influencing many other modern artists of the period. Along with his own ground-breaking paintings, this book had a tremendous impact on the development of modern art. The first part issues a call for a spiritual revolution in painting that will let artists express their own inner lives in abstract, non-material terms. Just as musicians do not depend upon the material world for their music, so artists should not have to depend upon the material world for their art. In the second part, Kandinsky discusses the psychology of colors, the language of form and color, and the responsibilities of the artist. An Introduction by the translator offers additional explanation of Kandinsky's art and theories.--From publisher description.},
    language={Translation of Ãœber das Geistige in der Kunst},
    publisher={Dover Publications},
    author={Kandinsky, Wassily and Sadleir, Michael},
    year={1977},
    note={OCLC: 3042682},
}

@inproceedings{furusawa_2O17,
    author={Furusawa, Chie and Hiroshiba, Kazuyuki and Ogaki, Keisuke and Odagiri, Yuri},
    title={Comicolorization: Semi-Automatic Manga Colorization},
    year={2017},
    isbn={9781450354066},
    publisher={Association for Computing Machinery},
    address={New York, NY, USA},
    doi={10.1145/3145749.3149430},
    abstract={We developed Comicolorization, a semi-automatic colorization system for manga images. Given a monochrome manga and reference images as inputs, our system generates a plausible color version of the manga. This is the first work to address the colorization of an entire manga title (a set of manga pages). Our method colorizes a whole page (not a single panel) semi-automatically, with the same color for the same character across multiple panels. To colorize the target character by the color from the reference image, we extract a color feature from the reference and feed it to the colorization network to help the colorization. Our approach employs adversarial loss to encourage the effect of the color features. Optionally, our tool allows users to revise the colorization result interactively. By feeding the color features to our deep colorization network, we accomplish colorization of the entire manga using the desired colors for each panel.},
    booktitle={SIGGRAPH Asia 2017 Technical Briefs},
    articleno={12},
    numpages={4},
    keywords={colorization, deep learning, manga},
    location={Bangkok, Thailand},
    series={SA '17}
}

@inproceedings{hensman_2017,
    author={P. Hensman and K. Aizawa},
    booktitle={2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)},
    title={cGAN-Based Manga Colorization Using a Single Training Image},
    year={2017},
    volume={3},
    issn={2379-2140},
    pages={72-77},
    keywords={image color analysis;image segmentation;training;colored noise;image edge detection;quantization (signal);image resolution},
    doi={10.1109/ICDAR.2017.295},
    publisher={IEEE Computer Society},
    address={Los Alamitos, CA, USA},
    month={nov}
}

@article{zhang_richard_2017,
    author={Zhang, Richard and Zhu, Jun-Yan and Isola, Phillip and Geng, Xinyang and Lin, Angela S. and Yu, Tianhe and Efros, Alexei A.},
    title={Real-Time User-Guided Image Colorization with Learned Deep Priors},
    year={2017},
    issue_date={July 2017},
    publisher={Association for Computing Machinery},
    address={New York, NY, USA},
    volume={36},
    number={4},
    issn={0730-0301},
    doi={10.1145/3072959.3073703},
    abstract={We propose a deep learning approach for user-guided image colorization. The system directly maps a grayscale image, along with sparse, local user "hints" to an output colorization with a Convolutional Neural Network (CNN). Rather than using hand-defined rules, the network propagates user edits by fusing low-level cues along with high-level semantic information, learned from large-scale data. We train on a million images, with simulated user inputs. To guide the user towards efficient input selection, the system recommends likely colors based on the input image and current user inputs. The colorization is performed in a single feed-forward pass, enabling real-time use. Even with randomly simulated user inputs, we show that the proposed system helps novice users quickly create realistic colorizations, and offers large improvements in colorization quality with just a minute of use. In addition, we demonstrate that the framework can incorporate other user "hints" to the desired colorization, showing an application to color histogram transfer.},
    journal={ACM Trans. Graph.},
    month=jul,
    articleno={119},
    numpages={11},
    keywords={edit propagation, vision for graphics, deep learning, colorization, interactive colorization}
}

@article{frans_2017,
    author={Kevin Frans},
    title={Outline Colorization through Tandem Adversarial Networks},
    journal={CoRR},
    volume={abs/1704.08834},
    year={2017},
    url={http://arxiv.org/abs/1704.08834},
    archivePrefix={arXiv},
    eprint={1704.08834},
    timestamp={Mon, 13 Aug 2018 16:46:51 +0200},
    biburl={https://dblp.org/rec/journals/corr/Frans17.bib},
    bibsource={dblp computer science bibliography, https://dblp.org}
}

@article{liu_2017,
    title={Auto-painter: Cartoon image generation from sketch by using conditional Wasserstein generative adversarial networks},
    journal={Neurocomputing},
    volume={311},
    pages={78-87},
    year={2018},
    issn={0925-2312},
    doi={10.1016/j.neucom.2018.05.045},
    author={Yifan Liu and Zengchang Qin and Tao Wan and Zhenbo Luo},
    keywords={Auto-painter, GAN, Wasserstein distance, WGAN, Deep learning, Neural networks},
    abstract={Recently, realistic image generation using deep neural networks has become a hot topic in machine learning and computer vision. Such an image can be generated at pixel level by learning from a large collection of images. Learning to generate colorful cartoon images from black-and-white sketches is not only an interesting research problem, but also a useful application in digital entertainment. In this paper, we investigate the sketch-to-image synthesis problem by using conditional generative adversarial networks (cGAN). We propose a model called auto-painter which can automatically generate compatible colors given a sketch. Wasserstein distance is used in training cGAN to overcome model collapse and enable the model converged much better. The new model is not only capable of painting hand-draw sketch with compatible colors, but also allowing users to indicate preferred colors. Experimental results on different sketch datasets show that the auto-painter performs better than other existing image-to-image methods.}
}

@inproceedings{sangkloy_2016,
    author={Patsorn Sangkloy and Jingwan Lu and Chen Fang and Fisher Yu and James Hays},
    title={Scribbler: Controlling Deep Image Synthesis with Sketch and Color},
    booktitle={2017 {IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017},
    pages={6836--6845},
    publisher={{IEEE} Computer Society},
    year={2017},
    doi={10.1109/CVPR.2017.723},
    timestamp={Sat, 19 Oct 2019 20:23:54 +0200},
    biburl={https://dblp.org/rec/conf/cvpr/SangkloyLFYH17.bib},
    bibsource={dblp computer science bibliography, https://dblp.org}
}

@online{paintschainer_2017,
    author={Pixiv},
    title={Pelica Paint},
    url={https://petalica-paint.pixiv.dev/index_en.html},
    year={2017},
    urldate = {2023-01-26}
}

@inproceedings{ci_2018,
    author={Ci, Yuanzheng and Ma, Xinzhu and Wang, Zhihui and Li, Haojie and Luo, Zhongxuan},
    title={User-Guided Deep Anime Line Art Colorization with Conditional Adversarial Networks},
    year={2018},
    isbn={9781450356657},
    publisher={Association for Computing Machinery},
    address={New York, NY, USA},
    doi={10.1145/3240508.3240661},
    abstract={Scribble colors based line art colorization is a challenging computer vision problem since neither greyscale values nor semantic information is presented in line arts, and the lack of authentic illustration-line art training pairs also increases difficulty of model generalization. Recently, several Generative Adversarial Nets (GANs) based methods have achieved great success. They can generate colorized illustrations conditioned on given line art and color hints. However, these methods fail to capture the authentic illustration distributions and are hence perceptually unsatisfying in the sense that they often lack accurate shading. To address these challenges, we propose a novel deep conditional adversarial architecture for scribble based anime line art colorization. Specifically, we integrate the conditional framework with WGAN-GP criteria as well as the perceptual loss to enable us to robustly train a deep network that makes the synthesized images more natural and real. We also introduce a local features network that is independent of synthetic data. With GANs conditioned on features from such network, we notably increase the generalization capability over "in the wild" line arts. Furthermore, we collect two datasets that provide high-quality colorful illustrations and authentic line arts for training and benchmarking. With the proposed model trained on our illustration dataset, we demonstrate that images synthesized by the presented approach are considerably more realistic and precise than alternative approaches.},
    booktitle={Proceedings of the 26th ACM International Conference on Multimedia},
    pages={1536â€“1544},
    numpages={9},
    keywords={gans, interactive colorization, edit propagation},
    location={Seoul, Republic of Korea},
    series={MM '18}
}

@inproceedings{zhang_ji_2017,
    author={L. {Zhang} and Y. {Ji} and X. {Lin} and C. {Liu}},
    booktitle={2017 4th IAPR Asian Conference on Pattern Recognition (ACPR)},
    title={Style Transfer for Anime Sketches with Enhanced Residual U-net and Auxiliary Classifier GAN},
    year={2017},
    volume={},
    number={},
    pages={506-511},
    doi={10.1109/ACPR.2017.61}
}

@inproceedings{kim_2019,
    author={H. {Kim} and H. Y. {Jhoo} and E. {Park} and S. {Yoo}},
    booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)},   
    title={Tag2Pix: Line Art Colorization Using Text Tag With SECat and Changing Loss},   
    year={2019},
    volume={},
    number={},
    pages={9055-9064},
    doi={10.1109/ICCV.2019.00915}
}

@inproceedings{saito_2015,
    author={Saito, Masaki and Matsui, Yusuke},
    title={Illustration2Vec: A Semantic Vector Representation of Illustrations},
    booktitle={SIGGRAPH Asia 2015 Technical Briefs},
    series={SA '15},
    year={2015},
    isbn={978-1-4503-3930-8},
    location={Kobe, Japan},
    pages={5:1--5:4},
    articleno={5},
    numpages={4},
    url={http://doi.acm.org/10.1145/2820903.2820907},
    doi={10.1145/2820903.2820907},
    acmid={2820907},
    publisher={ACM},
    address={New York, NY, USA},
    keywords={CNNs, illustration, search, visual similarity},
}

@inproceedings{hati_2019,
    author={Hati, Yliess and Jouet, Gregor and Rousseaux, Francis and Duhart, Clement},
    title={PaintsTorch: A User-Guided Anime Line Art Colorization Tool with Double Generator Conditional Adversarial Network},
    year={2019},
    isbn={9781450370035},
    publisher={Association for Computing Machinery},
    address={New York, NY, USA},
    doi={10.1145/3359998.3369401},
    abstract={The lack of information provided by line arts makes user guided-colorization a challenging task for computer vision. Recent contributions from the deep learning community based on Generative Adversarial Network (GAN) have shown incredible results compared to previous techniques. These methods employ user input color hints as a way to condition the network. The current state of the art has shown the ability to generalize and generate realistic and precise colorization by introducing a custom dataset and a new model with its training pipeline. Nevertheless, their approach relies on randomly sampled pixels as color hints for training. Thus, in this contribution, we introduce a stroke simulation based approach for hint generation, making the model more robust to messy inputs. We also propose a new cleaner dataset, and explore the use of a double generator GAN to improve visual fidelity.},
    booktitle={European Conference on Visual Media Production},
    articleno={5},
    numpages={10},
    keywords={user-guided colorization, deep learning, datasets, neural networks, generative adversarial network},
    location={London, United Kingdom},
    series={CVMP '19}
}

% TODO: Replace with published updates when DOI available
@inproceedings{hati_2022,
    author={Hati, Yliess and Thevenin, Vincent and Nolot, Florent and Rousseaux, Francis and Duhart, Clement},
    title={StencilTorch: an Iterative and User-Guided Framework for Anime Lineart Colorization},
    year={2022},
    series={IVCNZ '22}
}

@misc{rombach_2021,
    title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
    author={Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and BjÃ¶rn Ommer},
    year={2021},
    eprint={2112.10752},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@online{openai_2023,
    author={OpenAI},
    title={CHATGPT: Optimizing language models for dialogue},
    url={https://openai.com/blog/chatgpt/},
    year={2023},
    month={Jan},
    urldate = {2023-01-26}
}

@article{wolf_2017,
    author = {Wolf, M. J. and Miller, K. and Grodzinsky, F. S.},
    title = {Why We Should Have Seen That Coming: Comments on Microsoft's Tay "Experiment," and Wider Implications},
    year = {2017},
    issue_date = {September 2017},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {47},
    number = {3},
    issn = {0095-2737},
    doi = {10.1145/3144592.3144598},
    abstract = {In this paper we examine the case of Tay, the Microsoft AI chatbot that was launched in March, 2016. After less than 24 hours, Microsoft shut down the experiment because the chatbot was generating tweets that were judged to be inappropriate since they included racist, sexist, and anti-Semitic language. We contend that the case of Tay illustrates a problem with the very nature of learning software (LS is a term that describes any software that changes its program in response to its interactions) that interacts directly with the public, and the developer's role and responsibility associated with it. We make the case that when LS interacts directly with people or indirectly via social media, the developer has additional ethical responsibilities beyond those of standard software. There is an additional burden of care.},
    journal = {SIGCAS Comput. Soc.},
    month = {sep},
    pages = {54â€“64},
    numpages = {11},
    keywords = {AI, technologies of humility, learning software development, software profession, responsibility}
}

@article{dartmouth_2006,
    title={A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955},
    volume={27},
    url={https://ojs.aaai.org/index.php/aimagazine/article/view/1904},
    DOI={10.1609/aimag.v27i4.1904},
    abstractNote={The 1956 Dartmouth summer research project on artificial intelligence was initiated by this August 31, 1955 proposal, authored by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. The original typescript consisted of 17 pages plus a title page. Copies of the typescript are housed in the archives at Dartmouth College and Stanford University. The first 5 papers state the proposal, and the remaining pages give qualifications and interests of the four who proposed the study. In the interest of brevity, this article reproduces only the proposal itself, along with the short autobiographical statements of the proposers.},
    number={4},
    journal={AI Magazine},
    author={McCarthy, John and Minsky, Marvin L. and Rochester, Nathaniel and Shannon, Claude E.},
    year={2006},
    month={Dec.},
    pages={12}
}

@inbook{mccarthy_1978,
    author={McCarthy, John},
    title={History of LISP},
    year={1978},
    isbn={0127450408},
    publisher={Association for Computing Machinery},
    address={New York, NY, USA},
    url={https://doi.org/10.1145/800025.1198360},
    booktitle={History of Programming Languages},
    pages={173â€“185},
    numpages={13}
}

@book{russell_2016,
    title={Artificial Intelligence: a modern approach},
    author={Russell, Stuart J. and Norvig, Peter},
    edition={3},
    year={2009},
    publisher={Pearson}
}

@article{rosenblatt_1958,
    author={Rosenblatt, F.},
    doi={10.1037/h0042519},
    issn={0033-295X},
    journal={Psychological Review},
    number={6},
    pages={386--408},
    title={{The perceptron: A probabilistic model for information storage and organization in the brain.}},
    volume={65},
    year={1958}
}

@book{minsky_1969,
    address={Cambridge, MA, USA},
    author={Minsky, Marvin and Papert, Seymour},
    keywords={linear-classification neural-networks seminal},
    publisher={MIT Press},
    title={Perceptrons: An Introduction to Computational Geometry},
    year={1969}
}

@book{jackson_1998,
    author={Jackson, Peter},
    title={Introduction to Expert Systems},
    year={1998},
    isbn={0201876868},
    publisher={Addison-Wesley Longman Publishing Co., Inc.},
    address={USA},
    edition={3rd},
    abstract={From the Publisher:The third edition of Peter Jackson's book, Introduction to Expert Systems, updates the technological base of expert systems research and embeds those results in the context of a wide variety of application areas. The earlier chapters take a more practical approach to the basic topics than the previous editions, while the later chapters introduce new topic areas, such as case-based reasoning, connectionist systems, and hybrid systems. Results in related areas, such as machine learning and reasoning with uncertainty are also accorded a thorough treatment.}
}

@book{lieto_2021,
    author={Lieto, Antonio},
    year={2021},
    title={Cognitive Design for Artificial Minds (1st ed.)},
    publisher={Routledge},
    isbn={9781315460536},
    doi={10.4324/9781315460536},
    pages={136}
}

@article{john_1992,
    ISSN={00029556},
    abstract={An explanation of automaticity within the framework of the Adaptive Control of Thought (ACT*) production system theory (Anderson, 1983, 1987) is presented. There is no automaticity mechanism per se in ACT*. This is as we would expect it to be. It would be the exception rather than the rule that we would find in a scientific theory mechanisms that directly correspond to natural language concepts. The critical question is whether ACT* can give an account of the phenomena associated with the term automaticity. This article is structured as follows: First, I will try to identify the phenomena of automaticity to be explained, then give a brief overview of the ACT* theory, and finally explain how these phenomena of automaticity are to be understood in terms of the theory.},
    author={John R. Anderson},
    journal={The American Journal of Psychology},
    number={2},
    pages={165--180},
    publisher={University of Illinois Press},
    title={Automaticity and the ACT Theory},
    urldate={2023-01-30},
    volume={105},
    year={1992},
    doi={10.2307/1423026}
}

@book{larid_2019,
    author={Laird, John E.},
    title={The Soar Cognitive Architecture},
    isbn={9780262538534},
    publisher={The MIT Press},
    pages={390},
    day={20},
    month={August},
    year={2019}    
}

@article{fukushima_1980,
    author={Fukushima, Kunihiko},
    title={Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position},
    journal={Biological Cybernetics},
    year={1980},
    month={Apr},
    day={01},
    volume={36},
    number={4},
    pages={193-202},
    abstract={A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by ``learning without a teacher'', and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname ``neocognitron''. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of ``S-cells'', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of ``C-cells'' similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any ``teacher'' during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
    issn={1432-0770},
    doi={10.1007/BF00344251},
}

@article{cortes_1995,
    author={Cortes, Corinna and Vapnik, Vladimir},
    title={Support-vector networks},
    journal={Machine Learning},
    year={1995},
    month={Sep},
    day={01},
    volume={20},
    number={3},
    pages={273-297},
    abstract={Thesupport-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
    issn={1573-0565},
    doi={10.1007/BF00994018}
}

@article{hochreiter_1997,
    author={Hochreiter, Sepp and Schmidhuber, JÃ¼rgen},
    title="{Long Short-Term Memory}",
    journal={Neural Computation},
    volume={9},
    number={8},
    pages={1735-1780},
    year={1997},
    month={11},
    abstract="{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn={0899-7667},
    doi={10.1162/neco.1997.9.8.1735}
}

@article{lecun_1989,
    author={LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
    title={{Backpropagation Applied to Handwritten Zip Code Recognition}},
    journal={Neural Computation},
    volume={1},
    number={4},
    pages={541-551},
    year={1989},
    month={12},
    abstract="{The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.}",
    issn={0899-7667},
    doi={10.1162/neco.1989.1.4.541}
}

@article{lecun_1998,
    author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
    journal={Proceedings of the IEEE}, 
    title={Gradient-based learning applied to document recognition}, 
    year={1998},
    volume={86},
    number={11},
    pages={2278-2324},
    doi={10.1109/5.726791}
}

@inproceedings{deng_2009,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  pages={248-255},
  doi={10.1109/CVPR.2009.5206848}
}

@inproceedings{krizhevsky_2012,
    author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
    booktitle={Advances in Neural Information Processing Systems},
    editor={F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
    publisher={Curran Associates, Inc.},
    title={ImageNet Classification with Deep Convolutional Neural Networks},
    url={https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
    volume={25},
    year={2012}
}

@inproceedings{he_2016,
    title={Deep residual learning for image recognition},
    author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages={770--778},
    year={2016}
}

@article{senior_2020,
    author={Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and {\v{Z}}{\'i}dek, Augustin and Nelson, Alexander W. R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
    title={Improved protein structure prediction using potentials from deep learning},
    journal={Nature},
    year={2020},
    month={Jan},
    day={01},
    volume={577},
    number={7792},
    pages={706-710},
    abstract={Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence1. This problem is of fundamental importance as the structure of a protein largely determines its function2; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures3. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force4 that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction5 (CASP13)---a blind assessment of the state of the field---AlphaFold created high-accuracy structures (with template modelling (TM) scores6 of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined7.},
    issn={1476-4687},
    doi={10.1038/s41586-019-1923-7}
}

@article{jumper_2021,
    author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'i}dek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
    title={Highly accurate protein structure prediction with AlphaFold},
    journal={Nature},
    year={2021},
    month={Aug},
    day={01},
    volume={596},
    number={7873},
    pages={583-589},
    abstract={Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1--4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence---the structure prediction component of the `protein folding problem'8---has been an important open research problem for more than 50Â years9. Despite recent progress10--14, existing methods fall farÂ short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
    issn={1476-4687},
    doi={10.1038/s41586-021-03819-2}
}

@article{vinyals_2019,
    author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
    title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
    journal={Nature},
    year={2019},
    month={Nov},
    day={01},
    volume={575},
    number={7782},
    pages={350-354},
    abstract={Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1--3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8{\%} of officially ranked human players.},
    issn={1476-4687},
    doi={10.1038/s41586-019-1724-z}
}

@article{silver_2016,
    author={Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
    title={Mastering the game of Go with deep neural networks and tree search},
    journal={Nature},
    year={2016},
    month={Jan},
    day={01},
    volume={529},
    number={7587},
    pages={484-489},
    abstract={The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
    issn={1476-4687},
    doi={10.1038/nature16961}
}

@book{goodfellow_2016,
    title={Deep Learning},
    author={Ian J. Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    year={2016},
    address={Cambridge, MA, USA},
    note={\url{http://www.deeplearningbook.org}}
}

@article{aston_zhang_2021,
    title={Dive into Deep Learning},
    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
    journal={arXiv preprint arXiv:2106.11342},
    year={2021}
}

@article{duchi_2011,
    author={John Duchi and Elad Hazan and Yoram Singer},
    title={Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
    journal={Journal of Machine Learning Research},
    year={2011},
    volume={12},
    number={61},
    pages={2121--2159},
    url={http://jmlr.org/papers/v12/duchi11a.html}
}

@misc{hinton_lecture6a,
    title={Neural Networks for Machine Learning},
    subtitle={Overview of mini-batch gradient descent},
    author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
    urldate = {2023-02-03},
}

@article{kingma_2014,
    title={Adam: A method for stochastic optimization},
    author={Kingma, Diederik P and Ba, Jimmy},
    journal={arXiv preprint arXiv:1412.6980},
    year={2014}
}

@article{loshchilov_2017,
    title={Decoupled weight decay regularization},
    author={Loshchilov, Ilya and Hutter, Frank},
    journal={arXiv preprint arXiv:1711.05101},
    year={2017}
}

@article{qian_1999,
    title={On the momentum term in gradient descent learning algorithms},
    journal={Neural Networks},
    volume={12},
    number={1},
    pages={145-151},
    year={1999},
    issn={0893-6080},
    doi={10.1016/S0893-6080(98)00116-6},
    author={Ning Qian},
    keywords={Momentum, Gradient descent learning algorithm, Damped harmonic oscillator, Critical damping, Learning rate, Speed of convergence},
    abstract={A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.}
}